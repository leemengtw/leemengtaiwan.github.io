<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 給所有人的深度學習入門：直觀理解神經網路與線性代數</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="這是篇透過大量動畫幫助你直觀理解神經網路的科普文。我們將介紹基礎的神經網路與線性代數概念，以及兩者之間的緊密關係。我們也將實際透過神經網路解決二元分類任務，了解神經網路的運作原理。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定 AI 之旅的基礎。" />
        <meta name="keywords" content="深度學習, Manim, TensorFlow" />
        <meta name="tags" content="深度學習" />
        <meta name="tags" content="Manim" />
        <meta name="tags" content="TensorFlow" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/CoverSymbolicTwoByThreeMatrixAndNN.png" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html" />
        <meta property="og:title" content="給所有人的深度學習入門：直觀理解神經網路與線性代數" />
        <meta property="og:description" content="這是篇透過大量動畫幫助你直觀理解神經網路的科普文。我們將介紹基礎的神經網路與線性代數概念，以及兩者之間的緊密關係。我們也將實際透過神經網路解決二元分類任務，了解神經網路的運作原理。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定 AI 之旅的基礎。" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <!--for responsive embed youtube video-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/embed_youtube.css">

    <!--for prettify dark-mode result-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/darkmode.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="給所有人的深度學習入門：直觀理解神經網路與線性代數">給所有人的深度學習入門：直觀理解神經網路與線性代數</a><ul><li><a class="toc-href" href="#一些有用的背景知識" title="一些有用的背景知識">一些有用的背景知識</a></li><li><a class="toc-href" href="#深度學習框架操作容易，但你真的了解神經網路嗎？" title="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？</a></li><li><a class="toc-href" href="#用二元分類連結神經網路-&amp;-線性代數" title="用二元分類連結神經網路 &amp; 線性代數">用二元分類連結神經網路 &amp; 線性代數</a></li><li><a class="toc-href" href="#怎樣的神經網路才是好的分類器？" title="怎樣的神經網路才是好的分類器？">怎樣的神經網路才是好的分類器？</a></li><li><a class="toc-href" href="#由淺入深：解決看似不可能的分類任務" title="由淺入深：解決看似不可能的分類任務">由淺入深：解決看似不可能的分類任務</a></li><li><a class="toc-href" href="#結語：往下一站出發" title="結語：往下一站出發">結語：往下一站出發</a></li></ul></li></ul></div>
      </div>
    </div>

    <!--custom images with icon shown on left nav-->
    <!--the details are set in `pelicanconf.py` as `LEFT_NAV_IMAGES`-->

    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/CoverSymbolicTwoByThreeMatrixAndNN.png)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                            <a href="https://leemeng.tw/tag/shen-du-xue-xi.html" rel="tag">深度學習</a>
                            <a href="https://leemeng.tw/tag/manim.html" rel="tag">Manim</a>
                            <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">TensorFlow</a>
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html" title="">
                            給所有人的深度學習入門：直觀理解神經網路與線性代數
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2019-10-13 (Sun)</li>
                        <li class="page-view">
                            26,286 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTocNav()"></i>
        </a>
    </div>

    <div id="social-wrap" style="cursor: pointer">
        <a class="open-popup" title="訂閱最新文章">
            <i class="im im-newspaper-o" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html" target="_blank" title="分享到 Facebook">
            <i class="im im-facebook" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html&title=%E7%B5%A6%E6%89%80%E6%9C%89%E4%BA%BA%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%85%A5%E9%96%80%EF%BC%9A%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%88%87%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8&summary=%E9%80%99%E6%98%AF%E7%AF%87%E9%80%8F%E9%81%8E%E5%A4%A7%E9%87%8F%E5%8B%95%E7%95%AB%E5%B9%AB%E5%8A%A9%E4%BD%A0%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E7%9A%84%E7%A7%91%E6%99%AE%E6%96%87%E3%80%82%E6%88%91%E5%80%91%E5%B0%87%E4%BB%8B%E7%B4%B9%E5%9F%BA%E7%A4%8E%E7%9A%84%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%88%87%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8%E6%A6%82%E5%BF%B5%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%A9%E8%80%85%E4%B9%8B%E9%96%93%E7%9A%84%E7%B7%8A%E5%AF%86%E9%97%9C%E4%BF%82%E3%80%82%E6%88%91%E5%80%91%E4%B9%9F%E5%B0%87%E5%AF%A6%E9%9A%9B%E9%80%8F%E9%81%8E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A7%A3%E6%B1%BA%E4%BA%8C%E5%85%83%E5%88%86%E9%A1%9E%E4%BB%BB%E5%8B%99%EF%BC%8C%E4%BA%86%E8%A7%A3%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E7%9A%84%E9%81%8B%E4%BD%9C%E5%8E%9F%E7%90%86%E3%80%82%E8%AE%80%E5%AE%8C%E6%9C%AC%E6%96%87%EF%BC%8C%E4%BD%A0%E5%B0%87%E8%83%BD%E5%A4%A0%E6%B7%B1%E5%88%BB%E5%9C%B0%E9%AB%94%E6%9C%83%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%88%87%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8%E4%B9%8B%E9%96%93%E7%9A%84%E7%B7%8A%E5%AF%86%E9%97%9C%E4%BF%82%EF%BC%8C%E5%A5%A0%E5%AE%9A%20AI%20%E4%B9%8B%E6%97%85%E7%9A%84%E5%9F%BA%E7%A4%8E%E3%80%82&source=https%3A//leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html" target="_blank" title="分享到 LinkedIn">
            <i class="im im-linkedin" aria-hidden="true"></i>
        </a>
    </div>
    <div id="social-wrap">
        <a href="https://twitter.com/intent/tweet?text=%E7%B5%A6%E6%89%80%E6%9C%89%E4%BA%BA%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%85%A5%E9%96%80%EF%BC%9A%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%88%87%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8&url=https%3A//leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html&hashtags=shen-du-xue-xi,manim,tensorflow" target="_blank" title="分享到 Twitter">
            <i class="im im-twitter" aria-hidden="true"></i>
        </a>
    </div>


    <!--custom images with icon shown on left nav-->

</div>

            <div class="col-full blog-content__main">

                <div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這是篇幫助你直觀理解神經網路的科普文。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定 AI 之旅的基礎。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>（小提醒：因本文圖片與動畫皆為黑色背景，強烈推薦用左下按鈕以 Dark Mode 閱讀本文）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是個對人工智慧（<strong>A</strong>rtificial <strong>I</strong>ntelligence, AI）趨之若鶩的時代。此領域近年的蓬勃發展很大一部份得歸功於<a href="https://leemeng.tw/deep-learning-resources.html">深度學習</a>以及<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神經網路</a>的研究。現行的深度學習框架（framework）也日漸成熟，讓任何人都可以使用 <a href="https://www.tensorflow.org/overview">TensorFlow</a> 或 <a href="https://pytorch.org/">PyTorch</a> 輕鬆建立神經網路，解決各式各樣的問題。</p>
<p>舉例而言，你在 30 秒內就可訓練出一個能夠辨識數字的神經網路：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="c1"># 此例使用 TensorFlow，但各大深度學習框架的實現邏輯基本上類似</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># 載入深度學習 Hello World: MNIST 數字 dataset</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># 建立一個約有 10 萬個參數的「小型」神經網路</span>
<span class="c1"># 在現在模型參數動輒上千萬、上億的年代，此神經網路不算大</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># 選擇損失函數、optimizer</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="c1"># 訓練模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 訓練後的 NN 在測試集上可得到近 98% 正確辨識率</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># 實際測試結果</span>
<span class="c1"># loss: 0.0750 - accuracy: 0.9763</span>
</pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>是的，扣除註解不到 15 行就可以把讀取數據、訓練 <code>model</code> 以及推論全部搞定。這邊秀出程式碼只是想讓你感受一下現在透過框架建立一個神經網路有多麽地「簡單」。事實上，這也是絕大多數線上課程以及教學文章會教你的東西。對此數字辨識應用有興趣的讀者稍後也可自行參考<a href="https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/beginner.ipynb"> TensorFlow 的 Colab 筆記本</a>。</p>
<p>我等等要秀給你看的任何一個神經網路都要比這個 <code>model</code> 還簡單個一萬倍（以參數量而言），但觀察並理解這些「簡單」神經網路，將成為你的 AI 旅程中最有趣且實用的經驗之一。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="一些有用的背景知識">一些有用的背景知識<a class="anchor-link" href="#一些有用的背景知識">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>文中有很多動畫帶你直觀理解神經網路（<strong>N</strong>eural <strong>N</strong>etwork, 後與 <strong>NN</strong> 交替使用）與<a href="https://www.youtube.com/watch?v=fNk_zzaMoSs">線性代數（Linear Algebra）</a>之間的緊密關係。以下知識能幫助你更容易地掌握本文內容：</p>
<ol>
<li>能讀懂文章開頭建立 NN 的 <a href="https://www.python.org/">Python</a> 程式碼</li>
<li>了解線上課程都會教的<a href="https://www.youtube.com/watch?v=Dr-WRlEFefw">超基本 NN 概念</a><ul>
<li>何謂<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（Fully Connected Layer）</a></li>
<li>常見的 activation functions 如 <a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">ReLU</a></li>
</ul>
</li>
<li>基本的線性代數概念如矩陣相乘、向量空間</li>
</ol>
<p>別擔心，這些是 nice-to-have。我等等會盡量囉唆點，讓你就算空手而來也能滿載而歸。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/00010.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        一個簡單 NN 嘗試解決二元分類的過程
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>另外值得一提的是，本文主要展示<strong>已經訓練好</strong>的 NN，不會特別說明<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">訓練一個 NN </a>的細節。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="深度學習框架操作容易，但你真的了解神經網路嗎？">深度學習框架操作容易，但你真的了解神經網路嗎？<a class="anchor-link" href="#深度學習框架操作容易，但你真的了解神經網路嗎？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>再次回到文章開頭的 MNIST 例子。</p>
<p>我們剛剛使用 TensorFlow 高層次 API <a href="https://www.tensorflow.org/guide/keras">Keras</a> 建立了一個神經網路 <code>model</code>：</p>
<div class="highlight"><pre><span></span><span class="c1"># 在 TensorFlow 裡全連接層被稱作 Dense</span>
<span class="c1"># 因為這些層之間的神經元「緊密」連接</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
<p>就跟你在多數教學文章裡頭會看到的一樣，現在要使用深度學習框架建立神經網路十分容易，只要當作疊疊樂一個個 layer 疊上去就好了。下圖則將此 <code>model</code> 用視覺上更容易理解的方式呈現：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/manim/mnist-simple.jpg"/>
</center>
<center>
                        輸入是 28*28 = 784 維的圖片像素，輸出則是 10 個數字類別的簡單 2-layers NN
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>先不看神經網路最左邊的輸入層。右側兩 layers 的每個神經元（neuron）都跟<strong>前一層的每個</strong>神經元相連，所以被稱之為<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E5%85%A8%E9%80%A3%E6%8E%A5%E5%B1%A4">全連接層（<strong>F</strong>ully <strong>C</strong>onnected Layer，後簡稱 FC）</a>。而因為相連<strong>緊密</strong>的特性，TensorFlow 將它們稱作 <code>Dense</code> layer。</p>
<p>我們可以透過 <code>summary</code> 函式輕鬆計算這個神經網路有多少參數：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_9 (Dense)              (None, 128)               100480    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_10 (Dense)             (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>儘管參數量超過 10 萬，此 <code>model</code> 是個在深度學習領域裡只能被歸類在 Hello World 等級的可憐 NN。畢竟這世界很瘋狂，<a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">我們以前討論過的 BERT</a> 以及 <a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">GPT-2</a> 都是現在 NLP 界的知名語言模型，而它們可都是擁有<strong>上億</strong>參數的強大 NN。這些模型的大小可有 <code>model</code> 的 100 倍之大。</p>
<p>但先別管 BERT 或 GPT-2 了，就算是這個 Hello World 等級的 NN，你真的覺得你對它的運作機制有足夠的理解嗎？</p>
<p>講白點，儘管現在路上隨便拉個人都會使用 TensorFlow 或是 PyTorch 來建立神經網路，許多人（包含剛入門的我）對最基本的神經網路的運作方式都沒有足夠<strong>直觀</strong>的理解。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>
<img src="https://leemeng.tw/images/manim/three-key-components.jpg"/>
</center>
<center>
                        構成本文的關鍵三要素：矩陣運算、二元分類以及神經網路
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了讓你能夠直觀且正確地理解神經網路，我將透過二元分類（Binary Classification）任務說明其與線性代數之間的緊密關係。前言很長，但如果你想要了解神經網路的本質，或是想要為自己之後的 AI 學習之旅打下良好基礎，那我會建議你繼續往下閱讀：）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="用二元分類連結神經網路-&amp;-線性代數">用二元分類連結神經網路 &amp; 線性代數<a class="anchor-link" href="#用二元分類連結神經網路-&amp;-線性代數">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>不只出現在深度學習領域，<a href="https://en.wikipedia.org/wiki/Binary_classification">二元分類</a>是<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html">機器學習（Machine Learning）</a>領域裡一個十分基本的任務，其目標是把一個集合裡的所有數據點（data points）依照某種分類規則劃分成<strong>兩</strong>個族群或類別（classes）。經典的例子有我們之前看過的<a href="https://demo.leemeng.tw/">貓狗圖像辨識</a>。在這篇文章裡，我假設所有數據點<strong>最多只有兩個維度（dimensions）</strong>。</p>
<p>如果你讀過之前的文章，可能會覺得這假設是在「羞辱」我們。畢竟我們已用過神經網路來：</p>
<ul>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">執行假新聞偵測（BERT）</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">生成新金庸小說（GPT-2）</a></li>
<li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html">把英文翻成中文（Transformer）</a></li>
<li><a href="https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html">生成新海誠動畫（CartoonGAN）</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這些任務的複雜程度要比<strong>二維</strong>二元分類多了幾個數量級（當然也非常有趣。你稍後可以點擊相關連結深入了解）。但這篇文章之所以選擇二維二元分類作為目標任務正是因其簡單明暸，我們將能以此窺探神經網路的本質（essence）。</p>
<p>以下是一個包含兩條曲線的資料集：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/showlinearseparabledatapoints.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        包含兩類別的二維雙曲線資料集
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在此雙曲線資料集裡，兩條曲線分別來自不同類別，各自包含了 100 個數據點 $x$。每個數據點 $x$ 都可以很自然地以二維 $\left (x_{coord}, y_{coord}  \right )$ 座標來呈現。右下角也標注了兩曲線所屬類別：黃點的標籤 $y = 0$，藍點則為 $1$。</p>
<p>另外，圖中也描繪了此向量空間中的基底向量：</p>
<ul>
<li>x 軸上藍色的 $\vec{i}$</li>
<li>y 軸上紅色的 $\vec{j}$ </li>
</ul>
<p>等等你會看到，這兩個基底向量（basis vector）能夠幫助我們了解神經網路的運作方式。至於要如何分類這個資料集呢？回想一下<a href="https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html">之前 AI For Everyone 的重要概念</a>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        目前多數的機器學習以及 AI 應用本質上都是讓電腦學會一個映射函數，幫我們將輸入的數據 x 轉換到理想的輸出 y。
                        <br/>
<span style="float:right;margin-right: 1.5rem">─ Andrew Ng</span>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>套用相同概念，要處理這個分類任務，我們要問的問題就變成「給定所有藍點與黃點 $x = \left (x_{coord}, y_{coord}  \right )$，我們能不能找出一個 $x$ 的函數 $f(x)$，將這些二維數據 $x$ 完美地<strong>轉換</strong>到它們各自所屬的一維標籤 $y$ 呢？」</p>
<p>換句話說，我們想要找出一個函數 $f(x)$，使得以下式子成立：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) 
&amp; = f(\begin{bmatrix} x_{coord} \\ y_{coord} \end{bmatrix}) \\
&amp; = f(\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}) \\
&amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果我們能找到符合這個條件的 $f(x)$，就能在一瞬間預測出某個數據點 $x_{i}$ 比較可能是哪個類別了。我們有非常多種 model $f(x)$ 的方法，但在線性代數的世界裡，我們可以用矩陣運算的形式來定義一個 $f(x)$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) &amp; = W x + b \\
  &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} x + b \\ 
  &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
  &amp; = y
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我希望你至少還記得國中、高中或是大學裡任何一位數學老師的諄諄教誨。</p>
<p>在上面的式子裡：</p>
<ul>
<li>$x$ 是一個二維 column vector</li>
<li>$W$ 是一個 1 &times; 2 的<a href="https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%98%B5">權重矩陣（weight matrix）</a></li>
<li>$b$ 為偏差（bias），是一個純量（scalar）</li>
</ul>
<p>如果我們先暫時忽略 $b$，事實上 $f(x)$ 對輸入 $x$ 做的<strong>轉換</strong>跟深度學習領域裡時常會使用到的全連接層（FC）的運算是完全相同的。換句話說，使用一層 FC 的 NN 基本上就是在做矩陣運算（假設激勵函式為線性、偏差為 0）。因此用矩陣運算定義的 $f(x)$ 與神經網路之間有非常美好的對應關係：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/symboliconebytwomatrixandnn.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        1 &times; 2 矩陣運算與全連接層的對應關係
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>沒錯，透過矩陣運算，我們剛剛建立了這世上最簡單的 1-Layer 神經網路！而之所以是 1-Layer，原因在於 NN 的第一層為原始數據（raw data），第二層才是我們新定義的神經網路（一層 FC）。矩陣運算跟 FC 的對應關係家喻戶曉，但這應該是你第一次看到兩者共舞。</p>
<p>從<strong>線性代數</strong>的角度來看，我們透過矩陣運算將二維的 $x$ 轉換成一維的 $y$；而以<strong>神經網路</strong>的角度檢視，我們則是將以二維向量表示（represent）的數據點 $x$ 透過與權重 $W$ 進行加權總和後得到新的一維表徵（representation）$y$。這是常有人說神經網路在做<a href="https://arxiv.org/abs/1206.5538">表徵學習（Representation Learning）</a>的原因。</p>
<p>另外值得一提的是矩陣 $W$ 裡的每個參數 $w_{mn}$ 實際上就對應到 NN 某一層中第 $n$ 個神經元（neuron）連到其下一層中第 $m$ 個神經元的<strong>邊（edge）</strong>。別擔心，<a href="#結語：往下一站出發">在本文的最後面</a>還會有動畫幫助你記憶此對應關係。</p>
<p>就跟文章開頭看到的一樣，要使用 TensorFlow 定義這個 1-Layer NN 也十分容易：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="c1"># 將 2 維 input  轉成 1 維 output</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> 
                                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 1)                 2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>扣除偏差後整個神經網路的確只剩 2 個參數。為了讓你加深印象，讓我們將一些數字代入 $W$ 與 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
W = \begin{bmatrix} 1 &amp; 5  \end{bmatrix} \\ 
x = \begin{bmatrix} 3 \\ 3 \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>接著再看一次剛剛的運算過程：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/numberonebytwomatrixandnn.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        1 &times; 2 矩陣運算與全連接層的對應關係（實際數值）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這邊我刻意讓 $x_{1} = x_{2} = 3$。你可以清楚地看到不同大小的 $w_{mn}$ 可以讓不同維度、但相同值的 $x_{n}$ 給與最終輸出 $y_{m}$ 不同程度（權重）的影響力，這也是 $W$ 之所以被稱之為權重（weights）的原因。</p>
<p>你現在知道最基本的矩陣運算、神經網路以及兩者之間的緊密關係了。更美妙的是，如果你已經了解<a href="https://youtu.be/fNk_zzaMoSs">線性代數的本質</a>，就會知道函數 $f(x) = W x + b = y$ 事實定義了兩個簡單轉換來將二維的輸入 $x$ 依序轉換成一維輸出 $y$：</p>
<ol>
<li>線性轉換：$W$</li>
<li>位移：$b$</li>
</ol>
<p>你多年前可能也已看過<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84">線性轉換（linear transformation, or linear map）</a>的正式數學定義：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<img src="https://leemeng.tw/images/manim/formal-linear-transformation-formula.jpg" style="mix-blend-mode: initial;"/>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是所有線性轉換都具備的<a href="https://en.wikipedia.org/wiki/Additive_map">可加性（additivity）</a>與<a href="https://zh.wikipedia.org/wiki/%E9%BD%90%E6%AC%A1%E5%87%BD%E6%95%B0">齊次性質（homogeneity）</a>。不過別擔心，在本文裡你不需了解這些定義也能直觀地理解線性轉換。用比較不嚴謹的說法，線性轉換會對其作用的向量空間（vector space）做<strong>旋轉</strong>、<strong>伸縮</strong>等「簡單」轉換。</p>
<p>要直觀瞭解這個概念，讓我們再次將幾個數字代入 $f(x)$ 的參數裡頭：</p>
\begin{align}
f(x) &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
 &amp; = \begin{bmatrix} 1 &amp; 1  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + (-2) 
\end{align}<p>這次假設 $w_{11} = w_{12} = 1, b = -2$。另外別忘了我們剛學的，這個函數 $f(x)$ 可以被表示成到一個特定的 1-Layer NN。我們可以觀察這個 NN（即 $f(x)$）如何轉換二維空間裡頭的 $x$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/applymatrixassingleLineartransformation.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        利用 1-Layer NN 將二維輸入轉換成一維輸出
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這個 NN 做的轉換十分簡單，但是隱含了不少重要概念。</p>
<p>首先，以<strong>神經網路</strong>的角度解讀 $f(x)$ 的話，此二維向量空間裡頭的任一數據點 $ x = \left (x_{coord}, y_{coord}  \right )$ 都對應到左上 NN 第一層（輸入層）中的 2 個神經元。權重 $W$ 則以<strong>邊</strong>的方式呈現，負責將第一層中 2 個神經元的值依照不同比重<strong>送去啟動</strong>（activate）下一層的神經元。啟動下層神經元後，NN 只要再將一個偏差值 $b$ 加到該神經元即完成此 layer 的轉換。</p>
<p>以<strong>線性代數</strong>的視角檢視 $f(x)$ 的話，裡頭的矩陣 $W$ 則定義了一個線性轉換。此轉換說明了原向量空間 $V_{original}$ 裡的 2 個基底向量 $\vec{i}$、$\vec{j}$ 在<strong>轉換後</strong>的一維空間 $V_{transformed}$ 中的目標位置。$w_{11} = 1$ 讓原 x 軸上的 $\vec{i}$ 保持在「原位」；$w_{12} = 1$ 則將 $\vec{j}$ 放到 $V_{transformed}$裡與 $\vec{i}$ 一樣的位置。</p>
<p>此例中 $V_{transformed}$ 是一個跟 x 軸重疊的一維數線（Number line），你在動畫最後面也可以看到 $\vec{i}$ 跟 $\vec{j}$ 在此數線上的相同位置。為了讓轉換過後的 $\vec{j}$ 能在指定的位置，y 軸順時針旋轉並轉換到了 $V_{transformed}$ 之上。你也能看到原二維空間裡頭的每個數據點 $x$ 都跟著 $\vec{j}$ 一起被轉換成該數線上的一個值，而不再是二維座標。透過 $W$ 被轉換到一維空間以後，該數線上的每個數據點只要再被加上偏差 $b$ 就完成兩步驟的轉換。</p>
<p>我幫你用兩種角度檢視 $f(x)$，但這邊最重要的啟示是：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        你可以用神經網路或是線性代數的角度來解讀 $f(x)$ 的作用，但兩者殊途同歸：它們實際上都是利用 $f(x)$ 將原始數據 $x$ 進行一系列幾何轉換後輸出理想的 $y$。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這也是為何<a href="https://demo.leemeng.tw/#%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%EF%BC%9A%E4%B8%80%E5%80%8B%E6%98%A0%E5%B0%84%E5%87%BD%E6%95%B8">我們之前說神經網路本質上是一個映射函數</a>的原因。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="怎樣的神經網路才是好的分類器？">怎樣的神經網路才是好的分類器？<a class="anchor-link" href="#怎樣的神經網路才是好的分類器？">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我們剛剛看到，1-Layer NN 可以透過一個 FC（線性轉換 $W$ + 位移 $b$）將二維輸入 $x$ 轉換成一維輸出 $y$。不過由於我們剛剛是隨意設定參數 $W$ 及 $b$，如果以分類器（classifier）的標準去評價該 NN 的話，其表現實在是令人不敢恭維。</p>
<p>讓我們快速回顧一下剛剛的 NN 做了怎樣的轉換，並計算分類準確率：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/applymatrixassingleLineartransformationandshowaccuracy.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        用 1-Layer NN 做二元分類（參數隨意設定）
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>我盡量讓所有動畫 speak for themselves，但希望你不介意我再嘮叨一下。</p>
<p>我們一樣透過 $f(x)$ 將兩組分別以黃色及藍色表示的二維數據點 $x$ 轉成一維的 $y$。轉換之後我們能找出一個 $y$ 值（此例中 $y = 1$），畫條垂直線將兩組輸出 $y$ 分開。這條分類界線（classification boundary）能使我們分對最多的 $x$。</p>
<p>我們可以用此分界線定出一個分類規則：任何數據點 $x$ 只要它的 $f(x) = y$ 在這條線的左邊，我們就預測其來自黃色曲線，反之則為藍色曲線。很明顯的，黃色在此界線的左側則是因其標籤數值（label）比藍線小。</p>
<p>溫馨提醒，這是我們剛剛用來轉換 $x$ 的神經網路 $f(x)$：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
f(x) &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + b \\ 
 &amp; = \begin{bmatrix} 1 &amp; 1  \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} + (-2) 
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>將 $W$ 設為 $[1, 1]$、$b$ 設定為 $-2$ 以後，$f(x)$ 函數對輸入 $x$ 做的轉換不至於錯得離譜，但也就只能把觀測到的 200 個數據點 $x$ 裡頭的 68 個點正確分類，其準確率（accuracy）只有 66%。很明顯的，要透過 $f(x)$ 來解決當前的二元分類任務，我們要問的問題變成：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        這組參數的選擇是最佳解嗎？能不能找到一組最好的參數 $W$ 與 $b$，使得 $f(x)$ 最後的分類準確率最大？
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>答案當然是肯定的 Yes，只要適當地運用以下數學概念與技巧就能做到：</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=WUvTyaaNkzM&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">微積分（Calculus）</a></li>
<li><a href="https://youtu.be/IHZwWFHWa-w?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">梯度下降（Gradient Descent）</a></li>
<li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3">反向傳播演算法（Back Propagation）</a></li>
</ul>
<p>這也是深度學習核心之一的<strong>學習</strong>過程。在此例的 $f(x)$ 裡頭，其參數 $W$ 與 $b$ 的實際數值會在模型的訓練過程中不斷地被修正以提升分類準確率，所以一般會被稱作可訓練的參數 $\theta$。事實上，我們前面定義 $f(x)$ 時，並不只是定義一個特定的函數，而是一整個函數空間 $f_{\theta}(x)$。此空間裡頭的每個函數雖然<strong>架構</strong>上都是先對 $x$ 做線性轉換再進行位移，但因其參數值 $\theta$ 皆有所不同，實際上做的轉換也就有所差異。這也就意味著：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        我們可以透過矩陣運算的形式定義一個神經網路架構 $f_{\theta}(x)$，再透過微積分與反向傳播等方式，求得此架構裡能最能有效解決任務的參數 $\theta$。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這是為何有人會跟你說要<strong>真的</strong>學好<a href="https://leemeng.tw/deep-learning-resources.html">深度學習</a>，<a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">線性代數</a>以及<a href="https://www.youtube.com/watch?v=WUvTyaaNkzM&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">微積分</a>很重要的原因。前者讓你定義出一個具有解決複雜問題「潛力」的數據轉換架構（神經網路），後者則讓你實際找出每個轉換步驟所需的細節（參數權重）。當然，就算你完全不懂後者，深度學習框架也能幫你搞定一切：</p>
<div class="highlight"><pre><span></span><span class="c1"># 文章開頭的 MNIST 範例程式碼片段</span>
<span class="c1"># 定義一個能夠解決問題的模型架構</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># 設置學習實際參數所需的東西：損失函數、optimizer</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>

<span class="c1"># 訓練模型，取得最好的參數 theta</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 打完收工</span>
</pre></div>
<p>不幸的是，深度學習框架的方便也成為很多人不明就裡的原因之一。</p>
<p>如本文開頭所述，我的目標是讓你可以透過線性代數來直觀地了解<strong>已訓練好</strong>的 NN 如何轉換數據並達成任務目標。因此底下展示的都是我先幫你訓練好的 NN（即我們已經知道最好的 $\theta$ 為何）。你等等可以參考我在文後附上的連結，深入了解如何實際訓練一個 NN，找出一組好的參數 $\theta$。</p>
<p>事不宜遲，讓我們馬上看看一個訓練好的 1-Layer NN 能把這個二元分類任務做到多好：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/singlew1solvelinearseparablescene.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        完美分類兩曲線的 1-Layer NN
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>簡單而完美。事實證明，只要使用適合的參數 $\theta$，用矩陣形式定義的這個 1-Layer NN 能夠完美地將這兩條曲線分開。另外，如果你再稍微仔細觀察最初的雙曲線並結合前面談到的矩陣概念，就會發現 $W$ 這個線性轉換是非常有意思的：</p>
\begin{align}
W &amp; = \begin{bmatrix} w_{11} &amp; w_{12}  \end{bmatrix} \\
&amp; = \begin{bmatrix} 0.01 &amp; 1.67  \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你可以用底下動畫再次觀察轉換前的二維向量空間。</p>
<p>你會發現黃藍雙曲線本身雖然看似複雜，其實只要用一條幾乎跟 x 軸重疊的<strong>水平線</strong>就能將兩者切開。這也就意味著二維數據點 $x = \left (x_{coord}, y_{coord}  \right )$ 裡頭的第一個維度 $x_{coord}$ 的值幾乎不會影響<strong>分類</strong>兩曲線的決策。此 NN 學會將 $w_{11}$ 設成一個很小的值 $0.01$，讓原空間中 x 軸上的值都壓縮到幾乎等於 0。檢視動畫左上的 NN 示意圖，你會發現代表 $w_{11}$ 的<strong>第一條邊</strong>幾乎沒有將輸入層中第一個神經元的值 $x_{coord}$ 傳給輸出神經元。這也就代表 $x_{coord}$ 不太影響最後的 $y$ 值。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/showlinearseparabledatapoints.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        方便你對照，這邊再次展示轉換前的黃藍雙曲線
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>而因為一個數據點的第二個維度 $y_{coord}$ 才是用來判斷其所屬類別的重要信號（$y_{coord}$ 越大，藍線機率越大），且理想上轉換後的藍點標籤 $y$ 要大於黃點的 $y$，因此這個 NN 將 $w_{12}$ 設為一個相對 $w_{11}$ 大的正值，將原空間 $V_{original}$ 的 y 軸順時針旋轉，讓所有藍點的 $y$ 皆大於黃點的 $y$。很有意思，對吧？你可以把矩陣 $W$ 裡頭的值換掉，並測試看看自己是否能在腦海中想像其對應的線性轉換過程。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="由淺入深：解決看似不可能的分類任務">由淺入深：解決看似不可能的分類任務<a class="anchor-link" href="#由淺入深：解決看似不可能的分類任務">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果有反覆觀看前面動畫並停下來動腦思考，我相信你現在已經能夠直觀地理解並想像一個 1-Layer NN 是怎麼解決二元分類問題的。這是一個很不錯的開始。在這小節，我想提升分類任務的難度，讓你再多點對神經網路的直觀感受。</p>
<p>比方說我們可以將剛剛 1-Layer NN 完美分類的兩條曲線拉「近」點：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/makebinaryclassificationharder.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        提升分類任務的遊戲難度：讓兩條曲線靠近點
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>很明顯地，你現在再也無法找到一條平行線來將兩者切開了。相較於我們前面看過的簡單雙曲線，現在這兩條曲線靠得更近，也讓神經網路更難將它們切開了。讓我把這個新的資料集稱作困難雙曲線。</p>
<p>因為我們前面看過的 1-Layer NN 只透過 FC 做一組簡單的線性轉換與位移，我並不認為它能在困難雙曲線上得到多好的結果。不過眼見為憑，還是讓我們看一下 1-Layer NN 在困難雙曲線上的表現：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/onelinearlayersolvehardtwocurves.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        刁難 1-Layer NN：讓它處理更難的分類問題
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>不需特別計算準確率，你一眼就能看出 1-layer NN 在這資料集上表現地實在是力不從心。兩條曲線在被轉換之後仍然有非常多的重疊部分。換句話說，此 NN 做的（線性）轉換並無法將兩個不同類別的數據點 $x$ <strong>完全</strong>分開。</p>
<p>以線性代數的角度來看，這個 NN 就是透過矩陣 $W$ 對<strong>整個</strong>二維空間做<strong>一致</strong>的旋轉與伸縮，無法針對性地將兩條緊靠的雙曲線在空間中拉扯開來。不過別擔心，深度學習領域裡的人最愛玩疊疊樂了。讓我們多疊一層全連接層（FC），看看新的 2-Layers NN 如何處理這個問題：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/twolinearlayerssolvehardtwocurves.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        2-Layers NN 解決困難雙曲線分類問題的過程
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這個 2-Layers NN 的表現如你預期嗎？還是讓你失望了？在找戰犯之前，讓我先說明基本概念。</p>
<p>雖然這是本文第一次展示 2-Layers NN，動畫應該已經把所有東西都交代得很清楚了。一個 FC 就對應到一組矩陣 $W$ 以及偏差 $b$，因此一個有兩層 FCs 的神經網路 $g(x)$ 實際上就是對 $x$ 做兩次矩陣運算（與加上偏差）：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
g(x) &amp; = W_{2}(W_{1} x + b_{1}) + b_{2}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這邊 $W_{i}$ 與 $b_{i}$ 分別代表第 $i$ 個 FC 的權重矩陣與偏差。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
\text{1-Layer NN} = f(x) &amp; = W x + b
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>2-Layers NN 裡頭的矩陣 $W_{1}$ 跟 1-Layer NN 裡頭的矩陣 $W$ 都代表著<strong>第一個</strong> FC 做的線性轉換。但跟 1-Layer NN 不同的地方在於，$W_{1}$ 並不是直接把原始數據點 $x$ 轉換成一維的 $y$，而是先將 $x$ 轉到另個形式的二維表徵 $h$，接著再讓下一層 FC 將 $h$ 轉換至一維的 $y$。像是這種只存在輸入與輸出之間的表徵一般被稱作隱藏表徵（hidden representation），而生成這些表徵的 FC 層自然就被稱作隱藏層（hidden layer）。</p>
<p>而建構這種數據轉換架構的背後精神就是<a href="https://arxiv.org/abs/1206.5538">表徵學習（Representation Learning）</a>：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        透過對輸入 $x$ 做一連串簡單的幾何轉換，將此原始表徵 $x$ 逐漸轉換成能夠用來解決我們問題的隱藏表徵 $h$，最後輸出成目標結果 $y$。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>雖然這邊的 2-Layers NN 表現不佳，但這概念可是你在剛接觸深度學習時值得畫上三顆星的重點之一。</p>
<p>回到剛剛的 2 &times; 2 的 $W_{1}$ 矩陣。跟之前 1 &times; 2 矩陣一樣，這次讓我們看看 $W_{1}$ 跟 FC 之間的關係：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/symbolictwobytwomatrixandnn.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        2 &times; 2 矩陣 $W_{1}$ 的運算與全連接層之間的關係
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>你可能注意到矩陣 $W_{1}$ 裡頭每一行（column）顏色皆不相同。</p>
<p>矩陣 $W_{1}$ 定義了一個線性轉換，而裡頭的每個 column 則定義了原空間 $V_{original}$ 中的各個基底向量在新空間 $V_{transformed}$ 中的位置。而以神經網路的角度來看，矩陣 $W_{1}$ 中的每一行則對應到某個神經元連到下一層所有神經元的權重值。</p>
<p>另外值得一提的是，FC 跟矩陣的緊密關係事實上也部分解釋了為何神經網路常以一層層（Layered）形式出現的原因。雖然這邊的 NN 只有兩層，在深度學習領域裡我們時常會想對原始數據 $x$ 進行幾十、幾百次的轉換，而由一連串矩陣組成的運算非常容易平行化，因此在建立深度神經網路（Deep Neural Network）時，疊很多以矩陣運算為基礎的 Layers 是一個非常主流的做法。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/shownonlinearfunctions.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        線性整流函數 ReLU 可以讓神經網路對數據做更複雜的轉換
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>至於為何剛剛的 2-Layers NN 無法解決困難雙曲線？</p>
<p>在了解矩陣運算以及線性轉換以後，一切都變得很容易解釋。結合前面所學：</p>
<ul>
<li>一層 FC 對應到一矩陣運算（加偏差）</li>
<li>一個權重矩陣對應到一個線性轉換</li>
<li>一個線性轉換會對作用空間做旋轉伸縮</li>
</ul>
<p>就可以得知我們的 2-Layers NN $g(x)$ 事實上就是對原空間裡頭的數據點 $x$ 做連續兩次的旋轉與伸縮（當然，動畫已經告訴我們這件事情了）。很直觀地，不管神經網路做幾次這樣的轉換，最後的效果都可以被<strong>單一</strong>線性轉換取代。換句話說，對困難雙曲線連續做好幾次線性轉換是不會得到比單一線性轉換更好的結果的。你可以拉回去看看 2-Layers NN 如何「瞎忙」。</p>
<p>解法也很直覺。我們可以找一個非線性函數如 <a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">ReLU</a> 當作 FC 層的激勵函數（activation function），讓 FC 層跟其他層之間有<strong>非線性</strong>的轉換，讓神經網路掌握超越線性轉換的能力。</p>
<p>以下就是將 ReLU 加入 2-Layers NN 並解決困難雙曲線分類的過程：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/twolayersreluInbetweensolvehardtwocurves.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<br/>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這個轉換過程超美的，不是嗎？這個神經網路只有 9 個參數，卻是我看過最美麗的神經網路之一。</p>
<p>透過線性轉換與非線性轉換的交替使用，升級後的 2-Layers NN $h(x)$ 成功地將困難雙曲線任務解開，辦到前一代 $g(x)$ 做不到的事情，而兩者只差在一個 ReLU 函數：</p>
\begin{align}
g(x) &amp; = W_{2}(W_{1} x + b_{1}) + b_{2} \\
h(x) &amp; = W_{2} relu(W_{1} x + b_{1}) + b_{2}
\end{align}<p>再次提醒，激勵函數 $a(z) = relu(z)$ 一般是被<strong>各別</strong>套用到在計算完 $W(x) + b = z$ 的神經元之上，不需做矩陣運算，因此幾乎沒有計算成本（好啦，還是要看是哪個函數）。在深度學習領域裡，使用非線性函數來提升神經網路的處理能力是件稀鬆平常的事。除了常用的 ReLU 之外，其他知名的函數包含了 <a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a>、<a href="https://zh-yue.wikipedia.org/wiki/%E9%9B%99%E6%9B%B2%E5%87%BD%E6%95%B8">Tanh</a> 以及 <a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0">Leaky ReLU</a> 等等。</p>
<p>到此為止，你也已經了解非線性函數之於神經網路的重要性了。我們的旅程也將進入尾聲。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="結語：往下一站出發">結語：往下一站出發<a class="anchor-link" href="#結語：往下一站出發">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>呼！不知不覺就到了這趟旅程的終點了！結束地真快，你說是吧？</p>
<p>就算你一開始什麼都不懂，在閱讀本文以後你應該已經能夠：</p>
<ol>
<li>了解最基本的矩陣運算、神經網路以及兩者之間的緊密關係</li>
<li>想像線性轉換如何對向量空間做旋轉、伸縮等基本轉換</li>
<li>理解並想像一個 1-Layer NN 怎麼解決二元分類問題</li>
<li>了解如何透過矩陣運算的形式定義一個神經網路架構 𝑓𝜃(𝑥) </li>
<li>使用神經網路或是線性代數的角度來解讀 𝑓𝜃(𝑥) 的作用</li>
<li>明白神經網路就是將原始數據 𝑥 進行一系列幾何轉換後輸出理想的 𝑦 的映射函數</li>
<li>瞭解如何透過非線性的轉換，讓神經網路掌握更進階的能力</li>
<li>自由地連結矩陣運算以及神經網路的概念並在兩者之間切換</li>
</ol>
<p>沒錯，你可能會驚訝於自己學了那麼多。更美妙的是透過大量動畫，很多概念應該都已經深深地烙印在你的腦海中，你甚至不需要背什麼東西。在這個 AI Hype 時代，我們沒有談最新的 AI 論文，也沒有用深度學習框架做什麼酷炫的應用，但我相信這樣的文章才是大部分人以及生活在 AI 時代的下一代所需要的。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/symbolictwobythreematrixandnn.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        2 &times; 3 矩陣與全連接層的對應關係
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>如果你想要學習更多線性代數以及微積分的相關知識，強力推薦 <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/featured">3Blue1Brown 的 Youtube 頻道</a> ，本文所有動畫也都是用該頻道作者 <a href="https://www.3blue1brown.com/about">Grant Sanderson</a> 開源的 <a href="https://github.com/3b1b/manim">Manim</a> 製作的。另外如果你想要深入了解深度學習，<a href="https://leemeng.tw/deep-learning-resources.html">由淺入深的深度學習資源整理</a>是一個好的開始。</p>
<p>如果你已經有點 <a href="https://www.python.org/downloads/">Python</a> 基礎，想要看深度學習還能拿來做些什麼，可以參考之前的文章：</p>
<ul>
<li><a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">寫給所有人的自然語言處理與深度學習入門指南</a></li>
<li><a href="https://demo.leemeng.tw/">AI 如何找出你的喵：直觀理解卷積神經網路</a>。</li>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">進擊的 BERT：NLP 界的巨人之力與遷移學習</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html">直觀理解 GPT-2 語言模型並生成金庸武俠小說</a></li>
<li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html">淺談神經機器翻譯 &amp; 用 Transformer 與 TensorFlow 2 英翻中</a></li>
<li><a href="https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html">用 CartoonGAN 及 TensorFlow 2 生成新海誠與宮崎駿動畫</a></li>
</ul>
<p>底下是我為你做的最後一個動畫。你可以用它來測試自己在本文的所學，並熟悉矩陣與神經網路之間的關係。注意矩陣行與列中的參數 $w_{mn}$ 是怎麼對應到神經網路的邊（edge）的。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<video autoplay="" loop="" muted="" playsinline="" poster="https://leemeng.tw/20190921_manim_binary_classification.ipynb">
<source src="https://leemeng.tw/images/manim/randommatrixneuralnetworkmapping.mp4" type="video/mp4"/>
                    您的瀏覽器不支援影片標籤，請留言通知我：S
                </video>
<center>
                        更多矩陣與神經網路的對應關係
                        <br/>
<br/>
</center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在這個 AI 新聞滿天飛的時代，我希望這篇<strong>入門</strong>文章可以幫助更多人直觀地理解其背後的核心動力：神經網路並開始自己的 AI 之旅。如果這篇文章有幫助到你，希望你能發揮舉手之勞，幫忙<strong>分享本文</strong>，以讓更多、更多的人可以加入這個行列。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        也希望你喜歡這首神經網路與線性代數的雙重奏，我們下次見。
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                        <a href="https://leemeng.tw/tag/shen-du-xue-xi.html" rel="tag">深度學習</a>
                        <a href="https://leemeng.tw/tag/manim.html" rel="tag">Manim</a>
                        <a href="https://leemeng.tw/tag/tensorflow.html" rel="tag">TensorFlow</a>
                    </span>

                </p>







                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary ">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a href="https://leemeng.tw/essence-of-principal-component-analysis.html" rel="prev">
                                <span>Previous Post</span>
                                世上最生動的 PCA：直觀理解並應用主成分分析
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html" rel="next">
                                <span>Next Post</span>
                                直觀理解 GPT-2 語言模型並生成金庸武俠小說
                            </a>
                        </div>
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '給所有人的深度學習入門：直觀理解神經網路與線性代數';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<!-- footer
================================================== -->
<footer style="background:#0a0809">
    <div class="row">
        <div class="col-full">

            <div class="footer-logo">
                <a class="footer-site-logo" href="#0"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
            </div>

            <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>            </ul>
        </div>
    </div>
    <div class="row footer-bottom">
        <div class="col-twelve">
            <div class="go-top">
            <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
            </div>
        </div>
    </div> <!-- end footer-bottom -->
</footer> <!-- end footer -->


        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main_raw.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script><!--https://darkmodejs.learn.uno/-->
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.4.0/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.2s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTocNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}

function closeLeftNavImage(elementId) {
    document.getElementById(elementId).style.width = "0%";
}

function toggleLeftNavImage(elementId) {
    var current_width = document.getElementById(elementId).style.width;
    if (current_width == "100%") {
        document.getElementById(elementId).style.width = "0%";
    } else {
        document.getElementById(elementId).style.width = "100%";
    }
}

</script>


</body>
</html>