var tipuesearch = {"pages":[{"title":"監控資本主義時代下的資料科學、AI 與你我的數位未來","text":"監控資本主義在 21 世紀主導資料科學以及 AI 的發展方向，並持續往大多數人不樂見的方向邁進。只有在大眾能夠認知到並理解其商業邏輯，我們才能透過集體意識去改善每個人的數位未來。 ─ 1 分鐘懶人包 這是本文最想傳達的訊息。因為資訊量龐大，如果你現在還無法消化也沒有關係。閱讀完本文，你將能夠了解在背後驅使數位世界光速運轉的商業邏輯，並用一個可說是駭人卻又無比清晰的視角理解自己正身處在一個怎麼樣的數位時代。 引言 2018 年 5 月，我在 揭開資料科學的神秘面紗 中粗略地說明過為何 注意力經濟（Attention Economy） 促使近年大數據、資料科學以及人工智慧的蓬勃發展。注意力經濟將人類的注意力視為資訊過載時代下最寶貴的資源，而 谷歌（Google） 與 臉書（Facebook） 在一般使用者不知情或是沒有明確允許的情況下貪婪地蒐集、累積、分析進而預測其 廣告點擊 行為。他們想盡辦法使你我對其服務上癮，以將我們有限的注意力榨取成龐大的商業利益。 長年下來，越來越多人注意到「免費」的代價並看到此經濟邏輯的可怕之處，不少專家則開始將此經濟邏輯與商業模式稱之為 監控資本主義（Surveillance Capitalism） 。 現下熱門的數位服務與社群媒體，每天佔用了你多少注意力與時間？ 今天我想用簡單的經濟學、心理學與資料科學的角度帶你理解其背後的運作原理以及其對人類社會帶來的負面影響。 本文章節安排 儘管其商業模式早就統治了我們的世界，關於監控資本主義的結構性描述一直要等待哈佛商學院榮譽教授 肖莎娜・祖博夫（Shoshana Zuboff） 的 《監控資本主義時代》（後簡稱為《監控》） 出版後才比較被一般大眾所知。此書也 名列歐巴馬在 2019 年最愛的書籍之一 。 我在第一章會先以知名的臉書為例，簡述科技公司現在是如何監控我們的數位行為。接著，我將花 2 個章節跟你一起拿著祖博夫的放大鏡，用經濟學以及心理學直觀地理解監控資本主義的起源以及運作邏輯。然後我們會用 資料科學 （Data Science） 的視角來理解監控科技是如何被實現的。你將清楚地了解監控資本主義的商業邏輯是如何跟資料科學緊密耦合的。 你我的數位行為一直都被蒐集與監控 （圖片來源： 《個資風暴：劍橋分析事件》（圖片使用已獲同意） ） 閱讀完本文，你將能用更全面的視角思考這些大哉問： 為何社群網站助長了 2021 美國國會騷亂 ？ 推特是否有權 關閉美國總統川普帳號 ？ 為何 蘋果新的隱私權條款讓臉書大跳腳 ？ 為了讓你更輕鬆地掌握本文架構，我也附上各章節的傳送門（你也可以點擊左側導覽圖例）： 第一章 進入臉書監控你我的時代 第二章 以用戶出發的資訊服務：行為價值再投資循環 第三章 使用者淪為屍塊：發現並汲取大規模行為剩餘 第四章 數據神父手中的演算法與資料結構是生產手段 最終章 21 世紀第 3 個十年的認知革命 建議依序閱讀。 暫時封印手機並以桌筆電閱讀本文或許能幫助你集中注意力 在了解監控資本主義及其利弊後（劇透：長遠的弊遠大於利），我在最終章則會說明你能做些什麼來讓自己身處的數位環境變得更好。在繼續閱讀之前，我想給你個「自我數位專注挑戰」：看你能不能在將本文閱讀完以前都不做以下行為： 不點開任何 App 送給你的通知 不回覆任何電子郵件以及訊息 不滑臉書、推特、IG 和 YouTube（除了文中嵌入的影片） 我得承認這在現代非常不易做到。如果當前的客觀環境不允許，把本文存入書籤或 Pocket 並為自己找個能在 30 分鐘內專注閱讀並思考的地點與時間吧！閱讀完本文並完成挑戰的你將會發現，從中獲益最多的人將是自己。 第一章 進入臉書監控你我的時代 前陣子我在 Dcard 的一個分享會上問了台下的聽眾一個簡單的問題： 有多少人有這樣的經驗：在滑臉書時跳出了某個「準」到不行、燒到你的廣告，讓你相信臉書正在「監視」你的瀏覽網頁與朋友的聊天記錄或是「監聽」你的麥克風？ 毫不意外地，幾乎所有人都舉了手。大家都能想像臉書、谷歌與 推特（Twitter） 等科技公司的廣告以及內容 推薦系統（Recommender system, 在後面章節我們會討論它） 在螢幕的另一頭「監控」著我們數位足跡的事實。 第一次是我在使用 Messenger 時無意間跟朋友提到「奶粉」，接著大腦就被硬生生地植入一個自己從沒聽過、這輩子估計也不會曉得的奶粉商的廣告，無力抵抗。對廣告麻木的人可能覺得「只是個廣告，大驚小怪」。但這實際隱含的是：臉書可以在用戶不知情或是不情願的情況下，透過蒐集其行為數據來預測並 改變 用戶對世界的認知。改變了認知，就能在你我沒有意識到的情況下操弄我們的行為，使得我們的「行為」符合他們與廣告商承諾的「保證結果」：使用者點擊（clicks）。 如果你到現在還不清楚臉書及谷歌是如何實現廣告追蹤的，可以看看 Vox 與發明 Cookie 的 盧・蒙特利 的訪談影片： 臉書以及谷歌如何透過第三方 Cookies 監控並使得商家廣告能無時無刻地追蹤你我 順帶一提，你可以透過鮮為人知的 Facebook 站外動態 功能查看臉書透過上述影片的方法（即 Facebook 像素 ）從網路蒐集到的 你的 行為。資料顯示，臉書目前已經從 235 個應用程式以及網站蒐集到我的數位行為以進行 再行銷（retargeting） ：將我曾拜訪過的網站的產品在我下次「免費」滑臉書時透過其競標系統推出我的「個人化」廣告以誘使我點擊。 截至 2020 年 10 月為止，臉書的每日用戶數達到 18.2 億人 ，意味著每一天全球都會有 24％ 的人口造訪臉書並接受臉書為他們精心打造的「個人化」貼文與廣告。這可是人類歷史上最成功的實時監控系統之一，也超越任何 極權主義 家的瘋狂夢想。 事實上，祖博夫在《監控》裡就表示，這些私營科技公司跟被稱為 數位極權主義 的中國信用系統的差別，只在於前者是以經濟取向發展監控科技，後者則是為了政治權力。很多人忘了這些科技公司存在這世上的最大目的 始終都是最大化 股東價值 ，而不是優化你我的心理健康、減少假新聞傳播、避免社會分裂或是確保民主價值。 《監控資本主義時代》與 2 部討論相關主題的 Netflix 紀錄片 在對一般民眾說明時，臉書與谷歌等科技公司一直以來都將「監控」解釋為科技發展下的必然結果以及實現 個人化（personalization） 的必要之惡（或是否認有這回事）。但近年越來越多的用戶意識到了矛盾：一方面確實感受到這些服務帶給自己的便利之處，另一方面則持續感受到自己的隱私受到侵犯以及無可自拔的科技上癮。 全世界只有兩種產業把客戶稱為「使用者」，一個是毒品，一個是軟體。 ─ 統計學家 愛德華塔夫特 Netflix 劇情紀錄片 《智能社會：進退兩難》（The Social Dilemma） 生動地描述了這樣的情境。透過對自己親手打造的產物敲響警鐘的科技專家，該片探究了社群媒體的危險人為影響，如 陰謀論 的廣泛傳播、青少年心理健康議題以及 政治極化 。 另一部紀錄片 《個資風暴：劍橋分析事件》（The Great Hack） 則探究名為 劍橋分析 的數據公司如何不當地取得 5000 萬名臉書使用者的個資（超過全台灣人口的 2 倍）並在 2016 年美國總統選舉 後成為社群媒體黑暗面的象徵。 接下來的章節在探討監控資本主義時會以《監控》一書的內容為主，上述紀錄片的內容為輔（本文截圖之使用已獲 Netflix 同意）。想保留觀影新鮮感的讀者可以先去觀看這兩部紀錄片再回來閱讀本文（好啦，你可以重新開始專注挑戰）。 第二章 以用戶出發的資訊服務：行為價值再投資循環 文章開頭我以臉書作為當代監控資本家的一個例子，不過事實上依照《監控》一書的描述，谷歌才是監控資本主義的先驅。谷歌之於監控資本主義，如同一百年前的 福特汽車 或 通用汽車 之於工業資本主義。 為了更好地理解監控資本主義，讓我們先把時針倒回到距今超過 20 年前的 1998 年，也就是谷歌成立的那一年。 在谷歌成立時已經有 Mosaic 等瀏覽器讓所有電腦使用者都能連上 全球資訊網 。最初，谷歌也實踐了 資訊 資本主義的承諾，提供免費的線上搜尋服務讓大眾搜尋數位資訊。憑藉著卓越的 搜尋引擎（Search Engine） ，谷歌搜尋的使用者越來越多。僅僅不到一年，谷歌的搜尋引擎就得每天處理 700 萬筆搜尋指令，這些搜尋指令也是使用者與系統互動的主要方式。 每位用戶的每個行動，都視為可進行分析並再次投進系統中的信號。 ─ 范里安, 谷歌首席經濟學家 但之後讓谷歌跟其他搜尋服務競爭者拉開距離的，是谷歌工程師將自家的搜尋引擎轉變成一個能持續學習的遞迴系統。他們透過「使用者與系統互動所產生的行為副產品」來持續改善搜尋結果並刺激產品創新：拼字檢查、翻譯以及語音辨識。 有些人也把「行為副產品」稱作數據廢氣（Data Exhaust），也就是谷歌搜尋指令的附帶數據，比方說搜尋詞彙的拼字、停留時間、點擊模式以及地點。谷歌發現，這些儲存在數據緩存內的行為副產品能用來「回推」出每位使用者的思想、感受以及興趣。被譽為「谷歌經濟學的 亞當・斯密 」的 哈爾・范里安 則表示這是 逆向工程（reverse engineering） ：從使用者的行為數據回推他或她的意圖，提供更相關的搜尋結果。 您的瀏覽器不支援影片標籤，請留言通知我：S 《監控》：行為價值再投資循環 在谷歌搜尋引擎發展的初期，其使用者無意間提供的珍貴行為數據， 全部 都被谷歌拿來提昇服務品質並「全然」回饋給使用者，比方說提升搜尋的精確度與相關性。《監控》作者祖博夫將這種 從用戶出發 的循環稱之為行為價值再投資循環（Behavioral Value Reinvestment Cycle）：行為數據中隱含的價值經過再投資後被用來提升產品或服務品質。 在這個循環裡頭，谷歌跟使用者互相需要，達到一種力量平衡： 用戶需要「使用」谷歌的優秀搜尋功能完成知識獲取的需求 谷歌需要「消耗」用戶創造的價值來改善產品，而該價值可從用戶的行為副產品中汲取 群眾被視為企業營運的唯一目的，跟谷歌使命：「彙整全球資訊，供大眾使用，使人人受惠。」的宗旨完美契合。 谷歌：我們的搜尋原則 （ 圖片來源 ） 當初一切看來都是那麼地美好，令人莞爾。 但如果你是當初在 1999 年投資谷歌並急著想要回本的矽谷創投企業家，可能就笑不出來了。因為值得聲明的是谷歌的這個循環還不是你在 下個章節 會看到的監控資本主義，甚至不是你在大一經濟學碰過的 資本主義（Capitalism） 。谷歌搜尋引擎的這個行為價值再投資循環並無法將資金轉為收益。 資本主義是一齣由每一個投資者、員工、企業家、消費者攜手演出的一場毫無是非道德之分的鬧劇。 ─ 《矽谷潑猴》 雖然此循環概念是效仿之前 蘋果（Apple Inc.） 大紅大紫的 iPod 而來，谷歌並不像蘋果那樣具有 iPod 或數位歌曲。沒有利潤、沒有盈餘，完全沒有可銷售並轉換成收益的「實體商品」。 在谷歌與使用者之間完美的力量平衡下，向使用者收費會帶來極高財務風險，而在未付費的情況下把谷歌的 網路爬蟲（web crawler） 從他人手中取來的索引資訊貼上價格也是非常危險的先例。 在心急的矽谷創投給的壓力、2000 年網路泡沫化的雙重夾擊之下，谷歌終於不得不宣告例外狀態（state of exception），擁抱當初被其貶為最低階業務的廣告。當時由 7 名員工組成的迷你 AdWords 團隊最重要的使命，就是得想辦法用廣告讓公司獲利。 有點出戲，但當年谷歌對廣告態度的劇烈轉變總是讓我聯想到在 《進擊的巨人》 中與惡魔簽下契約的那名少女。其後世代代掌握強大的惡魔之力，在受眾人崇拜的同時遭人厭惡。其巨大的影響力也將引領人類走向不明的未來。 第三章 使用者淪為屍塊：發現並汲取大規模行為剩餘 為了減輕投資者們的焦慮，谷歌宣告例外狀態以擁抱廣告。但該怎麼說服成千上萬名用戶呢？ 為了向大眾合理化新的商業操作，谷歌創辦人 賴利歐・佩吉（Larry Page） 決定不走當年盛行的廣告關鍵字模式，堅持廣告商不能干預關鍵字的選擇：谷歌會替他們選擇關鍵字。谷歌向大眾表示會利用蒐集到的大量數據、運算能力以及科技專長確保如果「搜尋時顯示了廣告」，這些廣告也會保證跟「下搜尋指令的使用者相關」。 Google Ads 即為當年的 Google AdWords （ 圖片來源 ） 換句話說，這意味著谷歌會將特定廣告「對準」特定用戶，而不是讓廣告跟使用者搜尋指令中特定的關鍵字相連。秉持這項原則，谷歌宣稱可以兼顧所有人的顧慮： 投資者角度：穩定獲取利潤並使其成指數增長 廣告商角度：確保自己廣告能被目標用戶點擊 使用者角度：谷歌顯示的廣告會與其高度相關 谷歌與投資者想賺的錢由眾多廣告商提供，對一般使用者來說又能繼續「免費」地使用谷歌搜尋，一切又回歸美好。 谷歌沒有跟用戶說的是：為了達到這個目標，谷歌將利用其敏感的行為資料，將原本只被用來改善搜尋結果的行為副產品拿去針對特定用戶進行 指定目標廣告投放 。而因為這些行為副產品現在除了原本用途（提升服務品質），還額外具備了其他用途（替谷歌與廣告商賺取廣告收益），《監控》作者祖博夫將其稱之為行為剩餘（behavior surplus）。 上一章 看到的行為價值再投資循環（下方菱形）已悄悄地屈服於另一項規模更大、更複雜的商業操作（上方五角形）： 您的瀏覽器不支援影片標籤，請留言通知我：S 《監控》：發現行為剩餘（請繼續閱讀並適時回顧圖中五角形的各元件） 上面這套操作要成功，關鍵不在技術實現，而在於谷歌得暗中「監視」。如果當年谷歌公開跟每位使用者說： 嘿！我為了過活，要拿你的搜尋以及網路上的行為數據去建構你的個人檔案喔！我還會把它拿去跟那些你這輩子沒聽過的廣告商的廣告比對，預估你會點擊哪一個。在你下次搜尋時我就會把廣告顯示出來給你看溜！別忘了點進去瞧瞧：） 基於最基本的隱私考量，我想應該沒有一個理性的用戶會直接點頭同意這筆交易的。 儘管沒跟大眾說白，谷歌與後來的臉書為了目標廣告投放將包含你在內的所有用戶都建檔了。諷刺的是，這檔案裡雖然都是關於你的數位資訊，但卻不是為了你而建立，因此自然也不會讓你存取或是認知到它的存在。《監控》一書將其命名為「影子文本」，相對於你在網路上看得到的所有公開資訊（公開文本）。 在這個商業模式底下，使用者的行為數據仍然會被用來提升服務品質，但是更多的附帶數據被用來建立能夠預測使用者點擊行為的預測產品（Prediction Products）。用了這些能精準預測用戶行為的產品後，谷歌在行為未來市場（Markets in Future Behaviors）讓貪婪的廣告商們即時競價使用者對廣告的曝光（impressions）以及未來的點擊行為。 你現在可以回顧一下剛剛前面看到的五角形。我們會在 第四章 詳細說明有哪些生產手段。 《為廣告指定目標生成使用者資訊》 （ 圖片來源 ） 依據 2003 年谷歌持有的一個具指標性的專利 《為廣告指定目標生成使用者資訊》（Generating User Information for Use in Targeted Advertising） ，谷歌掌握的行為剩餘包含但不限於： 使用者曾瀏覽之網頁 心理變數（別問我谷歌怎麼取得這項的，我也不知道） 瀏覽活動 使用者先前被選定或投放之廣告資訊 使用者是否在瀏覽廣告後出現消費行為 進入 21 世紀第 3 個十年，更新後的名單可能比本文還長了。這些使用者屬性/特質數據交互組合起來，就是專利內所謂的用戶檔案資訊（ U ser P rofile I nformation, UPI）。3 名谷歌頂尖的資訊工程科學家跟其真正的 顧客 ：廣告商們表示，透過 UPI 就不用再玩傳統的廣告猜謎遊戲，不需浪費廣告預算就能夠確保使用者點擊。 而且他們也向顧客說明最大的瓶頸不是技術問題，而是社會層面的隱私問題：使用者可能不願交出自己的資料。但他們也讓顧客安心：他們有辦法在使用者不知情的情況下透過使用者行為推論、假設並演繹出其正確的 UPI 以跟廣告比對。 谷歌將每次點擊之價格，乘以他們估算的使用者點擊廣告之機率，並根據此運算結果，將最好的搜尋位置提供給可能支付最高報酬的廣告商，藉此最大化來自珍貴資產的收益。 ─ 彭博社, 2006 年 要能估算成千上萬的使用者以及廣告組合的點擊機率，需要為每位使用者都建構出具代表性的 UPI。而要具代表性，自然需要蒐集特定使用者的大量行為剩餘。理解了這邏輯，谷歌便從一開始無意間「發現」行為剩餘，逐步演變成無所不用其極地「獵捕」用戶行為數據。而這一切都是為了能更精準地預測使用者的點擊行為。 當年彭博記者筆下的「珍貴資產」，正是谷歌在絕多數用戶不知情的情況下，從其與搜尋系統的互動轉換而來的行為剩餘。 您的瀏覽器不支援影片標籤，請留言通知我：S 用有限的注意力追逐無限的數位內容 （ 圖片來源 ） 至此，新的資本主義變種隨之誕生：谷歌用以致富的行為剩餘即為監控資產，其既為賺取監控收益的原物料，也是監控資本的重要來源。最適合用來描述這套資本累積邏輯的詞彙，自然就是 監控資本主義（Surveillance Capitalism） 了。依據 2020 財年第三季度財報 ，谷歌母公司 Alphabet 的整體營收達到 461.7 億美元（約新台幣 1 兆 3205 億元），而其中廣告收入就佔了 8 成（包含搜尋廣告以及 YouTube 廣告）。 為了最大化其監控收益，谷歌在 21 世紀發展了一系列監控科技以及免費服務，務求讓每位使用者都能像隻小老鼠一樣，持續盯著眼前誘人的數位內容並在轉輪上留下數位足跡。監控資本家想不斷地從我們的數位與實體行為中汲取出不只更大量，還要更多元的行為剩餘。除了搜尋以外，我們還有其他便利的飼料： Gmail ：蒐集你的用字行為 Google 地圖 ：蒐集你在實體世界裡的活動 Google 助理 ：蒐集你的語音資訊 YouTube ：蒐集你所有的觀片記錄與喜好 深諳監控資本主義邏輯的企業家們朗朗上口的台詞： 「數據是新的石油！」 背後真正的意思昭然若揭。 熱門的 Google 產品 說白點，在監控資本主義裡頭實際進行價值交換的主體是谷歌與廣告商。我們這些使用者只是原物料（行為剩餘）抽取以及徵收的目標。因此你可能聽人說過：「如果有什麼東西免費，那真正的商品其實就是你。」 但《監控》一書的作者祖博夫針對這句話有更進一步的說明。她認為谷歌與臉書等監控資本家盜獵我們行為剩餘的程度，恰似盜獵者為了象牙殘殺大象的暴行，扔下我們體內、腦內、跳動心臟內蘊含的意義。她表示前面那句已經過時，事實上： 你才不是商品，你是被捨棄的屍塊，真正的「商品」源於從你人生奪取的剩餘。 ─ 《監控資本主義時代》 在監控資本主義的思維下，要賺取更多的廣告收益，就需要更多的廣告點擊。因此「跟使用者相關」很自然地就跟「使用者點擊」劃上了等號。谷歌創造了新的人類行為預測科學：「點擊物理學」，持續召募最聰明的 AI 人才去優化上述五角形中的 生產工具 ，為的是更精準地預測使用者對個人化廣告的 點擊率（ C lick- T hrought R ate, CTR） 。 一般來說，這些複雜的廣告預測系統的最佳化目標是最大化 CTR，因此並不會關心（也沒有意識）一名特定用戶在點擊某廣告時，內心是憤怒、悲傷還是快樂的。「我其實不在乎你的想法、感受或是靈魂，但我要確保你的（點擊）行為符合我的預測。」這樣的激進想法是監控資本主義從 激進行為主義（Radical behaviorism） 的「以他人觀點客觀地觀察某生物個體在環境中的行為」延伸而來，祖博夫則將其稱為「激進冷漠」。 谷歌透過這樣的想法打造出來的預測產品是所有公司夢寐以求的：能夠保證只要刊登廣告，就會有不錯的效果。一個谷歌能夠向其真正的顧客：廣告商販賣近似確定性（certainty）的市場：只要你給我足夠的錢，我就能用自家的預測產品給你要的保證結果：保證讓負責點擊的使用者買單。 Google Analytics 報表頁面 在絕大多數用戶還不知情的情況下，處於市場循環之外的使用者就已不再是 21 世紀多數科技公司營運的主要目的，淪落成了一個手段，一個為了達成陌生、貪婪廣告商的營利目的的 手段 。以上就是長達 800 頁的《監控資本主義時代》想傳達的核心概念。相信你現在也對其有一定的瞭解了。 對某些人來說，這一切可能聽起來十分地黑暗且令人焦慮。或許還會有人選擇直接視而不見並關掉此頁面。但要馴服一頭野獸，我們得先克服恐懼、直視它的雙眼並嘗試理解。轉頭奔逃很有可能只會讓你更快被其吞噬。 要能建立精準的預測產品，比方說廣告推薦系統，只有大量的行為剩餘是不成氣候的。監控資本家也需要全新的生產手段來打造這些資訊系統。在了解監控資本主義的商業操作以後，讓我們用資料科學的角度看看科技公司是怎麼打造這些神奇產品的。 喔對了！值得一提的是，與谷歌及臉書「冷漠」的廣告競標系統相比，採付費訂閱制的電影推薦服務 Netflix 比較有「人性」，看似接近 以用戶出發的資訊服務 。但一般內容推薦系統仍有其原罪：「無上限」地渴望著用戶的「有限」注意力，因此這類服務（Netflix 上看不完的電影、推特滑不完的動態 ）都存在著讓用戶沈迷於其服務的隱憂。 第四章 數據神父手中的演算法與資料結構是生產手段 《監控》一書將打造預測產品的「全新生產技術」（請回顧 前一章 的五角形）統稱為機器智慧（Machine Intelligence），對應到我們已耳濡目染的資訊科技技術：人工智慧、資料科學、 深度學習（Deep Learning） 以及預測分析。而使用這些技術打造預測模型的資料工程師、資料科學家以及機器學習工程師就是祖博夫筆下的「數據神父」。 用「機器智慧」來概括監控資本主義中的科技很方便，卻也讓前幾章對監控資本家們的「指控」顯得十分抽象。在這章節裡，我會用資料科學的角度描述數據神父們是如何地實作對科技公司來說最重要的預測產品： 推薦系統 。這章內容能讓你更直觀地連接監控資本主義的商業邏輯以及與之掛鉤的資料科學概念。 回到上古時代：基於協同過濾的推薦系統 協同過濾：從用戶與物品的互動矩陣建構用戶興趣與物品屬性嵌入 （ 圖片來源 ） 我在 2015 年完成 Coursera 上的推薦系統課程 時， 協同過濾（collaborative filtering） 仍是初學者必學的推薦方法之一。以 Netflix 的電影推薦場景來說，其原理是透過 矩陣拆解（matrix decomposition） 將所有用戶與所有物品互動的矩陣（電影評分紀錄） $\\mathbf{A}$ 拆解成最具代表性的用戶潛在興趣矩陣 $\\mathbf{U}$ 與物品潛在屬性矩陣 $\\mathbf{V&#94;{T}}$，使得 $\\mathbf{U}\\mathbf{V}&#94;{T} \\approx \\mathbf{A}$。 一旦有了這兩個子矩陣，我們就能預測任意使用者 $u$ 給任意電影 $v$ 的評分（先不考慮 新用戶 ）。該系統只要在用戶登入時將他或她還沒看過的電影的預測分數從高到低排序，並把預測評分前 N 個大的電影顯示給該用戶即可完成最簡單的電影推薦。這邊值得一提的是，任一用戶只能存取自己的評分紀錄（公開文本），而 Netflix 可以存取 截至 2020 年為止其在全球獲得的近 2 億訂閱用戶 的所有評分行為（影子文本），創造了和單一用戶之間龐大的知識與力量鴻溝。 另外在推薦系統領域中，所有人都已習慣將每一名用戶簡化為一個紀錄其喜好的高維向量了。 快轉到基於深度學習的推薦系統王朝 2016 年，Google Play 推薦團隊推出 Wide & Deep 模型 ，整合 邏輯回歸（logistic regression） 以及 深度神經網路（deep neural network） 的優點來做 App 推薦並看到卓越的指標提昇。隨後業界爭相使用此架構（或其變形）來改善自家的推薦系統。此研究也是讓 深度學習 更快普及到推薦系統領域的主要推手之一。 3 分鐘理解谷歌經典的 Wide＆Deep 推薦模型概念 谷歌的論文 用實際的業務成長說明其模型的卓越性。谷歌對當時的用戶進行 線上 A/B 測試 ：讓 1 % 的用戶繼續使用原始版本的 App 推薦系統（對照組），讓另外 1％ 的用戶使用類神經網路加持的 Wide & Deep 模型（實驗組）。實驗結果顯示谷歌可以讓實驗組在 Google Play 多點擊下載 3.9％ 的應用程式。要知道 Google Play 在 2016 年有 550 億下載，到了 2020 年更達到上千億 。以這個規模來評估模型帶來的影響力就十分驚人了。當然，一般用戶不會知曉這場偉大的實驗。 在 App 推薦取得巨大突破後，沒道理不應用到指定目標廣告投放的點擊率預測。你一樣可以把每一個廣告商的廣告視為一個物品，並依據用戶過去的點擊與瀏覽行為去預測他/她點擊每個廣告的機率。搭配無數廣告商的競標預算，谷歌在你搜尋任何東西時顯示他們有信心能讓你點擊的廣告。2017 年史丹佛大學與谷歌的 Deep & Cross Network for Ad Click Predictions 的論文標題就直白地說明了他們是改造 Wide ＆ Deep 模型來做廣告點擊預測。 谷歌的 Deep & Cross Network 被設計來做廣告點擊預測 （ 圖片來源 ） 為了避免失焦，我在這篇文章裡頭不會討論過多技術細節。值得強調的是，這些模型是科技公司與數據神父們的嘔心瀝血之作，但從一般人的角度來看可以說是不知所云。所以我完全可以理解心理學與經濟學背景的祖博夫給這些技術下的評論： 成千上百份研究成果，無論是小成就還是大躍進，全都是那昂貴、複雜、晦澀難懂，而且獨一無二的二十一世紀「生產手段」。 ─ 《監控資本主義時代》 這些生產工具為監控資本家帶來了巨大收益。在 一篇 2017 年的微軟跟 Bing 搜尋相關的廣告論文 開頭就寫著：「對搜尋引擎的收益來說，精準預估廣告的點擊率相當關鍵，就算只有將精準度提升 0.1％，也能增加數億美元的營收。」而他們最後顯示能夠用該論文提出的深度學習推薦模型提升 0.9％ 的離線與線上精準度。真正的卓越成就。 有趣的是，實際打造谷歌這些「生產工具」的是他們大力推行的開源軟體庫 TensorFlow ，也是無數機器學習工程師與資料科學家日夜使用的開發工具。所以現在大家一窩蜂在學的 TensorFlow 以及 PyTorch 可以說是生產工具的生產工具。 以下則是谷歌向開發者（developer）展示如何透過 TensorFlow 打造 Wide & Deep 模型的範例程式碼： estimator = DNNLinearCombinedClassifier ( linear_feature_columns = my_wide_features , dnn_feature_columns = my_deep_features , dnn_hidden_units = [ 256 , 64 , 16 ], ... ) estimator . fit ( ... ) estimator . evaluate ( ... ) 而因為這些開源軟體庫使用上十分便利，讓建構複雜神經網路模型像是在疊疊樂，業界內的工程師有時會自娛娛人： 用開源函式庫建構深度學習模型 （ 圖片來源 ） 更有趣的是，這些終日研讀技術文檔、開發的數據神父們有很大機率並不曉得自己在這樣的商業世界裡頭。 這我們等等還會提到。 一張圖說明監控資本主義與資料科學的聯姻 讀完上節，我相信你已經能夠直觀地將資料科學與監控資本主義的商業邏輯結合了。但我在撰寫此文時一直在想：「有沒有更淺白、更直觀的方式讓我把到目前為止談過的所有概念用一張圖呈現給你看呢？」 人類是視覺的動物。一個可行方案是透過 知識圖譜（knowledge graph） 的想法，將所有重要概念（監控資本主義、推薦系統等）視為一個個的節點（node），並將節點其中的關係視為邊（edge）來繪圖。你要不要猜一猜世上最大的 公開 知識圖譜在哪？沒錯，就是 維基數據（Wikidata） 。我們可以用類似 SQL 的語法即時查詢有提及《監控資本主義時代》作者 肖莎娜・祖博夫（Shoshana Zuboff） 名字的所有維基頁面： SELECT DISTINCT ( concat ( ? itemLabel , \" (\" , ? typeLabel , \")\" ) AS ? name ) ? itemLabel ? typeLabel WHERE { { SELECT ? item WHERE { SERVICE wikibase : mwapi { bd : serviceParam wikibase : endpoint \"en.wikipedia.org\" ; wikibase : api \"Generator\" ; mwapi : generator \"search\" ; mwapi : gsrsearch \"'Shoshana Zuboff'\" ; mwapi : gsrlimit \"max\" . ? item wikibase : apiOutputItem mwapi : item . } } LIMIT 100 } hint : Prior hint : runFirst \"true\" . ? item wdt : P31 | wdt : P279 ? type . OPTIONAL { ? item wdt : P18 ? pic } SERVICE wikibase : label { bd : serviceParam wikibase : language \"zh-hant,[AUTO_LANGUAGE]\" . } } LIMIT 100 有提及《監控》作者名稱的部分維基頁面 （圖片來源： Wikidata Query Service ） 如果你開啟 圖片來源連結 ，就能找到一些跟監控資本主義直接相關的概念： 資本主義 極權主義 精靈寶可夢 GO（還記得谷歌無所不用其極地蒐集你的行為剩餘嗎？） 網路訪客追蹤 Netflix 紀錄片：《智能社會：進退兩難》 圓形監獄 但這樣還不夠，我相信我們能做得更好。我用 Pyvis 打造了一個涵蓋監控資本主義以及資料科學相關概念的知識圖譜（或者說是我的心智圖）。在有了前面幾章的背景知識之後，你可以很輕鬆地解讀這個知識圖譜。要理解 谷歌 、 臉書 以及 推特 是怎麼賺進大把鈔票的，你只需找到下方圖譜中的 使用者 A（你我） 圖標，接著一路順著箭頭探索就能理解監控資本家是如何將你我的 注意力、行為與時間 轉換成 監控收益 的了，magic！ 你也可以自由縮放、拖曳圖譜。 #mynetwork { width: 100%; height:750px; border: 0px solid lightgray; position: relative; float: left; } // initialize global variables. var edges; var nodes; var network; var container; var options, data; // This method is responsible for drawing the graph, returns the drawn network function drawGraph() { var container = document.getElementById('mynetwork'); // parsing and collecting nodes and edges from the python nodes = new vis.DataSet([{\"fid\": \"1DcYj-O2gvG3M-uA65LmeLiB2R5s-oIi_\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u4f7f\\u7528\\u8005.png\", \"img\": \"\\u4f7f\\u7528\\u8005.png\", \"label\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\", \"shape\": \"image\", \"title\": \"\\u8207 \\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u516c\\u958b\\u6587\\u672c\\u003cbr\\u003e- \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"value\": 4}, {\"fid\": \"1DcYj-O2gvG3M-uA65LmeLiB2R5s-oIi_\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u4f7f\\u7528\\u8005.png\", \"img\": \"\\u4f7f\\u7528\\u8005.png\", \"label\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \", \"shape\": \"image\", \"title\": \"\\u8207 \\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"value\": 1}, {\"fid\": \"1DcYj-O2gvG3M-uA65LmeLiB2R5s-oIi_\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u4e0b\\u500b\\u4e16\\u4ee3\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u4f7f\\u7528\\u8005.png\", \"img\": \"\\u4f7f\\u7528\\u8005.png\", \"label\": \"\\u4e0b\\u500b\\u4e16\\u4ee3\", \"shape\": \"image\", \"title\": \"\\u8207 \\u4e0b\\u500b\\u4e16\\u4ee3 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"value\": 1}, {\"fid\": \"1DcYj-O2gvG3M-uA65LmeLiB2R5s-oIi_\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u4f7f\\u7528\\u8005 B\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u4f7f\\u7528\\u8005.png\", \"img\": \"\\u4f7f\\u7528\\u8005.png\", \"label\": \"\\u4f7f\\u7528\\u8005 B\", \"shape\": \"image\", \"title\": \"\\u8207 \\u4f7f\\u7528\\u8005 B \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \\u003cbr\\u003e- \\u8cc7\\u6599\\u5206\\u6790 \", \"value\": 4}, {\"fid\": \"1Twnpn3tPnWQ1U1lxiQGrQEdNkuQfPnMV\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u8c37\\u6b4c\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8c37\\u6b4c.png\", \"img\": \"\\u8c37\\u6b4c.png\", \"label\": \"\\u8c37\\u6b4c\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8c37\\u6b4c \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\\u003cbr\\u003e- \\u5927\\u6578\\u64da\\u003cbr\\u003e- \\u76e3\\u63a7\\u6536\\u76ca\", \"value\": 4}, {\"fid\": \"1nDZChV28WvJTGC4efiYEDbMTuK5jAKgJ\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u81c9\\u66f8\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u81c9\\u66f8.png\", \"img\": \"\\u81c9\\u66f8.png\", \"label\": \"\\u81c9\\u66f8\", \"shape\": \"image\", \"title\": \"\\u8207 \\u81c9\\u66f8 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\\u003cbr\\u003e- \\u5927\\u6578\\u64da\\u003cbr\\u003e- \\u76e3\\u63a7\\u6536\\u76ca\", \"value\": 4}, {\"fid\": \"1MCYSxMfE6MWHtqY8IFnQZntux92YbBGR\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u63a8\\u7279\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u63a8\\u7279.png\", \"img\": \"\", \"label\": \"\\u63a8\\u7279\", \"shape\": \"image\", \"title\": \"\\u8207 \\u63a8\\u7279 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\\u003cbr\\u003e- \\u5927\\u6578\\u64da\\u003cbr\\u003e- \\u76e3\\u63a7\\u6536\\u76ca\", \"value\": 4}, {\"fid\": \"1HKUgh9HtMdqYQmNo-xezOZbjMbqB_KwS\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u653f\\u6cbb\\u529b\\u91cf\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u653f\\u6cbb\\u529b\\u91cf.png\", \"img\": \"\", \"label\": \"\\u653f\\u6cbb\\u529b\\u91cf\", \"shape\": \"image\", \"title\": \"\\u8207 \\u653f\\u6cbb\\u529b\\u91cf \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u76e3\\u63a7\\u8cc7\\u672c\\u003cbr\\u003e- \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"value\": 4}, {\"fid\": \"1s0ZFeDBvNOBHBZtkMX6mu3cGSk8Rt9Zw\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u76e3\\u63a7\\u8cc7\\u672c\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u76e3\\u63a7\\u8cc7\\u672c.png\", \"img\": \"\", \"label\": \"\\u76e3\\u63a7\\u8cc7\\u672c\", \"shape\": \"image\", \"title\": \"\\u8207 \\u76e3\\u63a7\\u8cc7\\u672c \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u653f\\u6cbb\\u529b\\u91cf\\u003cbr\\u003e- \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"value\": 4}, {\"fid\": \"1xyU4R5t5u76FQNFjsTCazVd_Ju2AH7Ji\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6.png\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u6599\\u79d1\\u5b78\", \"value\": 4}, {\"fid\": \"1xyU4R5t5u76FQNFjsTCazVd_Ju2AH7Ji\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6.png\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u76e3\\u63a7\\u795e\\u7236\", \"value\": 1}, {\"fid\": \"1Q2hhS3BwCeL-vM0vGEMjIkQZxFroZP-L\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6a5f\\u5668\\u5b78\\u7fd2\", \"value\": 4}, {\"fid\": \"1Q2hhS3BwCeL-vM0vGEMjIkQZxFroZP-L\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \", \"shape\": \"image\", \"title\": \"\\u8207 \\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u76e3\\u63a7\\u795e\\u7236\", \"value\": 1}, {\"fid\": \"1GT1RL6OXyNTfZ_G4FilwhGRW47hYEJW_\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5927\\u6578\\u64da\", \"value\": 4}, {\"fid\": \"1GT1RL6OXyNTfZ_G4FilwhGRW47hYEJW_\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u76e3\\u63a7\\u795e\\u7236\", \"value\": 1}, {\"fid\": \"15grkRfj5ssYEU21JnQ7LXhL0lAStbvmR\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b\", \"shape\": \"image\", \"title\": \"\\u8207 \\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u7269\\u806f\\u7db2\", \"value\": 4}, {\"fid\": \"15grkRfj5ssYEU21JnQ7LXhL0lAStbvmR\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \", \"shape\": \"image\", \"title\": \"\\u8207 \\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u76e3\\u63a7\\u795e\\u7236\", \"value\": 1}, {\"fid\": \"1W3L1Okw10Exeo0pJn8cP8f4qOTa3Fh6u\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u793e\\u6703.png\", \"img\": \"\", \"label\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"shape\": \"image\", \"title\": \"\\u8207 \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5927\\u4ed6\\u8005\\u003cbr\\u003e- \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\\u003cbr\\u003e- \\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \\u003cbr\\u003e- \\u4e0b\\u500b\\u4e16\\u4ee3\\u003cbr\\u003e- \\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \\u003cbr\\u003e- \\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \\u003cbr\\u003e- \\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \\u003cbr\\u003e- \\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \\u003cbr\\u003e- \\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \", \"value\": 4}, {\"fid\": \"1Fio2I-oArtdVlo5XqisLZN5fKt_RCCD-\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u76e3\\u63a7\\u795e\\u7236.png\", \"img\": \"\", \"label\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"shape\": \"image\", \"title\": \"\\u8207 \\u76e3\\u63a7\\u795e\\u7236 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \\u003cbr\\u003e- \\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \\u003cbr\\u003e- \\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \\u003cbr\\u003e- \\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \\u003cbr\\u003e- \\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \", \"value\": 5}, {\"fid\": \"1SRs4OU-LzdwOrm-0KZIgDUCFN1Oti0uk\", \"font\": {\"color\": \"black\"}, \"group\": \"agent\", \"id\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b\", \"shape\": \"image\", \"title\": \"\\u8207 \\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u667a\\u6167\\u97f3\\u7bb1\", \"value\": 4}, {\"fid\": \"1SRs4OU-LzdwOrm-0KZIgDUCFN1Oti0uk\", \"font\": {\"color\": \"black\"}, \"group\": \"dummy_agent\", \"id\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b.png\", \"img\": \"\", \"label\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \", \"shape\": \"image\", \"title\": \"\\u8207 \\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u76e3\\u63a7\\u795e\\u7236\", \"value\": 1}, {\"fid\": \"11uzyxMNXYo2LosrdTTQg5cayBcB7ENa5\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9.png\", \"img\": \"\", \"label\": \"\\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\", \"value\": 5}, {\"fid\": \"16X6Kj0-o0wWfxw1cMYv5iAVVnx617XP-\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\\u6642\\u4ee3.jpg\", \"img\": \"\", \"label\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\", \"shape\": \"image\", \"title\": \"\\u8207 \\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9\\u003cbr\\u003e- \\u5927\\u4ed6\\u8005\\u003cbr\\u003e- \\u5546\\u696d\\u908f\\u8f2f\\u003cbr\\u003e- \\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\", \"value\": 7}, {\"fid\": \"1iuGbqN7eHutpGvgEPN-NP1p5_MMOMKAe\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5176\\u4ed6\\u8cc7\\u8a0a\\u8cc7\\u672c\\u4e3b\\u7fa9.png\", \"img\": \"\", \"label\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"shape\": \"image\", \"title\": \"\\u8207 \\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5546\\u696d\\u908f\\u8f2f\\u003cbr\\u003e- \\u6578\\u64da\\u5ee2\\u6c23\\u003cbr\\u003e- \\u8cc7\\u6599\\u5206\\u6790 \\u003cbr\\u003e- \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \", \"value\": 1}, {\"fid\": \"1TgHDz43bs5VPFs-k0bvZY9uf0X7W5UE6\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u6578\\u64da\\u5ee2\\u6c23\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6578\\u64da\\u5ee2\\u6c23.png\", \"img\": \"\", \"label\": \"\\u6578\\u64da\\u5ee2\\u6c23\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6578\\u64da\\u5ee2\\u6c23 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"value\": 1}, {\"fid\": \"1tGpWGAlIYPK5Hrr3gdxFwXuZD1s-TWIe\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u5546\\u696d\\u908f\\u8f2f\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5546\\u696d\\u6a21\\u5f0f.jpg\", \"img\": \"\", \"label\": \"\\u5546\\u696d\\u908f\\u8f2f\", \"shape\": \"image\", \"title\": \"\\u8207 \\u5546\\u696d\\u908f\\u8f2f \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\\u003cbr\\u003e- \\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"value\": 5}, {\"fid\": \"1M2Nw_aK01yfaqdUYeZ4jbBeb-WM-tiRq\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b.png\", \"img\": \"\", \"label\": \"\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\", \"shape\": \"image\", \"title\": \"\\u8207 \\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\\u003cbr\\u003e- \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"value\": 5}, {\"fid\": \"1jowkMalc5GjsU9xcWsaYDpP5FJPFdYBj\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u5927\\u4ed6\\u8005\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5927\\u4ed6\\u8005.jpg\", \"img\": \"\", \"label\": \"\\u5927\\u4ed6\\u8005\", \"shape\": \"image\", \"title\": \"\\u8207 \\u5927\\u4ed6\\u8005 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\\u003cbr\\u003e- \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"value\": 5}, {\"fid\": \"1ei0DhBFVx36MBQDgTP8hb228gE7aJNoS\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834.png\", \"img\": \"\", \"label\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"shape\": \"image\", \"title\": \"\\u8207 \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\\u003cbr\\u003e- \\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\\u003cbr\\u003e- \\u76e3\\u63a7\\u6536\\u76ca\\u003cbr\\u003e- \\u76e3\\u63a7\\u8cc7\\u672c\\u003cbr\\u003e- \\u653f\\u6cbb\\u529b\\u91cf\", \"value\": 5}, {\"fid\": \"159XsvGjsmHELLq_Zl7IUQTlL-oRbG1CL\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u884c\\u70ba\\u5269\\u9918\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u884c\\u70ba\\u5269\\u9918.png\", \"img\": \"\", \"label\": \"\\u884c\\u70ba\\u5269\\u9918\", \"shape\": \"image\", \"title\": \"\\u8207 \\u884c\\u70ba\\u5269\\u9918 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"value\": 5}, {\"fid\": \"1BiBTO26BoaljQBtzgWUdV25Zwc_nuZOd\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5.png\", \"img\": \"\", \"label\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"shape\": \"image\", \"title\": \"\\u8207 \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u8a0a\\u79d1\\u6280\\u003cbr\\u003e- \\u8cc7\\u6599\\u5206\\u6790\\u003cbr\\u003e- AB\\u6e2c\\u8a66\\u003cbr\\u003e- \\u8cc7\\u6599\\u79d1\\u5b78\\u003cbr\\u003e- \\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\\u003cbr\\u003e- \\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\\u003cbr\\u003e- \\u884c\\u70ba\\u5269\\u9918\", \"value\": 5}, {\"fid\": \"1WgFi9hD2E2BnOYFRgzscoIg7dQNCSCss\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u76e3\\u63a7\\u6536\\u76ca\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u76e3\\u63a7\\u6536\\u76ca.png\", \"img\": \"\", \"label\": \"\\u76e3\\u63a7\\u6536\\u76ca\", \"shape\": \"image\", \"title\": \"\\u8207 \\u76e3\\u63a7\\u6536\\u76ca \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\\u003cbr\\u003e- \\u8c37\\u6b4c\\u003cbr\\u003e- \\u81c9\\u66f8\\u003cbr\\u003e- \\u63a8\\u7279\", \"value\": 7}, {\"fid\": \"1J7Nrr2Z_3nkJkbHEhDotgqYq_MP1UPip\", \"font\": {\"color\": \"blue\"}, \"id\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6ce8\\u610f\\u529b\\u8207\\u6642\\u9593.png\", \"img\": \"\", \"label\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\\u003cbr\\u003e- \\u8c37\\u6b4c\\u003cbr\\u003e- \\u81c9\\u66f8\\u003cbr\\u003e- \\u63a8\\u7279\", \"value\": 5}, {\"fid\": \"1J7Nrr2Z_3nkJkbHEhDotgqYq_MP1UPip\", \"font\": {\"color\": \"blue\"}, \"id\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6ce8\\u610f\\u529b\\u8207\\u6642\\u9593.png\", \"img\": \"\", \"label\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \", \"shape\": \"image\", \"title\": \"\\u8207 \\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4f7f\\u7528\\u8005 B\\u003cbr\\u003e- \\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"value\": 5}, {\"fid\": \"1Bem-_uetEmwQ3lMsQCtQgrykSYii8_7I\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u8cc7\\u8a0a\\u79d1\\u6280\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u79d1\\u6280.jpg\", \"img\": \"\", \"label\": \"\\u8cc7\\u8a0a\\u79d1\\u6280\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u8a0a\\u79d1\\u6280 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\\u003cbr\\u003e- \\u5927\\u6578\\u64da\", \"value\": 3}, {\"fid\": \"1fiKgB8FjgiWbBPfgDRzlNv0RgtE8hr52\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u5927\\u6578\\u64da\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5927\\u6578\\u64da.jpg\", \"img\": \"\", \"label\": \"\\u5927\\u6578\\u64da\", \"shape\": \"image\", \"title\": \"\\u8207 \\u5927\\u6578\\u64da \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u8a0a\\u79d1\\u6280\\u003cbr\\u003e- \\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b\\u003cbr\\u003e- \\u7269\\u806f\\u7db2\\u003cbr\\u003e- \\u516c\\u958b\\u6587\\u672c\\u003cbr\\u003e- \\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\\u003cbr\\u003e- \\u8c37\\u6b4c\\u003cbr\\u003e- \\u81c9\\u66f8\\u003cbr\\u003e- \\u63a8\\u7279\", \"value\": 3}, {\"fid\": \"19M_rPCDxSOWDWv57HUBYENXgf13oYg47\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u516c\\u958b\\u6587\\u672c\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u516c\\u958b\\u6587\\u672c.png\", \"img\": \"\", \"label\": \"\\u516c\\u958b\\u6587\\u672c\", \"shape\": \"image\", \"title\": \"\\u8207 \\u516c\\u958b\\u6587\\u672c \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\\u003cbr\\u003e- \\u5927\\u6578\\u64da\", \"value\": 1}, {\"fid\": \"12S1tFNxmydt43VeEImIkhI5WGICVIXE-\", \"font\": {\"color\": \"red\"}, \"group\": \"sc\", \"id\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u5f71\\u5b50\\u6587\\u672c.png\", \"img\": \"\", \"label\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\", \"shape\": \"image\", \"title\": \"\\u8207 \\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u884c\\u70ba\\u5269\\u9918\\u003cbr\\u003e- \\u5927\\u6578\\u64da\\u003cbr\\u003e- \\u81c9\\u90e8\\u8fa8\\u8b58\\u003cbr\\u003e- \\u8a9e\\u97f3\\u8fa8\\u8b58\", \"value\": 7}, {\"fid\": \"1AWOBWlCCXl2Ifb6quVqFfLL7NcpTCbKM\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u7269\\u806f\\u7db2\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u7269\\u806f\\u7db2.jpg\", \"img\": \"\", \"label\": \"\\u7269\\u806f\\u7db2\", \"shape\": \"image\", \"title\": \"\\u8207 \\u7269\\u806f\\u7db2 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u5927\\u6578\\u64da\\u003cbr\\u003e- \\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b\\u003cbr\\u003e- \\u667a\\u6167\\u97f3\\u7bb1\", \"value\": 1}, {\"fid\": \"1-HW1XVNKes5x0umFLkQ11lgUiRodun3G\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8cc7\\u6599\\u79d1\\u5b78.jpg\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u79d1\\u5b78 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4eba\\u5de5\\u667a\\u6167\\u003cbr\\u003e- \\u8cc7\\u6599\\u5206\\u6790\\u003cbr\\u003e- AB\\u6e2c\\u8a66\\u003cbr\\u003e- \\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"value\": 1}, {\"fid\": \"1TzQCtWYeJx6e2AVQmxyZm2Qfy4BZv1wm\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u8cc7\\u6599\\u5206\\u6790\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u63a2\\u7d22\\u6027\\u8cc7\\u6599\\u5206\\u6790.jpg\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u5206\\u6790\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u5206\\u6790 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u6599\\u79d1\\u5b78\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"value\": 1}, {\"fid\": \"1TzQCtWYeJx6e2AVQmxyZm2Qfy4BZv1wm\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u8cc7\\u6599\\u5206\\u6790 \", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u63a2\\u7d22\\u6027\\u8cc7\\u6599\\u5206\\u6790.jpg\", \"img\": \"\", \"label\": \"\\u8cc7\\u6599\\u5206\\u6790 \", \"shape\": \"image\", \"title\": \"\\u8207 \\u8cc7\\u6599\\u5206\\u6790 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\\u003cbr\\u003e- \\u4f7f\\u7528\\u8005 B\", \"value\": 1}, {\"fid\": \"18CgvWVkmlTcgAGgaT-vTEb6dmO0nURLI\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"AB\\u6e2c\\u8a66\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/AB\\u6e2c\\u8a66.png\", \"img\": \"\", \"label\": \"AB\\u6e2c\\u8a66\", \"shape\": \"image\", \"title\": \"\\u8207 AB\\u6e2c\\u8a66 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u6599\\u79d1\\u5b78\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"value\": 1}, {\"fid\": \"1N_SqURgDIerhu21IisRZIjjN3kh8hiZV\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u4eba\\u5de5\\u667a\\u6167\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u4eba\\u5de5\\u667a\\u6167.jpg\", \"img\": \"\", \"label\": \"\\u4eba\\u5de5\\u667a\\u6167\", \"shape\": \"image\", \"title\": \"\\u8207 \\u4eba\\u5de5\\u667a\\u6167 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8cc7\\u6599\\u79d1\\u5b78\\u003cbr\\u003e- \\u6a5f\\u5668\\u5b78\\u7fd2\", \"value\": 1}, {\"fid\": \"1Ods_a_SfLaVTSmemeJyMrFl6ZjTAReP_\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6a5f\\u5668\\u5b78\\u7fd2.png\", \"img\": \"\", \"label\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6a5f\\u5668\\u5b78\\u7fd2 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4eba\\u5de5\\u667a\\u6167\\u003cbr\\u003e- \\u6df1\\u5ea6\\u5b78\\u7fd2\\u003cbr\\u003e- \\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b\", \"value\": 1}, {\"fid\": \"1p34c2K-Ieik9vsnPMicmIy2U0FEDBi8v\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u6df1\\u5ea6\\u5b78\\u7fd2.jpeg\", \"img\": \"\", \"label\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\", \"shape\": \"image\", \"title\": \"\\u8207 \\u6df1\\u5ea6\\u5b78\\u7fd2 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u6a5f\\u5668\\u5b78\\u7fd2\\u003cbr\\u003e- \\u63a8\\u85a6\\u7cfb\\u7d71\\u003cbr\\u003e- \\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\\u003cbr\\u003e- \\u96fb\\u8166\\u8996\\u89ba\", \"value\": 1}, {\"fid\": \"1io43NIQPwevrqEalsxfK4xLTMk7fWmFY\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u63a8\\u85a6\\u7cfb\\u7d71\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u63a8\\u85a6\\u7cfb\\u7d71.png\", \"img\": \"\", \"label\": \"\\u63a8\\u85a6\\u7cfb\\u7d71\", \"shape\": \"image\", \"title\": \"\\u8207 \\u63a8\\u85a6\\u7cfb\\u7d71 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\\u003cbr\\u003e- \\u6df1\\u5ea6\\u5b78\\u7fd2\", \"value\": 1}, {\"fid\": \"1iNdWdXjYYXILY3nVR6LBLFmc9hkpoPTW\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u9ede\\u64ca\\u7387\\u9810\\u6e2c.jpg\", \"img\": \"\", \"label\": \"\\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\", \"shape\": \"image\", \"title\": \"\\u8207 \\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u63a8\\u85a6\\u7cfb\\u7d71\\u003cbr\\u003e- \\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"value\": 1}, {\"fid\": \"1H9hLcOkVu5Csuxs9sTNDl5IjBch8x9RM\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406.png\", \"img\": \"\", \"label\": \"\\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\", \"shape\": \"image\", \"title\": \"\\u8207 \\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8a9e\\u97f3\\u8fa8\\u8b58\\u003cbr\\u003e- \\u6df1\\u5ea6\\u5b78\\u7fd2\", \"value\": 1}, {\"fid\": \"1oDe8EXFcP9ClvCaVO3_zzVCe9_ngpJqP\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u8a9e\\u97f3\\u8fa8\\u8b58\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u8a9e\\u97f3\\u8fa8\\u8b58.png\", \"img\": \"\", \"label\": \"\\u8a9e\\u97f3\\u8fa8\\u8b58\", \"shape\": \"image\", \"title\": \"\\u8207 \\u8a9e\\u97f3\\u8fa8\\u8b58 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\\u003cbr\\u003e- \\u667a\\u6167\\u97f3\\u7bb1\\u003cbr\\u003e- \\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\", \"value\": 1}, {\"fid\": \"1s_ZnAIIgkOkxODvVrCqGi1bT2bpIsMX-\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u667a\\u6167\\u97f3\\u7bb1\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u667a\\u6167\\u97f3\\u7bb1.png\", \"img\": \"\", \"label\": \"\\u667a\\u6167\\u97f3\\u7bb1\", \"shape\": \"image\", \"title\": \"\\u8207 \\u667a\\u6167\\u97f3\\u7bb1 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u8a9e\\u97f3\\u8fa8\\u8b58\\u003cbr\\u003e- \\u7269\\u806f\\u7db2\\u003cbr\\u003e- \\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b\", \"value\": 1}, {\"fid\": \"1iRG_NpjA1j8T6vp4lQYD0CFbnvnvupQj\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u96fb\\u8166\\u8996\\u89ba\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u96fb\\u8166\\u8996\\u89ba.png\", \"img\": \"\", \"label\": \"\\u96fb\\u8166\\u8996\\u89ba\", \"shape\": \"image\", \"title\": \"\\u8207 \\u96fb\\u8166\\u8996\\u89ba \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u81c9\\u90e8\\u8fa8\\u8b58\\u003cbr\\u003e- \\u6df1\\u5ea6\\u5b78\\u7fd2\", \"value\": 1}, {\"fid\": \"14Zep8RptfmK60H2a1ehEpws4GP3xMeqh\", \"font\": {\"color\": \"blue\"}, \"group\": \"tech\", \"id\": \"\\u81c9\\u90e8\\u8fa8\\u8b58\", \"image\": \"https://raw.githubusercontent.com/leemengtaiwan/leemengtaiwan.github.io/dev/content/images/\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9/knowledge-graph/\\u81c9\\u90e8\\u8fa8\\u8b58.png\", \"img\": \"\", \"label\": \"\\u81c9\\u90e8\\u8fa8\\u8b58\", \"shape\": \"image\", \"title\": \"\\u8207 \\u81c9\\u90e8\\u8fa8\\u8b58 \\u76f8\\u9023\\u5be6\\u9ad4\\uff1a\\u003cbr\\u003e - \\u96fb\\u8166\\u8996\\u89ba\\u003cbr\\u003e- \\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\", \"value\": 1}]); edges = new vis.DataSet([{\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6fc0\\u9032\\u884c\\u70ba\\u4e3b\\u7fa9\", \"to\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\", \"label\": \"\\u5275\\u9020\", \"to\": \"\\u5927\\u4ed6\\u8005\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5927\\u4ed6\\u8005\", \"label\": \"\\u9810\\u6e2c/\\u6821\\u6e96/\\u5236\\u7d04/\\u63a7\\u5236\", \"to\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"label\": \"\\u9810\\u6e2c/\\u6821\\u6e96/\\u5236\\u7d04/\\u63a7\\u5236\", \"to\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5546\\u696d\\u908f\\u8f2f\", \"to\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5546\\u696d\\u908f\\u8f2f\", \"to\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u8cc7\\u672c\\u4e3b\\u7fa9\", \"label\": \"\\u6700\\u9ad8\\u50f9\\u503c\\u4e4b\\u5546\\u54c1\", \"to\": \"\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"to\": \"\\u6578\\u64da\\u5ee2\\u6c23\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u8a0a\\u79d1\\u6280\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u8a0a\\u79d1\\u6280\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b\", \"label\": \"\\u5f9e\\u4e8b\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\", \"label\": \"\\u77e5\\u66c9\", \"to\": \"\\u516c\\u958b\\u6587\\u672c\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\", \"label\": \"\\u8403\\u53d6\", \"to\": \"\\u884c\\u70ba\\u5269\\u9918\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u7269\\u806f\\u7db2\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b\", \"label\": \"\\u958b\\u767c\", \"to\": \"\\u7269\\u806f\\u7db2\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"to\": \"\\u4eba\\u5de5\\u667a\\u6167\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"to\": \"\\u8cc7\\u6599\\u5206\\u6790\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"to\": \"AB\\u6e2c\\u8a66\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6\", \"label\": \"\\u5f9e\\u4e8b\", \"to\": \"\\u8cc7\\u6599\\u79d1\\u5b78\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u5206\\u6790\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"AB\\u6e2c\\u8a66\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u79d1\\u5b78\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\", \"label\": \"\\u751f\\u7522\\uff06\\u6539\\u5584\", \"to\": \"\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u4eba\\u5de5\\u667a\\u6167\", \"to\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\", \"to\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b\", \"label\": \"\\u5f9e\\u4e8b\", \"to\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u63a8\\u85a6\\u7cfb\\u7d71\", \"to\": \"\\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u4e92\\u52d5/\\u884c\\u70ba\\u9810\\u6e2c\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\", \"to\": \"\\u8a9e\\u97f3\\u8fa8\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u96fb\\u8166\\u8996\\u89ba\", \"to\": \"\\u81c9\\u90e8\\u8fa8\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u667a\\u6167\\u97f3\\u7bb1\", \"to\": \"\\u8a9e\\u97f3\\u8fa8\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u667a\\u6167\\u97f3\\u7bb1\", \"to\": \"\\u7269\\u806f\\u7db2\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b\", \"label\": \"\\u8a2d\\u8a08\", \"to\": \"\\u667a\\u6167\\u97f3\\u7bb1\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u5269\\u9918\", \"to\": \"\\u5168\\u65b0\\u751f\\u7522\\u624b\\u6bb5\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u9810\\u6e2c\\u6a21\\u578b\", \"to\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\", \"to\": \"\\u76e3\\u63a7\\u6536\\u76ca\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09\", \"label\": \"\\u63d0\\u4f9b\", \"to\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u653f\\u6cbb\\u529b\\u91cf\", \"label\": \"\\u5408\\u4f5c/\\u6297\\u8861\", \"to\": \"\\u76e3\\u63a7\\u8cc7\\u672c\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5927\\u6578\\u64da\", \"label\": \"\\u516c\\u958b\", \"to\": \"\\u516c\\u958b\\u6587\\u672c\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u5927\\u6578\\u64da\", \"label\": \"\\u7d2f\\u7a4d\", \"to\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"to\": \"\\u8c37\\u6b4c\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8c37\\u6b4c\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u6536\\u76ca\", \"to\": \"\\u8c37\\u6b4c\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"to\": \"\\u81c9\\u66f8\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u81c9\\u66f8\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u6536\\u76ca\", \"to\": \"\\u81c9\\u66f8\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593\", \"to\": \"\\u63a8\\u7279\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u63a8\\u7279\", \"to\": \"\\u5927\\u6578\\u64da\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u6536\\u76ca\", \"to\": \"\\u63a8\\u7279\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u4f7f\\u7528\\u8005 B\", \"label\": \"\\u63d0\\u4f9b\", \"to\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\", \"to\": \"\\u8cc7\\u6599\\u5206\\u6790 \"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8cc7\\u6599\\u5206\\u6790 \", \"label\": \"\\u63d0\\u5347\\u670d\\u52d9\\u54c1\\u8cea\", \"to\": \"\\u4f7f\\u7528\\u8005 B\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6ce8\\u610f\\u529b\\u3001\\u884c\\u70ba\\u8207\\u6642\\u9593 \", \"to\": \"\\u884c\\u70ba\\u50f9\\u503c\\u518d\\u6295\\u8cc7\\u5faa\\u74b0\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u8cc7\\u672c\", \"label\": \"\\u7af6\\u6a19\", \"to\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u653f\\u6cbb\\u529b\\u91cf\", \"label\": \"\\u7af6\\u6a19\", \"to\": \"\\u884c\\u70ba\\u672a\\u4f86\\u5e02\\u5834\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\", \"to\": \"\\u63a8\\u85a6\\u7cfb\\u7d71\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\", \"to\": \"\\u81ea\\u7136\\u8a9e\\u8a00\\u8655\\u7406\"}, {\"arrows\": \"to\", \"color\": \"blue\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u6df1\\u5ea6\\u5b78\\u7fd2\", \"to\": \"\\u96fb\\u8166\\u8996\\u89ba\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u81c9\\u90e8\\u8fa8\\u8b58\", \"label\": \"\\u7d2f\\u7a4d\", \"to\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u8a9e\\u97f3\\u8fa8\\u8b58\", \"label\": \"\\u7d2f\\u7a4d\", \"to\": \"\\u5f71\\u5b50\\u6587\\u672c/\\u77e5\\u8b58\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u4f7f\\u7528\\u8005 A\\uff08\\u4f60\\u6211\\uff09 \"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u4e0b\\u500b\\u4e16\\u4ee3\"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"to\": \"\\u8cc7\\u6599\\u79d1\\u5b78\\u5bb6 \"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"to\": \"\\u6a5f\\u5668\\u5b78\\u7fd2\\u5de5\\u7a0b\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"to\": \"\\u8cc7\\u6599\\u5de5\\u7a0b\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"to\": \"\\u7a0b\\u5f0f\\u8a2d\\u8a08\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"#848484\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u793e\\u6703/\\u6a5f\\u5668\\u8702\\u5de2\", \"to\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \"}, {\"arrows\": \"to\", \"color\": \"red\", \"font\": {\"color\": \"grey\"}, \"from\": \"\\u76e3\\u63a7\\u795e\\u7236\", \"to\": \"\\u79d1\\u6280\\u8a2d\\u8a08\\u5e2b \"}]); // adding nodes and edges to the graph data = {nodes: nodes, edges: edges}; var options = {\"nodes\": {\"font\": {\"size\": 14}}, \"edges\": {\"color\": {\"inherit\": true}, \"font\": {\"face\": \"tahoma\", \"size\": 14}, \"smooth\": {\"forceDirection\": \"none\", \"type\": \"dynamic\"}}, \"physics\": {\"minVelocity\": 0.75}, \"scaling\": {\"min\": 1, \"max\": 15}}; network = new vis.Network(container, data, options); return network; } drawGraph(); 世上唯一涵蓋監控資本主義與資料科學相關概念的知識圖譜 我希望你在瀏覽此圖譜時不只有一個 Aha moment ：比方說突然對世界的運作方式有了更深的領悟、頓悟資料科學跟監控資本主義的商業邏輯是如何緊密耦合的。如果你富有好奇心，應該會花不少時間瀏覽上圖並從中獲得一些有趣的發現： 紅色代表與監控資本主義相關的概念；藍色是資料科學；黑色則是指相關人物 影子文本是監控資本家的最大生財資產。一般用戶無法存取甚至不知道其存在 資訊科技不等於監控資本主義。後者是一種商業邏輯而前者是後者的生產手段 監控資本圈並不只科技公司牽涉其中。監控資本以及政治力量都會型塑著市場 行為未來市場最終能控制社會，且終將影響一般用戶以及打造它的數據神父們 行為價值再投資循環是一個相對健康的商業邏輯，但仍有前面提過的科技上癮 除了推薦系統， 自然語言處理 與電腦視覺也是監控資本主義中的生產手段之一 沒有藏頭，不用找了。我只是有點強迫症。 再訪影子文本：「我們絕不會賣用戶個資！」 有了我們的知識圖譜以後，上一節最後提到的論述應該都顯得十分直觀。不過我還是建議想深入了解監控資本主義的讀者找時間咀嚼 《監控資本主義時代》 。你可以透過該書深入了解祖博夫筆下的 大他者 以及 機器蜂巢 所代表的意義。 底下我再針對第 2 點提到的 影子文本 給一個實例說明。 如果你沒有在科技公司工作的經驗，可能難以想像祖博夫筆下的 影子文本 長什麼樣子。但對業界的工程師來說，所謂的影子文本自然指的是那些只能在企業內部存取的一筆筆使用者行為日誌（user behavior log）。這些日誌詳細紀錄了 每一個 用戶跟某個系統 / 網站 / App 的 每一次 互動，是萃取 行為剩餘 最好的原物料。科技公司對其隱密的程度讓維基上甚至沒有一個好的說明頁面。下圖則是 微軟亞洲研究院 於 2018 年發布的一篇新聞推薦論文 中出現的案例研究： 某位毫不知情的 Microsoft News 用戶的實際新聞瀏覽行為 （ 圖片來源 ） 在 這篇論文 裡，研究員們描述了他們怎麼使用微軟自有的 Satori 知識圖譜 以及我以前寫過的 注意力機制（attention mechanism） 來為一名隨機抽樣出來的用戶提供更好的新聞推薦（更多點擊 = 更好）：「我們看到注意力神經網路能準確地抓到此用戶喜歡汽車以及政治的喜好。因為在知識圖譜裡頭，通用汽車跟特斯拉距離很近。」 這邊的重點是所有你現在在用的數位服務都在做這件事情：研發最能抓住你注意力的演算法，讓你繼續點擊、往下滑並待在他們的 Apps 上以提升你的互動與參與程度（engagement）。當然，科技公司會跟你說這些私密數據都匿名化了，沒有隱私議題。而且你當初在下載 App 時也已經同意了使用者條款，不是嗎？（儘管沒人會/能仔細看冗長的說明書）。 科技公司監控用戶行為並使用能最大化用戶互動的演算法 （圖片來源： 《智能社會：進退兩難》 ） 透過蒐集成千上萬名用戶的數位行為，科技公司累積了人類史上最龐大的知識量，以此打造能夠緊緊抓住每一位用戶注意力的演算法。這些演算法與推薦系統是將知識轉成權力的最佳體現：「我比你還懂你自己，而且我覺得這個東西很重要，所以你應該馬上點開查看」。 《人類大未來》 作者，歷史學家哈拉瑞就在書中預言未來會是少數菁英統治 AI，AI 再統治絕大多數的人類。應該也有人跟我一樣嗅到經典的反烏托邦作品 《美麗新世界》 的味道：這些菁英怎麼聽起來那麼像阿爾法 $\\alpha $ ？ Github 聯合創立人普雷斯頓・沃納在 接受彭博採訪時也有類似的言論 ： 未來人類可能只會剩下兩種類型的工作：一種是你寫程式告訴機器怎麼做，另外一種則是機器告訴你該做什麼。 ─ Preston-Werner, Github 聯合創辦人 這就是影子文本的強大之處：累積巨大知識並能用來預測並引導底下用戶的未來行為。這是上對下的關係。 懂了這個以後，你就知道外界常常控訴「科技公司將用戶個資賣給第三方！」的這個議題根本打錯方向。一流的科技公司最珍視的就是自己的影子文本，谷歌跟臉書的發言人會跟你說：「我們怎麼可能出賣用戶個資！」而他們沒有說謊。使用者的數據隱私權不是沒了，而只是被科技公司「重新分配」了：雖然這些日誌都是關於你的行為，但我們不會讓你看到，而是自己用來預測你的未來行為。 卡羅爾在 TED 上與她口中的「矽谷神祇」們講述臉書在英國脫歐的角色和它對民主的威脅 2016 年劍橋分析醜聞 的 劍橋分析公司 被英國追蹤記者 卡洛爾・卡瓦拉德爾（Carole Cadwalladr） 揭露其非法取得 5,000 萬筆臉書用戶數據並透過臉書的廣告推薦系統操控易被左右的英國選民，在選民沒意識到的情況下影響他們的脫歐抉擇。 該醜聞的其中一個引爆點正是因帕森設計學院的副教授 大衛凱洛（David Carroll） 問了該公司一個很簡單的問題： 我能看看你握有的我的個人數據嗎？ ─ 大衛凱洛 而一直要到 2020 年，該公司宣告破產的 2 年多以後，大衛凱洛才 終於透過媒體拿到了屬於他自己的那一份「影子文本」 。大多數美國臉書用戶仍然沒親眼見過劍橋分析公司從臉書蒐集到的、那些屬於他們自己的私密數據。 個人化的極致：以個人為單位的平行世界與社會極化 在你想要去暴打你那位被祖博夫稱作「數據神父」的工程師或是資料科學家的朋友之前，讓我幫他/她辯解一下：他們自己可能也沒有意識到背後的商業邏輯。沒有貶低的意味，但在這年代利用最新生產手段（人工智慧、類神經網路）打造出能夠精準預測用戶行為的工程師們，其實就跟當年工廠生產線上的作業員沒差多少。我們都只是經濟體系裡服從資本的勞動力。 左右兩邊出乎意料地相似 只不過當年工廠中的作業員信奉的神諭是「大量生產」，現在的工程師們信奉的是「個人化」的信條。 幾乎所有你能看到的推薦系統論文都會在其研究動機以及背景提到個人化的重要以及必要性。工程師與研究員也早已習慣將「用戶點擊」、「用戶按讚」、「用戶轉發分享」或是「用戶接受通知進入 App」視為數位世界的真理：「這對使用者好」。這些使用者行為要不是 直接 增加企業收益，就是透過提升使用者與系統的互動程度 間接 為企業賺錢。 這就是為何臉書 / IG 有時會貼心地提醒你：「有人在照片上標注你了喔！」卻沒有直接把該照片附在通知裡的原因。在通知裡頭附上照片對用戶很有實質幫助，卻可能讓用戶不需點進 App，進而造成企業的利益損失。 《智能社會：進退兩難》 個人化當然有對用戶的好處及其必要性。不過我們也絕不能忽略極端的個人化會導致《智能社會》裡頭看到的科技上癮。無上限的個人化也會導致我們在 2016 年劍橋醜聞已經看過的社會極化：每個人都只看到自己想看到的。現在的監控資本家們就像是發給全世界每一位用戶一個免費的 意若思鏡 ，其大小就是你手中那只有幾寸的螢幕畫面，投射出所有你內心最渴望卻也最虛幻的慾望，而此單面鏡後不斷有人想著怎麼讓你的大腦再分泌一點多巴胺。 到頭來疑惑的人們會在社群媒體上丟出疑問，質疑跟自己想法不一樣的「他們」到底在想什麼： 為什麼那些人這麼地蠢？你看看我每天看到的臉書與推特動態、谷歌搜尋結果以及微軟新聞內容，難道這些資訊他們都沒看到嗎？ ─ 身處數位時代的你我會有的日常疑問 然後這些疑問還只有願意跟我們互動的同溫層會收得到，多虧了個人化演算法。 人們都會趨吉避凶，本能地排斥不同觀點並喜歡與符合自己觀點的想法/群體互動。這種人類天性被「最大化用戶互動程度以獲利」的社群媒體利用，直接導致了震驚全球的 2021 年 1 月美國國會示威騷亂 ：美國總統川普支持者強行闖入象徵該國民主的國會大廈，試圖推翻 2020 美國總統大選 結果。 用監控資本主義的框架思考，就能理解正是因為社群媒體的商業本質使得「川普將會帶領軍隊推翻選舉結果！」等陰謀論以及極右思想能夠以低廉的成本廣泛散播給數以百萬計的用戶，塑造他們的平行宇宙，最終導致這場悲劇。 BBC 指出早在騷動的前 65 天 就已經能看到陰謀論在臉書上流傳的跡象。以下則是 BBC 新聞攝影師在前線的紀錄 ： 監控資本主義給出的答案很殘酷。沒錯，那些跟你持不同想法的人們每天會看到並互動的個人化內容都跟你看到的不一樣。 而在美國國會大廈遭到衝擊之後，推特宣佈已經關閉了超過 7 萬個與陰謀論有關的賬戶，最後則是美國前總統川普本人的帳號。有些人將其視為左派的勝利，但如果你超越政治光譜，把思考問題的高度放到跟 德國總理梅克爾 一樣的位置，就知道真正需要被大眾討論跟解決的問題應該是： 為何社群媒體可以為了營利在一開始放縱這些陰謀論流竄？為何這些科技巨頭後來可以自己選擇下架美國總統？社群媒體自我審查的基準在哪裡？為何他們有那麼大的權力？民主與人權在哪裡？ 根據美國現行法律《通信規範法案》第 230 條 ，包括推特和臉書在內的社交媒體公司不會因用戶發帖內容而被追責，同時也可以刪除合法但會引起反感的內容。這是俗稱的「避風港條款」。 但我們需要承認新型態的資本主義發展速度太快，現行的法律已經跟不上腳步了。避風港條款是在 21 世紀初期通過，而監控資本主義那時也才剛剛萌芽。時至今日，如果今天推特 CEO 可以凌駕全世界所有法律，關閉有 8,800 萬追蹤者的美國總統帳號，同時宣稱該企業會「自主控管」以及擁有「言論自由」，還有什麼能限制他們呢？ 這些問題值得我們認真思考。 要改善我們的數位社會需要每個人的合作，近年工程師們也正努力地想辦法改善推薦系統的社會面向。幫助用戶脫離個人化資訊的舒適層（comfort zone）、增加推薦內容的多樣性（diversity）以提升用戶滿意度。「誰說我們只想著 CTR！」 雖然老實說是因為不少企業發現要保證用戶能持續地使用自家 App（另個企業指標：長期留存率！）得考慮 CTR 以外的事情，頂尖的推薦系統會議如 RecSys 在 2020 年都出現了更多試圖解決推薦系統對社會帶來的負面影響的研究。RecSys 2020 Keynotes 的主題就分別為 「4 個社群媒體使我們容易被操控的理由」以及「搜尋以及推薦系統的偏差」。 其他我印象比較深刻的近期研究（以下為我自己的翻譯）有： 肩負使命的推薦系統：評估新聞推薦裡頭的多樣性 對抗搜尋及推薦系統裡頭的偏見並提升公平性 PURS: 提升使用者滿意度的個人化非預期推薦系統 以後或許會撰文深入討論相關研究，但現在先讓我們就此打住，總結一下一路上聊了些什麼。 最終章 21 世紀第 3 個十年的認知革命 呼！一路走來，我相信你對自己身處的數位世界已經有更深一層的理解了。你已經知曉了監控資本主義扭曲的過去以及瘋狂的現在，是時候回顧一下我們在開始這趟驚悚旅程之前那段關於未來的訊息了： 監控資本主義在 21 世紀主導資料科學以及 AI 的發展方向，並持續往大多數人不樂見的方向邁進。只有在大眾能夠認知到並理解其商業邏輯，我們才能透過集體意識去改善每個人的數位未來。 ─ 本文主旨 你現在能夠理解這段訊息了。 監控資本主義的火車能這樣不受控制地在我們的數位世界裡橫衝直撞，要歸功於你我長期對監控科技發展的「無知」以及我們對「免費」的錯誤理解。但現在你已經「知曉」了監控科技的實際存在，也能明白一直以來的「免費使用」交易的是你在數位時代越來越寶貴的「注意力」。 認知到這些事實是任何偉大改變的開始。 在數位時代最重要的能力是能不被大量無用資訊分心的專注眼神 在這篇文章裡，我近乎囉唆地以各種視角與實例為你重現由（1）監控資本主義的商業邏輯以及（2）強大的資訊科技共同打造的數位世界，幫你從頭構築出一個能夠合理解釋這瘋狂年代的世界觀。事到如今我們已經無法成為拒絕使用數位服務的 盧德主義者 ，但我們能做的是重新找回自我。 資訊的豐富意味著它消耗著資訊接受人的注意力導致其變得稀缺。因此資訊之豐富創造了注意力之貧窮。資訊接受人需要在過剩的資訊來源中有效地分配其注意力。 ─ 赫伯特・亞歷山大・西蒙, 人工智慧之父 認知到注意力的珍貴以及數位服務對其之無限的渴望後，你就能開始做些改變。 你應該時時刻刻問自己： 「真的有需要一直往下滑臉書、推特或是 IG 嗎？沒有其他更重要的任務嗎？」 「App 通知來了，真的有需要現在、立刻、馬上打開查看嗎？」 「能不能改掉一看到手機螢幕亮就反射性去看是什麼通知的壞習慣？」 「當下這個時間點除了消耗數位內容，對我來說最重要的事情是什麼？」 「這篇貼文讓我情緒激動想分享轉貼留言，但它的內容可信嗎？」 「App 推薦的內容讓我很想點進去看，但那是我人生最高順位的事情嗎？」 「我真的需要邊吃飯邊聽 Podcast ，不給大腦自主思考的時間嗎？」 「我一天有幾分鐘是真的能夠靜下心思考重要問題而不是無腦觀看影片？」 沒人會在意你往自己腦中塞了什麼，你的思想與行動才是證明自己最大價值的事物。你可以試試數位內容的間歇性斷食，拿掉你的數位奶嘴，不要再讓那些無時無刻吵著「你的朋友標注了你！」「你的訂閱者出了新影片！」這種為了爭奪你寶貴注意力的「便利通知」而疏忽了你人生中最重要的事情了。 如果你也想要為世界創造些價值，就盡可能地專注吧！我們不該整天都把寶貴的注意力以及大腦資源拿來消耗「他們」要我們馬上查看的數位內容。從今天開始為自己決定優先順序吧。 我也希望你能幫忙把這些知識傳達給對你來說重要的人知道 。當越來越多人意識到真正重要的事情並改變行為，就能創造一股集體意識讓企業們做真正對我們有幫助的事情。監控資本主義的商業邏輯讓我們每個人有不同的現實（reality）進而各個擊破。透過這篇文章我希望能跟所有人一起建立一致的現實，並透過這股力量改善我們以及下一世代的數位未來。 除了重視自己的注意力並將其視為最重要的資產，所有人都應該開始向數位霸權宣示在這個數位時代裡，我們的新人權也包含了自己的數據權。行政院數位政委 唐鳳也建議我們應該要把自己的數據權當作基本人權 。 如果你是其中一名數據神父，別忘了能力越強，責任越大。我們得開始理解用戶的意圖（intention）並幫助他們解決真正的問題，而不是整天抓著他們的注意力（attention）。《個資風暴》裡頭我們看到數據分析人員如何因為看到廣告投放的實際 KPI 成長而忘了背後都是一名名真實活在世上的人類。就像在 《真確》 裡瑞典公衛教授漢斯・羅斯林跟我們說的： 我要你看到統計數據背後的個別故事，也要你看到個別故事背後的統計數據。不靠數據無法了解世界，但光靠數據也無法了解世界。 ─ 漢斯・羅斯林，《真確》 機器學習工程師以及資料科學家並不只是人型的「最佳化機器」。數據神父們得要承認人類的注意力、時間都是有限的，認識到人類本身的認知極限，並把這些因素考量進去決定演算法應該要最佳化什麼，而不只是去最佳化 CTR。 如果你是企業家或是正打算創業，多花點心思打造更符合「人性」的數位服務吧！這雖然不是一件容易的事情，但只要你成功了，我想全人類都會支持你的產品。 雖然在這篇我延續《監控資本主義時代》的思路抨擊谷歌、臉書以及推特等科技公司，但他們的服務也實際為社會做了不少貢獻。只是這樣還不夠，我希望能喚起大眾的集體認知，讓這些我們熱愛的數位服務往真正對人類未來發展有助益的方向邁進。如果他們宣稱拋棄自我是科技進步與數位化的必要代價，我想就是我們該為自己發聲的時候了。 讓我們修正監控資本主義創造的數位現實，建立以人為本並符合人體工學的數位未來。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-ai-and-our-digital-future-in-the-age-of-surveillance-capitalism.html","loc":"https://leemeng.tw/data-science-ai-and-our-digital-future-in-the-age-of-surveillance-capitalism.html"},{"title":"世上最生動的 PCA：直觀理解並應用主成分分析","text":"在這個萬物皆向量的時代，能夠了解事物本質的數據處理能力變得前所未有地重要。 主成分分析（ P rincipal C omponent A nalysis, 後簡稱為 PCA） 在 100 年前由英國數學家 卡爾·皮爾森 發明，是一個至今仍在機器學習與統計學領域中被廣泛用來 分析資料、降低數據維度以及去關聯 的 線性降維 方法。 因為其歷史悠久且相較其他降維手法簡單，網路上已有不少優質的 機器學習課程 以及 部落格 探討其概念。在這篇文章裡，我則將透過 Manim 動畫、 NumPy 以及 scikit-learn ，跟你一起用這世上最直觀的角度重新體會 PCA 之美以及其背後關鍵的 線性代數（Linear Algrbra） 與 統計（Statistic） 精神。 您的瀏覽器不支援影片標籤，請留言通知我：S 我們後面會看到將共變異數矩陣 $\\mathbf{K}$ 視為一線性轉換並套用到數據 $\\mathbf{X}$ 上會增強其變異趨勢 閱讀完本文，我相信你將能夠： 直觀且正確地理解 PCA 並欣賞其背後美麗的數學概念 了解如何運用 PCA 分析個人遇到的問題或是企業數據 具備能夠深入了解相關機器學習領域的基礎 希望你跟我一樣迫不及待地想要開始了！在正式踏上這趟旅途之前，我想說明一下你需要事先預習（或複習）的知識。另外因為本文內的動畫皆為黑底，我強烈推薦點擊左下按鈕以暗色模式繼續閱讀。 一些你需先具備的基礎知識 鑽研學問的這條道路之上，就連帝王也沒有捷徑可循。 如同以往文章，為了讓美麗的知識能夠散播到最遠的地方，我會盡可能地平鋪直述 PCA，以期能讓閱讀門檻被降到最低。我會用不少動畫帶你直觀理解 PCA 的本質，而不只是丟個公式給你，或是教你怎麼用機器學習函式庫的 API。 話雖這麼說，你仍需具備高中或大一程度的線性代數基礎。如果你 完全沒有 修過任何線代課程，我會強烈建議你先去觀看 3Blue1Brown 的線性代數本質 。這是世上最好的線代入門教學，能讓你對以下概念有最直觀的理解： 線性組合 Linear Combination 線性轉換 Linear Transformation 點積 Dot Product 基底變更 Change of Basis 特徵向量/值 Eigenvectors 與 Eigenvalues 本文則可以幫助你把基礎的線代知識無縫接軌地與 PCA 連結，並學會如何將 PCA 運用在真實世界的數據。如果你要更扎實、更正式的線性代數基礎課程，我會大力推薦 MathTheBeautiful 以及 Mathematics For Machine Learning 。 一個好的教學者會引導你去找到最好的學習資源。 我的上一篇文章， 給所有人的深度學習入門：直觀理解神經網路與線性代數 也用了大量動畫說明矩陣相乘（Matrix Multiplication）、線性轉換以及神經網路（ Neural Network） 與線代之間的緊密關係，建議事先閱讀。本文會聚焦在 PCA 身上。未來有時間的話，我會撰文說明 PCA 跟 深度學習 領域中的 Autoencoder 之間的美妙對應關係。想要先睹為快的讀者稍後可以觀看台大電機李宏毅教授的 PCA 課程 以及圖靈獎得主 Geoffrey Hinton 的 From PCA to autoencoders 。 您的瀏覽器不支援影片標籤，請留言通知我：S 《給所有人的深度學習入門》一文展示了神經網路解決二元分類的過程 我接著會假設你已將 上篇文章 以及 3Blue1Brown 的 影片 看過一遍，或是至少已了解剛剛提到的幾個基本線代概念。當然，我懂你想要「省時間」直接閱讀的心情，而你也完全可以這樣做！不過如果你等等發現自己的線代基礎不是那麼穩固，我會建議回到本節打好底子，或是點擊我在當下附的連結複習相關概念。另外，如果你只是被文章封面吸引過來，想要馬上看到用 PCA 分析線上遊戲 《英雄聯盟》 的案例，可以先跳到 踏入荒野：實際應用 PCA 來解析真實數據 一節。 萬丈高樓平地起，打好地基真的很重要。但現在假設你已經準備完畢，讓我們開始這趟 PCA 的深度探索之旅吧！ 世上最簡單的降維：給我一個數字就好！ 我們前面提到 PCA 可以用來有效地降低數據維度。跟 上篇文章討論過的二元分類 不同， 降維（Dimensionality Reduction） 是一種 無監督學習 ，其最主要的目的是「化繁為簡」：將原本 高維 的數據（比方說 $N$ 維）重新以一個 相較低維 的形式表達（比方說 $K$ 維，且 $K < N$）。理想上只要該 $K$ 維的表徵（representation）具有代表性，能夠抓住原來 $N$ 維數據的大部分特性，我們就能在沒有損失什麼資訊的情況下，用更簡潔的方式呈現該組數據，進而對其本質有更深的理解。 數學的本質不是將簡單的事情變複雜，而是將複雜的事物簡化。 ─ Stan Gudder 現在想像你這幾天都在重複進行一項複雜的實驗，好不容易取得了 20 個有效樣本（samples）。你將這些觀測值（observations）以一個 矩陣（matrix） 的形式存了下來。該數據矩陣 $X$ 裡頭的每一行（column）代表著一個特定樣本，而每一列（row）則代表著一個特定 特徵（ f eature） $f$ 的觀測結果： \"\"\" 為了教學目的，使用 NumPy 隨機初始化一些 demo 用的數據 現在你只需要關注最後生成的矩陣 `X` 即可（下一節我們會討論其他細節） \"\"\" import numpy as np from numpy.testing import assert_almost_equal # 顯示小數點下兩位，保持畫面簡潔 np . set_printoptions ( precision = 2 ) # reproductivity, 讓你可以跟著重現此 demo 數據 # 我也鼓勵你實際執行範例程式碼以內化本文概念 rng = np . random . RandomState ( 1 ) # 初始化實驗數據並減去平均（文後會說明為何要減去平均） W = rng . rand ( 2 , 2 ) X_normal = rng . normal ( scale = 5 , size = ( 2 , 20 )) X_orig = W @ X_normal # @ 就是你學過的矩陣相乘運算 X_mean = X_orig . mean ( axis = 1 )[:, np . newaxis ] X = X_orig - X_mean mean = X . mean ( axis = 1 ) # 測試 numerical 相等，確保樣本的平均已經為 0 # 這類數值測試在你需要自己實作 ML 演算法時十分重要 assert_almost_equal ( 0 , mean ) print ( 'X.shape:' , X . shape , ' \\n ' ) print ( X ) X.shape: (2, 20) [[ 2.89 0.32 5.8 -6.52 3.94 -4.21 0.45 2.14 1.3 -4.98 -2.4 -3.1 0.69 -1.59 -3.64 -0.24 6.81 4.63 -2.24 -0.06] [ 1.52 0.91 1.52 -0.88 -0.03 -1.26 -0.25 0.96 -0.89 -0.45 -0.88 -1.12 -0.86 0.13 -1.53 0.51 2.66 1.28 -0.14 -1.19]] 注意在這個例子裡，我們是怎麼解讀數據矩陣 $\\mathbf{X}$ 的： $\\mathbf{X}$ 維度為 (n_features, n_samples) 每一行（column）代表著一個特定的樣本 $x$ 每一列（row）則代表某特徵 $f$ 的所有觀測值 在矩陣 $\\mathbf{X}$ 中，每個樣本 $x$ 都是一個 2 維的行向量（column vector）。要取得第一個樣本 $x_{1}$： \"\"\" 每個樣本為一個 column vector，索引從 0 開始 第一個 「 : 」 代表取得所有對應的 rows \"\"\" X [:, 0 ] array([2.89, 1.52]) 除了 NumPy 比較特別以外，有實際用過 scikit-learn 、 PyTorch 或是 TensorFlow 做過矩陣運算的讀者們應該都清楚，實作上這些函式庫常會將數據矩陣 $X$ 做 轉置（transpose） ，使其維度變成 (n_samples, n_features) 。這樣的好處是每一個列向量（row vector）都直接對應到一個樣本。這使得我們可以更輕鬆地存取特定樣本： \"\"\" 跟上例一樣取出第一個樣本。 因第一維度為 n_samples ，只需要一維索引即可 \"\"\" # sanity check assert_almost_equal ( X [:, 0 ], X . T [ 0 ]) X . T [ 0 ] array([2.89, 1.52]) 但缺點就是所有對 $\\mathbf{X}$ 做的轉換也都得跟著轉置，導致有些本來很簡單的線性轉換變得不那麼地直覺： $$ \\begin{align} \\mathbf{ABCX} \\rightarrow (\\mathbf{ABCX})&#94;\\top \\rightarrow \\mathbf{X&#94;\\top C&#94;\\top B&#94;\\top A&#94;\\top} \\end{align} $$ 因此雖然多數 Python 函式庫實作上會先轉置數據 $\\mathbf{X}$ 再對其進行矩陣運算，為了與一般線性代數文獻中的慣例一致，本文皆以 行向量 （column vector）來表示每個樣本 $x$。事實上，你可以把行向量想像成是一個 $m\\times 1$ 的矩陣： $$ \\begin{align} x = \\begin{bmatrix} a_{11}\\\\ a_{21}\\\\ ...\\\\ a_{m1} \\end{bmatrix}, a_{i1} \\in R \\end{align} $$ 本文前半皆以 $m=2$ 作為例子。這樣表達 $x$ 的好處是你不需把以前學過的矩陣相乘再做一次轉置。等內化本文概念以後，你自然可以輕鬆地切換數據 $X$ 與其轉置 。為了幫助你進入狀況，底下展示如何取得數據 $\\mathbf{X}$ 裡一或多個特定樣本 $x$： 您的瀏覽器不支援影片標籤，請留言通知我：S 是的，在 資料分析 以及機器學習領域裡矩陣索引（indexing）十分基本，基本到你可能沒想過竟然會有人特地為此做動畫。不過我想現在不論背景，你應該都可以在腦中想像如何操作數據 $\\mathbf{X}$ 並取得特定的樣本了。這很重要，所以你得讓它變成直覺反應。在 淺談神經機器翻譯 裡，我們也運用相同的索引方式存取高達 4 維的批次（batch）詞向量數據。 所見即所得，我們可以再次確認同樣的索引可以得到跟動畫中相同的內容： X [:, : 6 ] array([[ 2.89, 0.32, 5.8 , -6.52, 3.94, -4.21], [ 1.52, 0.91, 1.52, -0.88, -0.03, -1.26]]) 了解如何操作 $\\mathbf{X}$ 後，讓我們再次回到降維的話題。 現在想像你興沖沖地跑去見指導教授，迫不及待地獻上你剛搜集到的熱騰騰數據 $\\mathbf{X}$。教授僅看了一眼便道： 兩個特徵有點多，你能不能想辦法只用一個特徵來表示這些樣本的特性？ 你連忙點頭稱是，接著便離開教授的辦公室。回到螢幕前，你盯著 $\\mathbf{X}$ 裡頭的這些數字 #越想越不對勁。到底要怎樣才能把這些 2 維向量 $x$ 各自用 一個 新的數值表示，同時又能保持這些樣本的 特性 不變呢？ 我們都是視覺動物。你可能會想嘗試視覺化（visualize）手中這些數據，看看是否有什麼顯而易見的幾何線索。 那要怎麼視覺化數據 $\\mathbf{X}$ 呢？在觀察到這些樣本只有 兩個 特徵值以後，我想絕大多數人的第一個念頭都是：選擇 一個 最為熟悉、地位崇高的 笛卡爾座標系統(Cartesian coordinate system) ，接著將每個樣本 $x_i$ 裡頭的第一特徵 $f_1$ 與第二特徵 $f_2$ 分別視為此系統裡的 x 與 y 座標，描繪其對應的幾何向量 $\\vec{x_i}$： 您的瀏覽器不支援影片標籤，請留言通知我：S 沒錯，僅僅是將看似毫無章法的數據 $\\mathbf{X}$ 描繪在這個座標系統上面，我們就能透過與生俱來的幾何直覺預測兩特徵 $f_1$ 與 $f_2$ 之間存在著某種程度的 線性關係 。這是幾何觀點上的一大勝利。這個發現讓我們離降維的目標近了許多。 但熟悉 Matplotlib 的讀者應該都明白，我只需用兩行程式碼就可以畫出 $\\mathbf{X}$，根本不用花那麼多時間製作這個動畫： import matplotlib.pyplot as plt plt . scatter ( X [ 0 ], X [ 1 ]) 你是對的。會花那麼多工夫是因為事實上我最想傳達的訊息是： 重新學習你認為自己已經學會的知識。你將發現自己能獲得更多。 因為我最想要你關注的並不是 $\\mathbf{X}$ 的分布，也不是最後幾秒那條顯而易見的斜線。我要你看的是最初一秒構成這個座標系統的兩個 基底向量 $\\hat{i}$、$\\hat{j}$。最關鍵的是理解每個樣本的 $(x, y)$ 座標是怎麼被生成的。首先動畫裡選擇了一組你習以為常的 $\\Re&#94;2$ 基底： $$ \\begin{align} \\hat{i} = \\begin{bmatrix} 1\\\\0 \\end{bmatrix}, \\hat{j} = \\begin{bmatrix} 0\\\\1 \\end{bmatrix}, \\Re&#94;2 = span[\\hat{i}, \\hat{j}] \\end{align} $$ 接著在 給定這組基底的前提 下，我們一步步找出前 4 個樣本對應的 $(x, y)$ 座標。換句話說，將每個樣本 $\\vec{x}$ 表達成這組基底的 線性組合（Linear Combination） 。每個樣本的 $(x, y)$ 座標是這樣被找到的： $$ \\begin{align} \\vec{x} &= \\begin{bmatrix} f_1\\\\f_2 \\end{bmatrix}\\\\ & = x \\begin{bmatrix} 1\\\\0 \\end{bmatrix} + y \\begin{bmatrix} 0\\\\1 \\end{bmatrix}, x, y \\in \\Re\\\\ & = f_1 \\begin{bmatrix} 1\\\\0 \\end{bmatrix} + f_2 \\begin{bmatrix} 0\\\\1 \\end{bmatrix}\\\\ & = f_1 * \\hat{i} + f_2 * \\hat{j} \\end{align} $$ 在線性代數的世界裡，我們常會把基底向量 $\\{\\vec{i}, \\vec{j}\\}$ 稱作標準基底 $\\mathbf{B}_{standard}$，而剛剛動畫裡出現的那些 $(x, y)$ 座標則是該基底的其成分表徵（Component Representation）。你可以把 $\\{\\vec{i}, \\vec{j}\\}$ 當成是 2 維 向量空間（Vector Space） 裡頭的基礎成分（Component）並將所有處在該空間裡的樣本 $\\vec{x}$ 表示成這 2 個成分的線性組合。因此要為數據 $\\mathbf{X}$ 取得一組成分表徵並將其繪製在對應的座標系統上，事實上你得先選擇一個 基底（Basis） 。 以上例而言， $\\mathbf{X}$ 的成分表徵之所以那麼好找，好找到你根本不覺得你有選擇 $\\mathbf{B}_{standard}$，是因為它太符合直覺了！如果你把其中一個基底向量換掉，找出來的 $(x, y)$ 座標就不再是 $(f_1, f_2)$ 。你可以試試用下面這組基底 $\\mathbf{B}_{pc}$ 來建構 $\\mathbf{X}$ 的 $(x, y)$ 座標： $$ \\begin{align} \\vec{b_1} = \\begin{bmatrix} 0.97\\\\0.25 \\end{bmatrix}, \\vec{b_2} = \\begin{bmatrix} -0.25\\\\0.97 \\end{bmatrix}\\\\ \\Re&#94;2 = span[\\vec{b_1}, \\vec{b_2}] \\end{align} $$ $\\vec{b_1}$ 與 $\\vec{b_2}$ 因為彼此 線性獨立（Linearly Independent） 且可以 span 出 $\\Re&#94;2$ ，是個合法的基底。但我想你會同意，如果我們選擇的是這組基底，要得到每個樣本的成分表徵就不再容易了。這是為何多數人在呈現數據時都會下意識地選擇 $\\mathbf{B}_{standard}$，因為它實在是太好用了。這也讓不少人從小到大沒考慮過其他基底，自然也不會挑戰 $\\mathbf{B}_{standard}$ 的權威。但 PCA 以及大半的線性代數文獻都在強調： 所有基底生而平等。但最後，你的應用情境與目的決定了哪組基底比其他的選擇好。 事實上如果掌握了這個概念，你從 PCA 的全名：主成分分析（Principal Component Analysis）就已經能用非常宏觀的角度理解它的終極目標了：找出一組最能代表你手中數據的主成分（Principal Components），並以此為基底重新得到數據的成分表徵。這個新的成分表徵能為數據降維、去關聯並幫助我們理解數據本質。 而我也可以跟你保證，就算 $\\mathbf{B}_{standard}$ 再怎麼地好用，大多數情況下它都不會是你手中數據的主成分。這概念是如此地重要，讓我想叫你拿出螢光筆畫上 100 遍。沒錯，你已經了解 PCA 的核心思想了！接下來我還用不少動畫以及不同的視角帶你多次體會這個道理。 另外你可能有猜到，我剛剛舉例的 $\\mathbf{B}_{pc}$ 正是數據 $\\mathbf{X}$ 的主成分。我們很快就會看到它跟 $\\mathbf{B}_{standard}$ 的區別、為何它們特別以及如何找出它們。上面的動畫中也依序展示了前 4 個樣本 $\\vec{x_i}, i = 1, 2, 3, 4$ 在 $\\mathbf{B}_{standard}$ 下的成分表徵，你可以自己對照一下： X [:, : 4 ] array([[ 2.89, 0.32, 5.8 , -6.52], [ 1.52, 0.91, 1.52, -0.88]]) 你也可以輕鬆地用 Matplotlib 在 2 維平面上繪製數據 $\\mathbf{X}$： import matplotlib.pyplot as plt plt . style . use ( 'seaborn' ) # 第一個參數為所有的 xs, 第二個參數為所有的 ys plt . scatter ( X [ 0 , :], X [ 1 , :]) plt . axis ( 'equal' ); 真的，你不得不承認用這個座標系統來呈現 $\\mathbf{X}$ 的感覺真好！但你很快就會發現，取決於應用情境，你認為萬能的標準基底 $\\mathbf{B}_{standard}$ 並不真的萬能。很明顯地，觀測到的數據裡存在著線性關係。在有了這個發現之後，你的下個直覺反應可能是把所有樣本 $\\vec{x}$ 都投影到 span 出該條直線的 單位向量（Unit Vector） $\\vec{v}$ 上。這是因為你知道 投影等於讓兩向量做點積（Dot Product） ，而每個樣本 $\\vec{x}$ 各自跟 $\\vec{v}$ 點積後所得到的 那一個 特別的數值 $l$，或許就是教授想要你讓他看的那一個數字！ 現在假設我們已經有個合適的單位向量 $\\vec{v}$，則你可以把所有的 $\\vec{x}$ 這樣投影到 $\\vec{v}$ 所 span 出來的直線： 您的瀏覽器不支援影片標籤，請留言通知我：S 簡單而美麗，對吧？你剛剛親眼目睹了這世上最簡單的降維過程，並了解 2 維空間的數據是如何被投影到一維空間上的。我相信動畫勝過千言萬語，但這邊仍有些概念值得強調： 將原空間中的任一向量投影到某低維子空間，事實上就是在線性地降低其維度。 在此例中，向量 $\\vec{v}$ 的所有線性組合 $k\\vec{v}, k \\in \\Re $ 自成一個在 2 維空間 $\\Re&#94;{2}$ 裡的 1 維 線性子空間（Linear Subspace） $\\Re&#94;{1}$。1 維的意思是說，本質上你只需要 1 維資訊就能描述該子空間（數線）上的任一點，儘管每個點都有自己的 2 維座標。這是因為給定一個向量 $\\vec{v}$，你只需要一個實數 $k$ 作為該向量的 scaler，就能用 $k\\vec{v}$ 描述該線上的任何位置，多虧了線性組合。將所有樣本 $\\vec{x}$ 投影到該子空間以後，自然 $\\vec{x}$ 的維度也從 $\\Re&#94;{2}$ 降到 $\\Re&#94;{1}$。 我們在本節最後會看到，向量 $\\vec{v}$ 本身的兩個維度值事實上就已經隱含了描述數據 $\\mathbf{X}$ 特性時所需的大半資訊，因此只要將每個樣本 $\\vec{x}$ 重新表示成向量 $\\vec{v}$ 的一個對應倍數（scaler）即可。 事實上這就是線性降維與 PCA 的核心精神：將原始數據拆解成更具代表性的主成分，並以其作為新的基準，重新描述數據。 線性代數裡充滿著這種關於 矩陣拆解（Matrix Decomposition） 的描述。不過別擔心，等等的動畫會讓你有更深刻的體會。另外別忘了我們之所以一開始會想把數據 $\\mathbf{X}$ 投影到這個 1 維子空間（直線）上，是因為依據我們的幾何直覺（視覺上的判斷），這些 2 維樣本 $\\vec{x}$ 的背後似乎有 一個 被隱藏起來、沒辦法馬上被寫出來的隱藏特徵 $l$ （latent feature）。 比起使用原來的兩特徵 $f_1$ 與 $f_2$ 來表示一個樣本 $\\vec{x}$，我們相信特徵 $l$ 可以被用來更精準且簡潔地描述這些樣本的特性（畢竟這是降維的主要目的）。透過投影到 $\\vec{v}$ 所在的一維空間，我們能輕鬆地得到所有樣本的新特徵 $l$。 在有單位向量 $\\vec{v}$ 的情況下，你可以輕易地做到這件事情： # 該直線的單位向量（顯示到小數後第兩位） v = np . array ([ 0.9691344 , 0.246533 ]) print ( \"v :\" , v ) # shape: (2,) assert_almost_equal ( 1 , np . linalg . norm ( v )) # 使用 v 建立投影矩陣 Ｐ1 # 因為 P 是將 X 投影到 1 維，因此加個 1 在後面 P1 = v [ np . newaxis , :] # shape: (1, 2) print ( \"P1 :\" , P1 ) # 利用 P1 將數據 X 投影到 v 所在子空間 L = P1 @ X # 前 4 個樣本的新特徵 L 跟動畫內結果相同 print ( \"L[:, :4]:\" , L [:, : 4 ]) v : [0.97 0.25] P1 : [[0.97 0.25]] L[:, :4]: [[ 3.18 0.53 5.99 -6.53]] 這邊的另個重點自然是 投影矩陣（Project Matrix） $\\mathbf{P_{1}}$。如果你已經觀看了 3Blue1Brown 的影片 ，就會了解為何當向量 $\\vec{v}$ 為單位向量時， 拿向量 $\\vec{x}$ 跟其點積的結果等同於 $\\vec{v}$ 對 $\\vec{x}$ 做一次投影的 線性轉換（Linear Transformation） 。這是數學上很美妙的一個 對偶（Duality） 例子。 我在這邊不會特別證明，但事實上針對任意線性轉換，我們都可以將其表示成一個特定的矩陣；而一個矩陣事實上也對應到一個特定的線性轉換。用線性轉換的角度來看，投影矩陣 $\\mathbf{P_{1}}$ 會將原本位於 2 維平面的數據 $\\mathbf{X}$ 線性壓縮到由向量 $\\vec{v}$ span 出的 1 維數線之上。 如果你了解 線性轉換 的本質，就會知道投影矩陣 $\\mathbf{P_{1}}$ \\begin{align} \\mathbf{P_{1}} & = \\begin{bmatrix} 0.97 & 0.25 \\end{bmatrix} \\end{align} 明確定義了原本的基底向量 $\\hat{i}$ 與 $\\hat{j}$ 在此線性轉換後的所在位置： $\\hat{i}$ 到新數線上 $0.97$ 的位置 $\\hat{j}$ 到新數線上 $0.25$ 的位置 你可以觀察此線性轉換過程中 $\\vec{i}$ 與 $\\vec{j}$ 的去向： 您的瀏覽器不支援影片標籤，請留言通知我：S 除了剛剛提到的線性轉換之外，我想透過這動畫傳達的另一個重要訊息是： 跳脫你的慣性思維，x 軸並不一定得水平展開。只要你想，這世上的任何直線都能是你的 x 軸。任何向量都可以是你描述手中數據的新基準。 你可以將這個斜數線當作是一個新的 x 軸，每個樣本都有其對應的 x 值。這數線跟你熟悉的水平 x 軸只差在繪製的角度有所不同而已（$\\vec{v}$ 的長度剛好也為 1）。而因為我們是透過投影矩陣 $\\mathbf{P_{1}}$，也就是一個 線性 轉換來降維，這樣的降維方法被稱作 線性降維 。線性降維中最著名的方法自然是本文主角 PCA。 PCA 的精神跟這裡說明的線性降維完全一致，差別只在於 PCA 會先系統性地拆解數據 $\\mathbf{X}$ (大多透過 奇異值分解 或是 特徵分解 )，找出一或多個 最能代表 其性質的主成分，再依照我們想要保留的維度將數據 $\\mathbf{X}$ 投影到前 $k$ 大的主成分上以降維。在下一章節你將看到，PCA 找出的主成分事實上就是數據 $\\mathbf{X}$ 的 共變異數矩陣 的 特徵向量（Eigenvectors） 。 值得一提的是，上述例子正好也是這世上最簡單的 PCA 結果。也就是說向量 $\\vec{v}$ 正好就是最能 代表 數據 $\\mathbf{X}$ 的第一主成分 $\\vec{pc_1}$。透過 scikit-learn ，要實作 PCA 並將本文的範例數據 $\\mathbf{X}$ 降維可以說是簡單到不可置信： # PCA 是一種拆解並重新表述數據的技巧 from sklearn.decomposition import PCA # 最大化 reproductivity random_state = 9527 # 使用 sklearn 實作的 PCA 將數據 X 線性地降到 1 維 # 這邊值得注意的是 sklearn API 預期的輸入維度為 # (n_samples, n_features), 輸出自然也是。 pca_1d = PCA ( 1 , random_state = random_state ) L_sk = pca_1d . fit_transform ( X . T ) . T print ( 'L_sk.shape:' , L_sk . shape ) print ( 'L_sk:' , L_sk [:, : 4 ]) # sklearn API 得到的結果跟我們手動計算結果相同 assert_almost_equal ( L_sk , L ) L_sk.shape: (1, 20) L_sk: [[ 3.18 0.53 5.99 -6.53]] 扣掉註解，只要 3 行程式碼就能得到我們要的結果。從 scikit-learn 得到的 $\\mathbf{L_{sk}}$ 跟我們剛剛手動計算的 $\\mathbf{L}$ 一樣，就是每個樣本 $\\vec{x}$ 投影到 $\\vec{pc_1}$ 後所得到的一維成分表徵。 我在前面就已經提過，多數 Python 機器學習函式庫的預期輸入都是 n_samples 優先。這是為何在呼叫 scikit-learn 時我先轉置 $\\mathbf{X}$ 使其維度變為 (n_samples, n_features) ，接著再將其結果轉置回我想要的 (n_transformed_features, n_samples) ： L_sk = pca_1d . fit_transform ( X . T ) . T 為了讓你能夠直觀地連結本文內容與線性代數的文獻，我在這篇文章裡偏好讓每個樣本 $\\vec{x}$ 保持 column vector 的形式。但如果你的數據一開始的維度就是 (n_samples, n_features) ，自然可以更輕鬆地呼叫 scikit-learn API： # 橫看成嶺側成峰 data = X . T L_transpose = pca_1d . transform ( data ) assert_almost_equal ( L . T , L_transpose ) # 通過測驗，沒有 assert error 你在 踏入荒野：實際應用 PCA 來解析真實數據 一節會看到真實世界的案例。這些說明看似繁瑣，但事實上非常地實際（practical）。因為作為一個 資料科學家 或是機器學習從業者，不少時間你可能都只是在呼叫某個 ML 函式庫的 API 而非自己從頭實作。你得確保自己能正確解讀手中數據以及 API 要求的維度。 你已經看到透過 scikit-learn ，要用 PCA 對數據做線性降維可以說是小菜一碟。如果你很實用主義，這篇文章看到這裡，把剛剛得到的一維特徵 $\\mathbf{L}$ 交給教授就可以了： L array([[ 3.18, 0.53, 5.99, -6.53, 3.81, -4.39, 0.37, 2.31, 1.04, -4.93, -2.54, -3.28, 0.46, -1.51, -3.9 , -0.11, 7.26, 4.81, -2.2 , -0.35]]) 但我相信我們可以對 PCA 有更直觀且深刻的理解。我們可以對 PCA 這個主題做 PCA，解構、探討其核心精神並重新體會其美麗之處。 還記得我剛剛說 PCA 的理念是找出一或多個 最能代表 數據 $\\mathbf{X}$ 的 $N$ 維向量並依此降維嗎？這邊的重點是如何定義「具有代表性」。一般而言，當某降維結果具有以下兩特性時，我們會認為是理想的： 最大變異：降維後所得到的新 $K$ 維特徵 $\\mathbf{L}$ 具有 最大 的 變異（Variance） 最小錯誤：用 $K$ 維的新特徵 $\\mathbf{L}$ 重新建構回 $N$ 維數據能得到 最小 的重建錯誤 在我們目前的例子裡頭，$N = 2, K = 1$。你馬上就會發現這兩特性是一體兩面，因為在 PCA 裡頭這兩性質會同時達成。首先讓我們看看第二項。 要計算重建錯誤（ R econstruction E rror，後簡稱為 RE），我們首先會將 所有樣本 降維後所得到的 1 維表徵 $\\mathbf{L}$ 再度還原回原 2 維空間。而其還原後的結果 $\\mathbf{X}_{proj}$ 跟原始數據 $\\mathbf{X}$ 之間的距離總和就是 RE。很直覺地，當 RE 越低就代表我們的降維結果越成功，因為這代表找出的新特徵 $\\mathbf{L}$ 越具代表性。 如果我們將「降維」這個運算以 $T$ 表示、「還原」該轉換的轉換以 $T&#94;{-1}$ 表示，則 RE 可以寫成以下形式： $$ \\begin{align} \\text{RE} = \\sum_{i=1}&#94;{\\text{n}_{samples}} \\left \\| \\vec{x_{i}} - T&#94;{-1}T(\\vec{x_{i}}) \\right \\| \\end{align} $$ 這邊的關鍵是認知到 $T$ 這個轉換因為牽涉到了降維，其反轉換 $T&#94;{-1}$ 一般是沒辦法將 $T(\\vec{x_{i}})$ 完美無損地還原回 $\\vec{x_{i}}$ 的。這中間無法復原、損失的資訊就是所謂的 RE。其實際計算結果如下： \"\"\" scikit-learn 裡頭的 pca 實作， transform() 就對應到上式的 T 而 inverse_transform() 則對應到 T-1。 \"\"\" # 從新的一維特徵 L 還原回原來的 2 維數據 X_proj = pca_1d . inverse_transform ( L . T ) . T # 注意維度 # 依照樣本的原始向量以及投影向量算距離後加總 # 你可以一個個樣本分開計算距離並加總與此數值比較 reconstruction_error = np . linalg . norm ( X - X_proj , axis = 0 ) . sum () reconstruction_error 12.084120788039439 當然，如果你有足夠的線代基礎，事實上不需要依賴 scikit-learn API，自己就可以利用投影矩陣 $\\mathbf{P_{1}}$ 的轉置來將 1 維表徵 $\\mathbf{L}$ 還原回原 2 維空間。以下運算也會產生跟 pca1d.inverse_transform 一致的結果： \"\"\" 手動得到 scikit-learn API 的計算結果 \"\"\" # 因為前面的 v 向量的 precision 不夠會導致誤差太大， # 這邊重新建構 P1 P1 = pca_1d . components_ assert_almost_equal ( P1 . T @ L , X_proj ) X_proj [:, : 4 ] array([[ 3.08, 0.52, 5.81, -6.33], [ 0.78, 0.13, 1.48, -1.61]]) 讓我們回到重建錯誤。上面列的式子看似複雜，但以本例而言，實際上就是在計算 2 維空間中所有原始數據白點 $\\mathbf{X}$ 與其對應的投影黃點 $\\mathbf{X_{proj}}$ 之間的距離總和，也就是下圖中所有黃線的長度總和： 您的瀏覽器不支援影片標籤，請留言通知我：S 值得注意的是，這邊說的重建錯誤指的是 一維 重建錯誤。因為我們是先降到一維後再還原回來二維空間。之後我會用 $RE_{1}$ 來表示這個錯誤。 如同前面說過的，假設我們當初火眼金睛，透過幾何直覺找出的投影向量 $\\vec{v}$ 正好也就是 PCA 找出來的 $\\vec{pc_{1}}$，則最後得到的 $RE_{1}$ 就會是 $12.08$。事實上，因為 $\\vec{pc_1}$ 是 $\\mathbf{X}$ 裡最具代表性的主成分，這是我們能得到的 最小 $RE_{1}$。 聽到這裡，你可能會有 2 點疑問： 「說不定實際上還存在著一個向量 $\\vec{w}$，使得 $\\mathbf{X}$ 投影後所得到的 $RE_{1}$ 比投影到 $\\vec{pc_{1}}$ 的結果還低啊！」 「最小化 $RE_{1}$ 跟最大化新特徵 $L$ 的變異有什麼關聯？」 考慮到多數讀者的學術背景，我在這裡並不會列出 嚴謹的數學證明 。反之，我想讓你能夠直觀地感受答案。要達到這個目標，我們可以找出 2 維平面上的任一直線，並看看將數據 $\\mathbf{X}$ 投影到這些線後所產生的 所有 $RE_{1}$ ： 您的瀏覽器不支援影片標籤，請留言通知我：S 這畫面太美，害我重複觀看而導致晚了幾週發表文章。這當然是玩笑話，但我真心建議你多看幾遍，培養對 PCA 降維的直觀感受。當然，你與生俱來的幾何直覺已經幫你回答了剛剛的疑問： PCA 找出來的主成分 $\\vec{pc_{1}}$ 是我們能找出最能代表數據 $X$ 變異傾向 的向量，它會使得投影後的 $RE_{1}$ 最小 $\\vec{pc_{1}}$ 在最小化 $RE_{1}$ 的同時也使得投影後的黃點分布最廣，這也就意味著最大化新特徵 $\\mathbf{L}$ 的變異 眼尖的你可能也已經發現，如果將數據 $\\mathbf{X}$ 投影到 PCA 中第二大的 $\\vec{pc_{2}}$ 上，你將得到 最大 的 $RE_{1}$。而因為投影到 $\\vec{pc_{1}}$ 能得到最小的 $RE_{1}$，這間接透露了 $\\vec{pc_{1}}$ 跟 $\\vec{pc_{2}}$ 互相垂直的事實（更正式點，兩者為 正交 ）。 另外如果你還記得一些基礎的 統計 知識，應該就能體會當我說 $\\vec{pc_1}$ 能代表 2 維數據 $\\mathbf{X}$ 的整體 變異（Variance） 時，事實上等同於宣稱 $\\vec{pc_1}$ 能夠用來代表 $\\mathbf{X}$ 中兩特徵 $f_1$ 與 $f_2$ 的 共變異（Covariance） 傾向。你從上圖可以明顯地看出，兩特徵呈現正向線性關係，而 $\\vec{pc_1}$ 所指的方向很好地描繪出該傾向。這是為何我們在下一節能從數據 $\\mathbf{X}$ 的 共變異數矩陣（Covariance Matrix） 中找出 $\\vec{pc_1}$ 與 $\\vec{pc_2}$ 並依此對數據 $\\mathbf{X}$ 去關聯（Decorrelate） 的原因。 好啦！我想你現在應該已經能夠直觀地理解 PCA 是怎麼將數據 $\\mathbf{X}$ 降維的，讓我們回頭解讀一下得到的結果。我們剛剛透過 $\\vec{pc_{1}}$ 將每個 2 維行向量 $\\vec{x}$ 轉換成一維特徵 $l$。當你把所有樣本對應到的特徵 $l$ 一行行放在一起，自然就會得到矩陣 $\\mathbf{L}$。 要直觀地解讀我們剛剛得到的降維結果，最好的方式是觀察 $\\vec{pc_{1}}$ 的數值： # scikit-learn 讓我們可以十分輕鬆地取得 PCs # 這值跟我們當初的向量 v 相同 # 索引 0 即為 PCA 中的第一主成分 pc1 = pca1d . components_ [ 0 ] pc1 array([0.97, 0.25]) $\\vec{pc_{1}}$ 裡頭的數值清楚地透露了關於數據 $\\mathbf{X}$ 的關鍵訊息： 原來的兩特徵 $f_1$ 與 $f_2$ 變化呈同方向，為 正相關（Positively Correlated） 對新特徵 $l$ 而言，原第一特徵 $f_1$ 的重要程度約是 $f_2$ 的 4 倍 是的，你會驚訝於 $\\vec{pc_{1}}$ 本身隱含了那麼多跟 $\\mathbf{X}$ 相關的寶貴資訊，而這正是其之所以為 $\\mathbf{X}$ 的主成分的原因。主成分分析 PCA 就是透過找出這些主成分來將數據降維，同時保留最關鍵的資訊。而因為 $\\vec{pc_{1}}$ 就自帶了幾乎所有 $\\mathbf{X}$ 在 2 維空間裡的變異，要表示 $\\mathbf{X}$ 裡頭每個樣本的特性就不再需要兩個數字，而只要一個新的特徵矩陣 $\\mathbf{L}$ 就好： L [:, : 4 ] array([[ 3.18, 0.53, 5.99, -6.53]]) 以下則是前 4 個樣本在降維前後的表徵對照： 您的瀏覽器不支援影片標籤，請留言通知我：S 特徵值 $l$ 告訴我們一個特定樣本 $\\vec{x}$ 符合 $\\vec{pc_{1}}$ 的程度。在 $\\vec{pc_{1}}$ 為正的情況下： 第三樣本的 $l = 5.99$ 代表其相當地符合 $\\vec{pc_{1}}$ 特性，可以想像其原特徵 $f_1$ 與 $f_2$ 皆為 正 且 $f_1> f_2$ 第四樣本的 $l = -6.53$ 則代表其與 $\\vec{pc_{1}}$ 呈負向關係，可以推測其原特徵 $f_1$ 與 $f_2$ 皆為 負 值 在這個降維例子裡頭，事實上我們是將 $\\vec{pc_{1}}$ 作為一維空間 $\\Re&#94;{1}$ 的基底。透過投影，我們以 $\\vec{pc_{1}}$ 為基準，重新描述本來處在二維空間 $\\Re&#94;{2}$ 裡的所有樣本 $x$，得到其新的一維成分表徵 $l$。 儘管只會產生一個數字，這個基底向量 $\\vec{pc_{1}}$ 可比我們從小習慣使用的 $\\hat{i}$ 與 $\\hat{j}$ 還能夠清楚地描述數據 $\\mathbf{X}$ 的本質。有了 $\\vec{pc_{1}}$，你就只需要看一維特徵 $\\mathbf{L}$ 而不再需要用 $\\mathbf{B}_{standard}$ 所描述的二維數據 $\\mathbf{X}$ 了： X [:, : 4 ] array([[ 2.89, 0.32, 5.8 , -6.52], [ 1.52, 0.91, 1.52, -0.88]]) 這正呼應到我們前面提過的重要概念： 線性降維的核心精神是將原始數據拆解成更具代表性的主成分，並以其作為新的基準，由此獲得更能描述數據本質的新成分表徵。 而這正是透過 PCA 來達到「化繁為簡」的至高精神，希望你能有所體悟。我稍後還會展示 如何用 PCA 來對真實世界中的高維數據降維 ，但從下節開始，讓我們先看看 PCA 是如何透過主成分以及座標轉換來將多個特徵 去關聯（Decorrelation） 的。我們將會看到更多振奮人心的動畫，但在那之前你得確保自己已熟悉以下線代概念： 基底變更 Change of Basis 特徵向量/值 Eigenvectors/Eigenvalues 記住，欲速則不達。你的目標應該放在「真正地理解並掌握 PCA」，而不是「在最短的時間閱讀完本文」。 世上最簡單的去關聯：數據原來的共變異去哪了？ 除了可以降低數據維度，PCA 也常被用來去除多個特徵 之間 的關聯。 去關聯（Decorrelation） 在機器學習領域裡有不少應用情境，比方說你可能會想要有一組能夠用來獨立解釋數據特質的特徵，或是想保證餵進 ML 模型的多個特徵彼此無關，據此簡化問題以幫助模型泛化（generalize）。這時 PCA 就可以被視為一種數據前處理手法，將多個特徵之間的關聯「拿掉」。 知己知彼。要理解「去關聯」，你得先對「關聯」與「共變異」有透徹的理解。 在統計以及機率論的世界裡，我們時常使用 樣本共變異數（Sample Cov ariance，後簡稱為 Cov） 來估計多個特徵兩兩之間的「共同變化程度」。你常聽到的 相關係數 則是其正規化後的結果。以數據 $\\mathbf{X}$ 裡頭的兩特徵 $f_1$ 與 $f_2$ 為例，你可以把 $\\operatorname{Cov}(f_1,f_2)$ 視為一個具有兩運算元的運算，其公式如下： $$ \\begin{align} \\operatorname{Cov}(f_1,f_2)=\\frac{1}{N-1}\\sum_{i=1}&#94;{N}\\left( f_{i1}-\\bar{f}_1 \\right) \\left( f_{i2}-\\bar{f}_2 \\right) \\end{align} $$ 其中 $f_{i1}$ 與 $f_{i2}$ 分別代表 $\\mathbf{X}$ 裡第 $i$ 個樣本 $x_i$ 的 $f_1$ 與 $f_2$ 的值； $\\bar{f}_1$ 為我們觀測到的所有 $f_1$ 值的平均（$f_2$ 同理）；$N$ 則為樣本數目。以數據 $\\mathbf{X}$ 而言， $N = 20$。值得一提的是，當 $f_2 = f_1$，共變異數就代表著特徵 $f_1$ 本身的 變異（Variance） 。因此共變異數只是你所熟悉的變異的通用版本。 有了上式以後，我們可以輕易地計算出 $\\operatorname{Cov}(f_1,f_2)$： # 別忘記 X 維度是 (n_features, n_samples) print ( \"X.shape:\" , X . shape , \" \\n \" ) print ( X , \" \\n \" ) # 分別計算兩特徵的樣本平均 f1_bar = X [ 0 ] . mean () f2_bar = X [ 1 ] . mean () # 因為我們當初已經減去各特徵平均， 兩者事實上已經為 0 # 也就是說 f1_bar 與 f2_bar 分別跟 y, x 軸重疊 print ( \"f1_bar:\" , f1_bar ) print ( \"f2_bar:\" , f2_bar ) assert_almost_equal ( 0 , f1_bar ) assert_almost_equal ( 0 , f2_bar ) # 樣本共變異估計平均每個樣本裡特徵 1 跟特徵 2 的共同變化程度 # 為了幫助你理解，我用世界上最沒效率，但跟公式最相近的方式計算 cov = 0 for x in X . T : # 這時就能看出 (n_samples, ) 維度在前的好處 f1 , f2 = x cov += ( f1 - f1_bar ) * ( f2 - f2_bar ) n = X . shape [ 1 ] cov /= n - 1 print ( \" \\n Cov(f1, f2): {:.2f} \" . format ( cov )) X.shape: (2, 20) [[ 2.89 0.32 5.8 -6.52 3.94 -4.21 0.45 2.14 1.3 -4.98 -2.4 -3.1 0.69 -1.59 -3.64 -0.24 6.81 4.63 -2.24 -0.06] [ 1.52 0.91 1.52 -0.88 -0.03 -1.26 -0.25 0.96 -0.89 -0.45 -0.88 -1.12 -0.86 0.13 -1.53 0.51 2.66 1.28 -0.14 -1.19]] f1_bar: -5.551115123125783e-18 f2_bar: -2.2204460492503132e-17 Cov(f1, f2): 3.27 碰！利用 20 筆樣本數據，我們估計出特徵 $f_1$ 與 $f_2$ 的樣本共變異數 $\\operatorname{Cov}(f_1,f_2)\\cong 3.27$。減去各特徵的平均很合理，因為我們關心的是各特徵之間 相對 （於平均）的變化關係，而非其絕對值變化。 我不曉得你的統計基礎如何，但如果這是你第一次聽到共變異數，我會推薦 StatQuest 的教學影片 。當然，要直觀地理解共變異數也沒那麼困難。不過比起實際數值 $3.27$，我會建議你關注在 $\\operatorname{Cov}(f_1,f_2)$ 的 正負號 並問自己以下 3 個問題： 怎樣的情況會讓 $\\operatorname{Cov}(f_1,f_2) > 0$？ 怎樣的情況會讓 $\\operatorname{Cov}(f_1,f_2) = 0$？ 怎樣的情況會讓 $\\operatorname{Cov}(f_1,f_2) < 0$？ 如果你能先自己想通這件事情，我相信會非常有成就感。讓我再次列出樣本共變異數的公式供你參考： $$ \\operatorname{Cov}(f_1,f_2)=\\frac{1}{N-1}\\sum_{i=1}&#94;{N}\\left( f_{i1}-\\bar{f}_1 \\right) \\left( f_{i2}-\\bar{f}_2 \\right) $$ 以純 代數 的角度來看，當第 $i$ 個樣本 $x_i$ 裡的這兩項結果： $f_{i1}-\\bar{f}_1$ $f_{i2}-\\bar{f}_2$ 為 同號 （即皆為正或皆為負）時，兩者相乘會得到正值。而如果多數樣本的相乘結果皆為正，將其加總並平均的結果自然仍為正值；反之，當兩者 異號 （一正一負）的情況較多時，最後的平均結果是負的機會就會比較大。換句話說： $\\operatorname{Cov}(f_1,f_2) > 0$ 時代表兩特徵的變化具有 相同 傾向，時常一起變大或一起變小 $\\operatorname{Cov}(f_1,f_2) = 0$ 時代表兩特徵的變化沒有明顯關係 $\\operatorname{Cov}(f_1,f_2) < 0$ 時代表兩特徵的變化具有 相反 傾向，時常一個變大，一個變小 當然，統計做的事情是估計，當我們手中有越多觀測結果，估計出來的樣本共變異數自然就會越準，降低過適（overfit）的可能性。為了讓你能夠更直觀地了解共變異數，底下我以數據 $\\mathbf{X}$ 為例，用 幾何 的觀點展示 $\\operatorname{Cov}(f_1,f_2)$ 的計算過程： 您的瀏覽器不支援影片標籤，請留言通知我：S 我想這應該是你這輩子看過最直觀的共變異數計算過程。在計算過程中值為正的物件皆被以藍色所表示；紅色則代表其值為負。每個數據點 $x_i$ 的顏色依照其 $(f_{i1}-\\bar{f}_1)(f_{i2}-\\bar{f}_2)$ 的相乘結果有所變化： 藍點：$(f_{i1}-\\bar{f}_1)(f_{i2}-\\bar{f}_2) > 0$ 紅點：$(f_{i1}-\\bar{f}_1)(f_{i2}-\\bar{f}_2) < 0$ 透過這個動畫，你可以清楚地觀察到以下結果： 因為兩特徵值的平均皆為 $0$，$\\bar{f}_1$ 與 $\\bar{f}_2$ 正好分別跟 y、x 軸重合（圖中黃線） 在第一與第三象限的數據點因為 $(f_{i1}-\\bar{f}_1)$ 跟 $(f_{i2}-\\bar{f}_2)$ 同號（距離線皆為紅或藍），相乘結果為正（顯示為藍點） 在第二及第四象限則因為異號（距離線一紅一藍），相乘結果為負（數據點呈紅色） 因為藍點的結果加總較大，數據 $\\mathbf{X}$ 的共變異數為正，代表平均來說，兩特徵存在著正向關聯 我們也可以做個簡單歸納，當一三或是二四象限的數據點較其他象限多時，我們更容易觀察到兩特徵呈現線性關係，且共變異數不太可能為零。這些都是不錯的觀察，不過現在問你自己： 如果數據 $\\mathbf{X}$ 的共變異數為正，跟它最像但共變異數為零的成分表徵 $\\mathbf{L}$ 長什麼樣子？如果的確存在一個共變異數為零的成分表徵 $\\mathbf{L}$ 的話，怎樣才能在不改變 $\\mathbf{X}$ 本質的情況下轉變到 $\\mathbf{L}$？ 如果你能想通這件事情，用 PCA 去關聯就完全是小事一樁。我馬上就會揭曉答案，但我希望你能先自己動腦想想。提示：選擇一組適當的基底 $\\mathbf{B}$ 並旋轉 $\\mathbf{X}$ 到適當的位置。如果你完全沒有想法，我相信底下的動畫可以派上用場。觀察： 什麼時候共變異數最小 $\\mathbf{X}$ 的兩主成分 $\\vec{pc_1}$ 與 $\\vec{pc_2}$ 的位置 $\\mathbf{X}$ 的變異趨向 您的瀏覽器不支援影片標籤，請留言通知我：S 怎麼樣？有點感覺了嗎？我想你會同意，要在不改變數據 $\\mathbf{X}$ 本身的情況下將特徵 $f_1$、$f_2$ 之間的關聯消除，最簡單且有效率的作法就是旋轉座標了。你可以想像在旋轉的過程中，我們不斷地將當下跟水平 x 軸以及垂直 y 軸重疊的 2 個單位向量視為基底向量 $b_1$ 與 $b_2$，並將數據 $\\mathbf{X}$ 裡頭的每個樣本 $\\vec{x_i}$ 重新表達成這兩向量的線性組合，即： $$ \\begin{align} \\vec{x_i} = l_1 \\vec{b_1} + l_2 \\vec{b_2}, l_1, l_2 \\in \\Re \\end{align} $$ 每組 $b_1$ 與 $b_2$ 都是一個合法的 $\\Re&#94;2$ 基底 $B$，所以上面動畫實際上無時無刻都在進行 基底變更（Change of Basis） 。在旋轉開始之前，初始基底自然就是我們一開始表達數據 $\\mathbf{X}$ 的 $\\mathbf{B}_{standard}$，因此每個樣本 $x$ 的成分表徵，也就是 $f_1$ 以及 $f_2$ 的值並沒有被改變。但當基底 $B$ 對應到 $\\{\\vec{pc_1},\\vec{pc_2}\\}$，也就是文章開頭劇透過的 $\\mathbf{B}_{pc}$ 時，美好的結果就誕生了： $\\mathbf{X}$ 的變異傾向不再是由左下至右上的斜線，而是被限制在水平或是垂直軸上，新的兩特徵看似不再相關 $\\mathbf{B}_{pc}$ 自帶的特徵關聯信息分別被編碼在新的 x、y 軸上，這使得新的兩特徵之間不再相關 透過投影到 $\\mathbf{B}_{pc}$，每個樣本 $x$ 獲得一組可以互相獨立解釋的成分表徵 $l_1$ 與 $l_2$ 新特徵的共變異數 $\\operatorname{Cov}(l_1,l_2)$ 被最小化至零 這些敘述都是等價的。而你現在知道去關聯時選擇對的基底有多重要了。你也可以想像這動畫是前面旋轉向量 $\\vec{v}$ 並計算一維重建錯誤 $RE_1$ 的 2 維版本。當時為了降維，我們只投影到 $\\vec{pc_1}$；但現在為了去關聯，我們選擇投影到 $\\vec{pc_1}$ 以及 $\\vec{pc_2}$，獲得另外一組 2 維成分表徵。 我們甚至可以說用 PCA 對數據 $\\mathbf{X}$ 去關聯就是將主成分當作新的基底 $B_{pc}$ 並進行基底變更。我們用一組更具代表性的基底 $\\{\\vec{pc_1},\\vec{pc_2}\\}$ 來重新表述數據 $\\mathbf{X}$，由此獲得一組彼此沒有關聯的全新特徵 $l_1$ 與 $l_2$ 。 您的瀏覽器不支援影片標籤，請留言通知我：S 我們剛剛看過的 1 維投影與重建錯誤 透過剛剛的動畫以及解說，我想你已經能夠直觀地理解 PCA 是怎麼將數據 $\\mathbf{X}$ 去關聯的。如果數據只有 2 維，那麼就只要將 2 個主成分作為新的基底，將數據投影到上面並得到新的特徵 $l_1$ 及 $l_2$ 即可。現在只剩一個謎團還沒解開：怎麼實際找出主成分？數據 $\\mathbf{X}$ 的主成分究竟從哪冒出來的？ 我在降維時已經透露過，數據 $\\mathbf{X}$ 的 共變異數矩陣（Covariance Matrix） 裡頭的 2 個 Eigenvectors 事實上就是我們要找的 $\\vec{pc_1}$ 以及 $\\vec{pc_2}$。我等等會給你個更直觀的解釋，但現在先讓我們算出該共變異數矩陣： \"\"\" 透過 NumPy 計算 Ｘ 的共變異數矩陣 K。 跟多數 Python ML API 不同， NumPy API 預期的輸入維度大都是 (n_features, n_samples)。事實上這比較符合線性代數直覺， 每個 column 是一個樣本 x。這也是我喜歡 NumPy 的原因 \"\"\" # 注意不像 scikit-learn, 我並沒有先將 X 轉置才傳入 K = np . cov ( X ) # 共變異數矩陣 diagonal 方向上為各特徵自己的變異 K array([[13.35, 3.27], [ 3.27, 1.33]]) 不只是兩特徵的共變異數，共變異數矩陣事實上也包含了兩特徵自身的變異。如果你的線代神經夠敏銳，應該可以理解為何共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 是 對稱矩陣（Symmetric Matrix） 。這是因為共變異數的計算跟 $f_1$ 與 $f_2$ 的輸入順序無關，自然 $\\mathbf{K}_{f_1f_2} = \\mathbf{K}_{f_1f_2}&#94;{T}$。我們稍後將利用這個性質直探共變異數矩陣的本質。 我在這裡不會說明 如何計算一個矩陣的 Eigenvalues 以及 Eigenvectors ，但我們可以透過 NumPy 來將它們找出來： \"\"\" 透過 NumPy 我們能輕易地找出共變異數矩陣的 Eigenvectors 不像 scikit-learn，通常我們得自己依照 eigenvalue 大小排序 eigenvectors 但在此例中，正好第一行為 Eigenvalue 最大的 Eigenvector \"\"\" # 透過特徵拆解取得共變異數矩陣的 Eigenvectors/values eig_vals , eig_vecs = np . linalg . eig ( K ) # eig_vecs 的每一行（column）為一個 Eigenvectors print ( f \"eig_vecs.shape:\" , eig_vecs . shape ) print ( eig_vecs ) print ( '-' * 40 ) # eig_vals 裡則是對應的 Eigenvalues print ( \"eig_vals.shape:\" , eig_vals . shape ) print ( eig_vals ) eig_vecs.shape: (2, 2) [[ 0.97 -0.25] [ 0.25 0.97]] ---------------------------------------- eig_vals.shape: (2,) [14.18 0.5 ] 沒錯， $\\mathbf{K}_{f_1f_2}$ 的第一個 Eigenvector $\\vec{v_1}$ 就是我們降維 $\\mathbf{X}$ 時用的第一主成分 $\\vec{pc_1}$： # 兩者完全一樣，毫不意外 pc1 = pca_1d . components_ . T assert_almost_equal ( eig_vecs [:, : 1 ], pc1 ) # 降維時我們用的第一主成分 pc1 array([[0.97], [0.25]]) 你現在也知道數據 $\\mathbf{X}$ 的第二主成分 $\\vec{pc_2}$ 就是 $\\mathbf{K}_{f_1f_2}$ 的第二大 Eigenvector $\\vec{v_2}$。我們也清楚 $\\{\\vec{v_1}, \\vec{v_2}\\}$ 就等同於之前提過的 $\\mathbf{B}_{pc}$，其跟標準基底 $\\mathbf{B}_{standard}$ 一樣可以作為 $\\Re&#94;2$ 基底。跟 $\\mathbf{B}_{standard}$ 不同的是， 用 $\\mathbf{B}_{pc}$ 為基底獲得的 2 維成分表徵能最有效率地表達數據 $\\mathbf{X}$ 的特性。且由於這兩個向量同時為 $\\mathbf{K}_{f_1f_2}$ 的 Eigenvectors，我們一般也稱這種基底為 Eigenbasis $\\mathbf{B}_{eigen}$。 如果你有依照建議事先觀看 3Blue1Brown 的基底變更介紹 ，就能明白我們剛剛得出的 eig_vecs 事實上就代表著一個 基底變更矩陣（ C hange o f B asis Matrix, 後簡稱為 COB 矩陣） 。這個 COB 矩陣做的事情是將： $C_{eigen}$：以 $\\mathbf{B}_{eigen}$ 為基準的 座標系統 ，即以 $\\mathbf{B}_{eigen}$ 所表示的成分表徵 $l_1, l_2$ 轉換成 $C_{standard}$：以 $\\mathbf{B}_{standard}$ 為基準的座標系統，以 $\\mathbf{B}_{standard}$ 所表示的成分表徵 $f_1, f_2$ 這也是為何 COB 矩陣常被說是在進行座標轉換，就像是你在之前的旋轉動畫看到的那樣。為了方便起見，我將這個 COB 矩陣以 $\\mathbf{Q}$ 表示。而因為 $\\mathbf{Q}$ 裡頭的每個 eigenvectors $\\vec{v_i}$ 長度皆為 1 且互相正交， $\\mathbf{Q}$ 是一個 正交矩陣（Orthogonal Matrix） 。其美妙的性質值得特別空一行出來表示： $$ \\begin{align} \\mathbf{Q}&#94;{-1} = \\mathbf{Q}&#94;\\mathsf{T} \\end{align} $$ 別忘了我們最一開始得到的 $(x, y)$ 座標是 $\\mathbf{B}_{standard}$ 的成分表徵，因此是被以 $C_{standard}$ 的形式表示。我們實際想要做的事情是 $\\mathbf{Q}$ 的逆運算 $\\mathbf{Q}&#94;{-1}$：將 $C_{standard}$ 轉換成 $C_{eigen}$。幸運的是，你已經知道 $\\mathbf{Q&#94;{-1}} = \\mathbf{Q}&#94;\\mathsf{T} $！水到渠成，最令人期待的時刻到來了，我們現在可以用 $\\mathbf{Q}&#94;\\mathsf{T} $ 將數據 $\\mathbf{X}$ 重新表示成 $\\mathbf{B}_{eigen}$ 的成分表徵以去關聯： 您的瀏覽器不支援影片標籤，請留言通知我：S 簡單而美麗。這動畫總結了所有我們在這章節談到的概念。為了把 $\\mathbf{X}$ 去關聯，我們將 $\\mathbf{K}_{f_1f_2}$ 的 Eigenvectors 作為新的基底並建構出 COB 矩陣，成功地將彼此關聯的兩特徵 $f_1$ 與 $f_2$ 重新表示成互相獨立的隱藏表徵 $l_1$ 與 $l_2$。 換句話說，我們是透過 PCA 找出數據 $\\mathbf{X}$ 中的主成分，並以此為基底 $\\mathbf{B_{eigen}}$ 算出 $\\mathbf{X}$ 的主成分表徵（Principal Component Representation）。我們在前面也已經看過，如果只投影到第一主成分 $\\vec{pc_1}$，那就等同於最有效的一維線性降維。 這組主成分可以從 $\\mathbf{X}$ 的共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 中找出且專屬於 $\\mathbf{X}$；而我們一開始依照直覺用來表達 $\\mathbf{X}$ 的成分 $\\{\\hat{i}, \\hat{j}\\}$ 是一組通用成分（General Components）：非常地泛用，但因為沒有針對 $\\mathbf{X}$ 客製化，在對 $\\mathbf{X}$ 去關聯及降維的情境中並不是最好的基底。 花了不少時間，但現在你應該已經能夠深刻地體會我在文章開頭所陳述的重要概念： 所有基底生而平等。但最後，你的應用情境與目的決定了哪組基底比其他的選擇好。 如果今天我們只是想要簡單地視覺化數據 $\\mathbf{X}$，則 $\\mathbf{B}_{standard}$ 是我們的第一人選；但如果我們的目的是將 $\\mathbf{X}$ 降維或者是去關聯，則 $\\mathbf{B}_{pc}$ 是你最好的選擇；而如果今天的數據不再是 $\\mathbf{X}$ 而是另組數據 $\\mathbf{Y}$，$\\mathbf{B}_{pc}$ 也就不再是最好的選擇，需要重新尋找。 掌握了這些道理，我們要自己用 PCA 為 $\\mathbf{X}$ 去關聯可說是蛋糕一片： # Q 代表將 Eigenbasis 的空間轉回標準的笛卡爾空間 # 因此我們要做的是 Q-1 = Q.T Q = eig_vecs X_trans = Q . T @ X X_trans array([[ 3.18, 0.53, 5.99, -6.53, 3.81, -4.39, 0.37, 2.31, 1.04, -4.93, -2.54, -3.28, 0.46, -1.51, -3.9 , -0.11, 7.26, 4.81, -2.2 , -0.35], [ 0.76, 0.81, 0.04, 0.75, -1. , -0.18, -0.35, 0.4 , -1.18, 0.79, -0.27, -0.33, -1. , 0.52, -0.59, 0.55, 0.9 , 0.09, 0.42, -1.13]]) 真的，關鍵就一行矩陣相乘而已，樸實無華。你也可以確認 $\\mathbf{X}_{trans}$ 的共變異數已經為 $=0$： K1 = np . cov ( X_trans ) print ( K1 ) assert_almost_equal ( 0 , K1 - np . diag ( np . diagonal ( K1 ))) [[1.42e+01 8.42e-16] [8.42e-16 5.02e-01]] 你也可以將去關聯前後的數據都畫出來比較一下： plt . scatter ( X [ 0 ], X [ 1 ], alpha = 0.2 ) plt . scatter ( X_trans [ 0 ], X_trans [ 1 ]) plt . axis ( 'equal' ); 很直覺的， 在我們的例子裡 PCA 實際上就是稍微旋轉數據，使得讓兩特徵的共變異數 $=0$。而且旋轉後的數據呈現水平分布，的確沒有相關，你說是吧？去關聯任務，大成功！到此為止，我想你已經能夠直觀地理解 PCA 怎麼將數據去關聯，並能實際將這個數據處理技巧應用在自己的數據上了，恭喜！ 我在這章節用了不少篇幅闡述 PCA 的去關聯應用，如果你到目前為止都有好好跟上，那你的 PCA 基礎就已經勝過大多數人了。不過，如果你內心還是覺得好像少了點什麼，可能是因為你還無法給這個問題一個非常直觀的解釋： 我們透過 PCA 將數據去關聯了，但原來特徵之間的共變異去了哪裡？ 事實上這是本章標題丟出的提問。再繼續閱讀之前，用一分鐘想像你迫不及待地想跟別人解釋 PCA 是怎麼將數據去關聯時，他 / 她問了這個問題。你要怎麼回答？ 喔，拜託別直接往下拉！這是一個複習本章概念以及所有你學過的線代概念的最佳時機，也是你唯一一次有機會用自己的話解釋 PCA 概念。我保證，在觀看接下來幾個動畫之後，你就永遠沒有這個機會了。你可以快速地重新瀏覽本章內容或是 Google，尋找此題解答的蛛絲馬跡。 希望你自己有想出一些合理的解釋，因為這對內化 PCA 概念有很大的幫助。你的解釋應該跟數據 $\\mathbf{X}$ 的共變異數矩陣 $\\mathbf{K}_{f_1f_2}$有關，畢竟我們已經知道 PCA 找出的主成分就是該矩陣的 Eigenvectors。如果讓我回答剛剛的問題，我會說： 數據的共變異一直都在，是去關聯後數據的成分表徵改變了。去關聯後，數據的共變異性質體現在其主成分本身之上，而共變異的程度對應到主成分的變異。 再換句話說，共變異數矩陣的 Eigenvectors 事實上就隱含了特徵之間的共變異性質，而該共變異的程度則由對應的 Eigenvalues 所表示。如果你有類似的回答，很好！你可以直接 跳到下一章，看看如何將 PCA 套用到真實世界的數據 。但如果你對這個概念還是有點模糊，我會再花一點篇幅讓你多點直觀感受。 關鍵在於意識到 $X$ 的共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 完整地儲存了 $\\mathbf{X}$ 的變異資訊，同時本身也是一個 線性轉換 的事實： # K 是 X 的宇宙魔方，隱含了 X 的關鍵資訊 K array([[13.35, 3.27], [ 3.27, 1.33]]) 如同 之前神經網路的文章 以及前面對一維投影矩陣 $\\mathbf{P}_1$ 的解釋，你可以想像如果我們將 $\\mathbf{K}_{f_1f_2}$ 套用（apply）到數據 $\\mathbf{X}$ 上，它會增強其 $\\mathbf{X}$ 本來的變異傾向，使得每個樣本 $x$ 依照兩特徵 $f_1$ 與 $f_2$ 的線性關係做延伸： 您的瀏覽器不支援影片標籤，請留言通知我：S 你等等可以數數格子，確認轉換後的 $\\hat{i}$ 與 $\\hat{j}$ 是否的確移動到 $\\mathbf{K}_{f_1f_2}$ 所定義的位置。這邊最重要的是，因為共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 本身就代表著數據 $\\mathbf{X}$ 的變異以及特徵 $f_1$ 與 $f_2$ 之間的線性關係，其所對應的線性轉換自然能把最吻合該變異、特徵之間相對關係的向量 等倍 縮放。 而什麼向量能被 $\\mathbf{K}_{f_1f_2}$ 轉換後維持等倍縮放而不改變自己的方向？自然是該矩陣的 Eigenvectors！你也可以清楚地看到，第一大的 Eigenvector $\\vec{v_1}$ 直接地反映了整體數據的變異傾向。這是為何 $\\mathbf{K}_{f_1f_2}$ 的 Eigenvectors 可以作為數據 $\\mathbf{X}$ 的主成分的原因。而儘管 $\\lambda_2$ 不大，第二個 Eigenvector $\\vec{v_2}$ 則解釋了跟 $\\vec{v_1}$ 正交方向的數據變異。把兩者解釋的變異放在一起，我們就能還原數據 $X$ 的原貌。 我們剛剛用 NumPy 得到的 Eigenvalues $\\lambda_1, \\lambda_2$ 就是這兩個 Eigenvectors 套用 $\\mathbf{K}_{f_1f_2}$ 後的伸縮倍率： # K @ pc1 = 14.2 * pc1 # K @ pc2 = 0.5 * pc2 eig_vals array([14.18, 0.5 ]) 另外你可以看到 $\\lambda_{1}$ 已經解釋了大部分 $\\mathbf{X}$ 的變異，這是為何投影到 $\\vec{v_1}$ 是個好的線性降維。更美妙的是，因為共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 是一個 實數對稱矩陣（Real Symmetric Matrix） ，我們可以對其進行 特徵分解（Eigen Decomposition） ： $$ \\begin{align} \\mathbf{K}_{f_1f_2} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}&#94;\\mathsf{T} \\end{align} $$ 如果你腦筋動得夠快，且還記得我們前面是怎麼用 $\\mathbf{Q}&#94;\\mathsf{T}$ 將數據 $\\mathbf{X}$ 旋轉 並去關聯的，看到上面這個式子你應該跟當初的我一樣激動地跳起來並撞到天花板才對！不不不，並不是因為右項長得很像表情符號，而是因為 $\\mathbf{K}_{f_1f_2}$ 所代表的線性轉換可以被拆解成 3 個更簡單的線性轉換：旋轉（rotate）、伸縮（stretch）、再旋轉！ $\\mathbf{Q}$ 正是我們前面去關聯時交手過的那個 正交矩陣（Orthogonal Matrix） ，裡頭的每個 column 都對應到一個 Eigenvector $\\vec{v_i}$；$\\mathbf{\\Lambda}$ 則是一個 對角矩陣（Diagonal matrix） ，其對角線皆為 Eigenvalues $\\lambda_i$，分別對應到 $\\mathbf{Q}$ 裡頭的 Eigenvectors 使得： $$ \\begin{align} \\mathbf{K}_{f_1f_2}\\vec{v_i} = \\lambda_i \\vec{v_i} \\end{align} $$ 而我們之前是透過 $\\mathbf{Q}&#94;\\mathsf{T}$ 旋轉座標來獲得共變異數 $=0$ 的 $\\mathbf{X}_{trans}$： $$ \\begin{align} \\mathbf{X}_{trans} = \\mathbf{Q}&#94;\\mathsf{T} \\mathbf{X} \\end{align} $$ 透過以上幾個式子，我們也可以將 $\\mathbf{K}_{f_1f_2}$ 套用到 $\\mathbf{X}$ 的過程重新表示成以下形式： $$ \\begin{align} \\mathbf{K}_{f_1f_2}\\mathbf{X}&= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}&#94;\\mathsf{T}\\mathbf{X}\\\\ &= \\mathbf{Q}\\mathbf{\\Lambda} \\mathbf{X}_{trans} \\end{align} $$ 把你從本文學到的所有概念拿出來，你會知道 $\\mathbf{K}_{f_1f_2}$ 隱含的 3 個簡單轉換分別為： 旋轉：透過 $\\mathbf{Q}&#94;\\mathsf{T}$ 將 $\\mathbf{X}$ 旋轉到以 $\\mathbf{B}_{eigen}$ 為基底的座標 $C_{eigen}$ 伸縮：透過 $\\mathbf{\\Lambda}$ 將轉換到 $C_{eigen}$ 上的 $\\mathbf{X}_{trans}$ 依照 Eigenvector 方向伸縮 Eigenvalue 倍 旋轉：透過 $\\mathbf{Q}$ 將伸縮完的 $\\mathbf{X}_{trans}$ 旋轉回原來座標 $C_{standard}$ 沒錯，共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 本身代表著 3 個連續的簡單線性轉換：旋轉、伸縮、再旋轉。而在共變異的情境下，代表著將數據中各特徵 $f_i$ 的共變異傾向重新表達成主成分 $\\vec{pc_j}$ 自己的變異。 子曰：『朝聞道，夕死可矣。』 ─ 《論語．里仁》 我承認這用詞有點浮誇，但這的確是我當初想通這道理時腦中浮現的第一個句子。如果你平常沒有接觸線性代數，那從代數的角度看來可能只是一些符號的相乘轉換；但如果你從幾何的觀點觀察，這拆解結果實在是美到不要不要的： 您的瀏覽器不支援影片標籤，請留言通知我：S 你可以拉回去跟上個 $\\mathbf{K}_{f_1f_2}$ 的轉換動畫做比較，但我可以直接告訴你兩者代表一模一樣的轉換。從幾何角度我們可以清楚地看到兩個旋轉 $\\mathbf{Q}$ 與 $\\mathbf{Q}&#94;\\mathsf{T}$ 都只是在進行基底變更（改變觀察視角），並不會改變任何 $\\mathbf{X}$ 本身的變異。因此事實上 $\\mathbf{X}$ 的兩特徵的共變異趨勢（Covariance）都被以 $\\mathbf{K}_{f_1f_2}$ 裡頭的 Eigenvectors $\\vec{v_i}$ 所指的方向所表達，而 Eigenvalue $\\lambda_i$ 則代表了該變異的大小（Variance）。 現在你能夠直觀地回答本章的大哉問了：「數據的共變異去了哪裡？」 數據的共變異一直都在，是去關聯後數據的成分表徵改變了。去關聯後，數據的共變異性質體現在其主成分本身之上，而共變異的程度對應到主成分的變異。 就跟剛剛我們利用 $\\mathbf{Q}&#94;\\mathsf{T}$ 改變基底一樣，在你理解對稱矩陣的特徵拆解以及其所代表的幾何意義以後，要用程式碼實現這些運算真的是簡單到不行，多虧了無數開發者的功勞： \"\"\" 利用 NumPy 取得原始數據 X 的共變異數矩陣 K 並實現 Eigen Decomposition \"\"\" print ( \"原始數據 X:\" ) print ( X ) # 計算共變異數矩陣 K = np . cov ( X ) print ( \" \\n 共變異數矩陣 K:\" ) print ( K ) # 實現對稱矩陣的 Eigen Decomposition eig_vals , Q = np . linalg . eig ( K ) La = np . diag ( eig_vals ) print ( \" \\n Q:\" ) print ( Q ) print ( \" \\n Lambda:\" ) print ( La ) print ( \" \\n Q.T:\" ) print ( Q . T ) # 我們可以確認共變異數矩陣的確等於這三個矩陣相乘 assert_almost_equal ( K , Q @ La @ Q . T ) print ( \" \\n K = Q @ Lambda @ Q.T !!\" ) 原始數據 X: [[ 2.89 0.32 5.8 -6.52 3.94 -4.21 0.45 2.14 1.3 -4.98 -2.4 -3.1 0.69 -1.59 -3.64 -0.24 6.81 4.63 -2.24 -0.06] [ 1.52 0.91 1.52 -0.88 -0.03 -1.26 -0.25 0.96 -0.89 -0.45 -0.88 -1.12 -0.86 0.13 -1.53 0.51 2.66 1.28 -0.14 -1.19]] 共變異數矩陣 K: [[13.35 3.27] [ 3.27 1.33]] Q: [[ 0.97 -0.25] [ 0.25 0.97]] Lambda: [[14.18 0. ] [ 0. 0.5 ]] Q.T: [[ 0.97 0.25] [-0.25 0.97]] K = Q @ Lambda @ Q.T !! 好啦！遮掩住 PCA 本質的最後一塊迷霧也已經散去。你現在能夠直觀地理解為何人們總是說共變異數矩陣 $\\mathbf{K}_{f_1f_2}$ 的 Eigenvectors 是數據 $\\mathbf{X}$ 的主成分了（因為這些向量直接解釋了 $\\mathbf{X}$ 中特徵間的共變異傾向）。你也懂得如何利用 PCA 找出的主成分來對數據去關聯，這可是一個不小的成就！ 從下一節開始，我們將從美麗的理論基礎走向實際的 PCA 應用。我將簡單分享 2 個透過 PCA 解析真實數據的例子。閱讀完該節後，你也能用最有效率的方式分析自己感興趣的數據並獲得無數有趣的洞見。 踏入荒野：實際應用 PCA 來解析真實數據 閱讀完前面兩節，你現在已經能夠直觀且正確地理解 PCA 了。你也學會如何利用 NumPy 找出數據 $\\mathbf{X}$ 的共變異數矩陣 $\\mathbf{K}$ 並對其進行特徵拆解、建立基底變更矩陣 $\\mathbf{Q}$、$\\mathbf{Q}&#94;\\mathsf{T}$ 並將數據線性降維以及去關聯。這些是 PCA 最核心的概念與技巧。在掌握這些基礎以後，讓我們用前面已經展示過的 scikit-learn 來提升分析數據的效率。 以下是幾個網路上常被用來展示 PCA 概念的數據集： 鳶尾花瓣 The Iris Dataset 真實人臉 The Olivetti faces 手寫數字 Handwritten Digits 這些數據集非常有名，而它們最棒的地方在於取得容易，且網路上也有大量的相關教學。我鼓勵有興趣的讀者稍後自行點擊以上連結，深入了解如何透過 PCA 來解析這些數據。但我強烈建議： 要內化並掌握某個數據分析技巧，將其應用在自己感興趣的數據與問題之上。 而這也是我在本章打算展示給你看的事情。閱讀過我其他文章的讀者們想必都清楚，任何機器學習模型或是 Python 函式庫說到底都只是一種 工具 ；最重要的不是學習如何使用某個工具，而是了解該工具如何幫助你達成 你想要的目的 。以下是一些過往案例： 直觀理解 GPT-2 語言模型並 生成金庸武俠小說 淺談神經機器翻譯 & 用 Transformer 英翻中 用 CartoonGAN 及 TensorFlow 生成新海誠動畫 用 BERT：NLP 界的巨人之力 進行假新聞分類 除了使用的「工具」以外，你可以看到我的文章充滿著「目的性」。畢竟，真正有用的是那些能被實際用來解決你手邊問題的工具。秉持著這個想法並冒著被視為宅宅的風險， 與其使用隨處可見的數據集然後展示乏善可陳的結果，在這節我將 PCA 套用在自己喜歡的一款多人線上遊戲： 英雄聯盟（ L eague o f L egends，常被簡稱為 LOL） 的 公開數據 之上。這應該是你最想不到會被拿來當作 PCA 案例的數據。 以下就是該遊戲目前所有可供使用的 147 名英雄角色（champions）： 您的瀏覽器不支援影片標籤，請留言通知我：S # 使用方便的資料分析函式庫 pandas 處理 CSV 數據 import pandas as pd from IPython.display import display # 將事先預處理過的英雄數據讀取為 pandas 的 DataFrame # 你可以從同樣的 url 獲得本文 demo 的數據 df = pd . read_csv ( \"https://bit.ly/2FkIaTv\" , sep = \" \\t \" , index_col = \"名稱\" ) print ( \"df.shape:\" , df . shape ) # 展示前 5 rows print ( \"前五名英雄數據：\" ) display ( df . head ( 5 )) # 顯示各特徵的平均與標準差 print ( \"各特徵平均與標準差：\" ) df_stats = df . describe () . loc [[ 'mean' , 'std' ]] df_stats . style . format ( \" {:.2f} \" ) df.shape: (147, 12) 前五名英雄數據： .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 類型 攻擊距離 魔力 魔力回復 魔力提升 生命提升 生命 生命回復 移動速度 物理攻擊 物理防禦 魔法防禦 名稱 厄薩斯 鬥士 175 0 0.0 0 90 580.0 3.0 345 60.0 38.0 32.1 阿璃 法師 550 418 0.8 25 92 526.0 6.5 330 53.0 20.9 30.0 阿卡莉 刺客 125 200 0.0 0 95 575.0 8.0 345 62.4 23.0 37.0 亞歷斯塔 坦克 125 350 0.8 40 106 573.4 8.5 330 61.1 44.0 32.1 阿姆姆 坦克 125 287 0.5 40 84 613.1 9.0 335 53.4 33.0 32.1 各特徵平均與標準差： 攻擊距離 魔力 魔力回復 魔力提升 生命提升 生命 生命回復 移動速度 物理攻擊 物理防禦 魔法防禦 mean 326.39 309.66 0.57 33.03 88.52 555.83 6.60 336.31 59.37 30.06 31.13 std 196.52 115.40 0.28 16.62 6.65 37.33 1.78 7.56 6.17 6.72 1.71 對熟悉此遊戲的玩家們（players）而言，我相信這數據集本身就顯得十分有趣並值得深入探索了。不過在這篇文章裡，我將聚焦在 PCA 身上而不會進行 探索性數據分析 EDA 。另外，我會用 資料科學家的 pandas 實戰手冊 裡頭闡述過的技巧來處理這些英雄數據。如果你想要用最短的時間上手 pandas，稍後可以自行 前往閱讀該篇文章 。 現在我們手上握有每位英雄的詳細資料，包含了英雄名稱、所屬類型以及 11 種屬性值（特徵值）。除非有什麼領域知識（Domain Knowledge）讓你確信某個特徵 $f_i$ 的單位變異比另個特徵 $f_j$ 的單位變異要來得重要，在套用 PCA 或是任何機器學習模型之前，你都應該將數據 正規化（Normalization） ，也就是將每個數據點 $x$（此例中為一名名英雄向量） 減去平均後除以 標準差（Standard Deviation） ： $$ \\begin{align} z &= \\operatorname{normalize}(x)\\\\ &= (x - \\mu) / \\sigma \\end{align} $$ 數據正規化的實作也十分容易： \"\"\" 將英雄數據正規化。使用對的 API 能幫我們省下不少時間。 這邊為了教學目的，同時呼叫 scikit-learn API 並手動計算比較結果 \"\"\" from sklearn.preprocessing import StandardScaler from numpy.testing import assert_almost_equal # 將類型以外的 11 個特徵全取出 X = df . iloc [:, 1 :] # (n_samples, n_features) # 使用 scikit-learn 內建的 API 正規化 scaler = StandardScaler () Z_sk = scaler . fit_transform ( X ) # 注意維度 # 手動正規化當然也能得到跟 scikit-learn API 相同的結果 # 注意我們有所有英雄數據（母體）而非抽樣，自由度 = 0 Z = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 , ddof = 0 ) assert_almost_equal ( Z , Z_sk ) # 更新我們的 DataFrame df . iloc [:, 1 :] = Z # 展示前 5 rows print ( \"正規化後前五名英雄數據：\" ) display ( df . head ( 5 ) . style \\ . format ( \" {:.2f} \" , subset = df . columns [ 1 :])) # 顯示各特徵的平均與標準差 print ( \"各特徵平均與標準差：\" ) df_stats = df . describe () . loc [[ 'mean' , 'std' ]] df_stats . style . format ( \" {:.2f} \" ) 正規化後前五名英雄數據： 類型 攻擊距離 魔力 魔力回復 魔力提升 生命提升 生命 生命回復 移動速度 物理攻擊 物理防禦 魔法防禦 名稱 厄薩斯 鬥士 -0.77 -2.69 -2.02 -1.99 0.22 0.65 -2.03 1.15 0.10 1.19 0.57 阿璃 法師 1.14 0.94 0.84 -0.48 0.52 -0.80 -0.06 -0.84 -1.04 -1.37 -0.66 阿卡莉 刺客 -1.03 -0.95 -2.02 -1.99 0.98 0.52 0.79 1.15 0.49 -1.05 3.45 亞歷斯塔 坦克 -1.03 0.35 0.84 0.42 2.64 0.47 1.07 -0.84 0.28 2.08 0.57 阿姆姆 坦克 -1.03 -0.20 -0.23 0.42 -0.68 1.54 1.35 -0.17 -0.97 0.44 0.57 各特徵平均與標準差： 攻擊距離 魔力 魔力回復 魔力提升 生命提升 生命 生命回復 移動速度 物理攻擊 物理防禦 魔法防禦 mean -0.00 -0.00 0.00 -0.00 0.00 -0.00 0.00 0.00 0.00 0.00 0.00 std 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 稍微有接觸過電玩的讀者們應該都能夠直觀地解讀這些特徵各自代表的涵義。注意正規化前後的數據 $\\mathbf{X}$ 與 $\\mathbf{Z}$ 維度皆為 (n_samples, n_features) ，這也是實務上你最常遇到的情境。你也可以看到正規化後 $\\mathbf{Z}$ 的各特徵平均皆為 $0$、標準差為 $1$。 這節最重要的概念是這裡的每位英雄都對應到 11 維特徵空間 $\\Re&#94;{11}$ 裡頭的一個特定向量（Vector）。儘管生活在 $\\Re&#94;3$ 的我們無法描繪出 $\\Re&#94;{11}$，你可以想像這是一個非常龐大的向量空間。這空間大到一個被隨機初始化的 $\\Re&#94;{11}$ 向量 $\\vec{v}$ 會長得像這些英雄向量的機率近似於 $0$。因此： 我們可以合理地假設這些英雄數據存在於一個被嵌入在高維空間 $\\Re&#94;{11}$ 裡頭的低維空間 $\\Re&#94;{k}$，且 $k$ 遠小於 $11$。 再換句話說，我們並不需要整整 11 個數字來形容一個英雄，只需要 $k$ 個 具有代表性的 數字就好。這正是機器學習、尤其是近年 深度學習（Deep Learning） 領域一直信奉著的 流形假設（Manifold Hypothesis） 。 而既然我們不需要那麼多個數字來表示一個英雄的特性，我們可以將數據 $\\mathbf{X}$ 降維來取得這些英雄的新 $k$ 維表徵。現在假設這些英雄所對應的向量存在於 $\\Re&#94;2$，即 $k = 2$。在學會 PCA 以後，你的第一反應應該是設法找出數據前兩大主成分 $\\{\\vec{pc_1}, \\vec{pc_2}\\}$ 並將數據投影到上去以得到新的 2 維（主）成分表徵 $\\mathbf{L}$。 透過 scikit-learn 的 API，線性降維是件輕鬆寫意的差事： \"\"\" 透過 scikit-learn 將 11 維的 LOL 英雄數據降到 2 維 \"\"\" from sklearn.decomposition import PCA # 我們只要最大的兩個主成分。scikit-learn 會自動幫我們 # 依照 eigenvalue 的大小排序共變異數矩陣的 eigenvectors n_components = 2 random_state = 9527 pca = PCA ( n_components = n_components , random_state = random_state ) # 注意我們是對正規化後的特徵 Z 做 PCA L = pca . fit_transform ( Z ) # (n_samples, n_components) # 將投影到第一主成分的 repr. 顯示在 x 軸，第二主成分在 y 軸 plt . scatter ( L [:, 0 ], L [:, 1 ]) plt . axis ( 'equal' ); 透過幾行非常簡單的程式碼，我們成功地將 11 維的英雄數據降維並表達成更具代表性的 2 維主成分表徵了。讓我們利用過去幾節所學，解讀一下這兩個主成分所代表的潛在意涵： \"\"\" 解析英雄數據的前兩大主成份所代表的意涵。 顏色越突出代表其絕對值越大 \"\"\" pcs = np . array ( pca . components_ ) # (n_comp, n_features) df_pc = pd . DataFrame ( pcs , columns = df . columns [ 1 :]) df_pc . index = [ f \"第 {c} 主成分\" for c in [ '一' , '二' ]] df_pc . style \\ . background_gradient ( cmap = 'bwr_r' , axis = None ) \\ . format ( \" {:.2} \" ) #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col0 { background-color: #0000ff; color: #f1f1f1; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col1 { background-color: #4a4aff; color: #f1f1f1; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col2 { background-color: #7a7aff; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col3 { background-color: #9494ff; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col4 { background-color: #ffd8d8; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col5 { background-color: #ff7e7e; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col6 { background-color: #ff8484; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col7 { background-color: #ff7676; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col8 { background-color: #ff6c6c; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col9 { background-color: #ff7070; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row0_col10 { background-color: #ff7272; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col0 { background-color: #b8b8ff; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col1 { background-color: #ff4444; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col2 { background-color: #ff0000; color: #f1f1f1; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col3 { background-color: #ff1c1c; color: #f1f1f1; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col4 { background-color: #ff7a7a; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col5 { background-color: #ffa4a4; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col6 { background-color: #ffb6b6; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col7 { background-color: #d6d6ff; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col8 { background-color: #e8e8ff; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col9 { background-color: #fff8f8; color: #000000; } #T_6d221378_2f7e_11ea_b199_0242ac1c0002row1_col10 { background-color: #ffcece; color: #000000; } 攻擊距離 魔力 魔力回復 魔力提升 生命提升 生命 生命回復 移動速度 物理攻擊 物理防禦 魔法防禦 第一主成分 0.43 0.28 0.19 0.14 -0.14 -0.32 -0.31 -0.33 -0.35 -0.34 -0.34 第二主成分 0.072 -0.43 -0.56 -0.5 -0.32 -0.24 -0.21 0.014 -0.021 -0.08 -0.16 我們之前就說過主成分本身的方向就解釋了原始數據中多個特徵之間的共變異傾向。透過簡單的觀察，你可以發現： 第一主成分代表著「遠攻」或是「魔力型」英雄。這是因為 x 值越大會讓攻擊距離越長、魔力相關屬性皆有所提升；其代價是生命與防禦相關的數值降低。這些是被俗稱為「脆皮」的血少攻高類型英雄。 第二主成分則是以降低魔力相關屬性的數值以換取更遠的攻擊距離以及更高跑速的物理英雄。 從第一主成分我們可以看出攻擊距離以及魔力造就了英雄聯盟裡最大的整體英雄數值差異，而這也或多或少反映了設計該遊戲的公司以及設計師理念。這可是個非常有趣且實用的洞見！不過雖然我們已經知道第一主成分解釋了最多的數據變異，但實際上它到底解釋（explain）了多少？有沒有什麼辦法可以量化這件事情？ 變異解釋率（Explained Variance Ratio）是一個常被用來量化某個主成分 $\\vec{v_i}$ 解釋數據變異程度的指標： $$ \\begin{align} \\frac{\\lambda_i}{\\sum \\lambda_i} \\ |\\ \\mathbf{K}\\vec{v_i} = \\lambda_i \\vec{v_i} \\end{align} $$ 別忘了 $\\mathbf{K}$ 是數據 $\\mathbf{X}$ 的共變異數矩陣，而 $\\lambda_i$ 是其 Eigenvector $\\vec{v_i}$ 所對應的 Eigenvalue。將某個主成分 $\\vec{v_i}$ 的 $\\lambda_i$ 拿去除以所有 Eigenvalues 的總和 $\\sum \\lambda_i$ 就能得到該主成分的變異解釋率。如果你還記得我們前面說過主成分的 Eigenvalue 解釋了主成分變異趨勢的大小，那麼這個公式對你來說就十分直覺了。 我們可以看看前 10 大主成分各自解釋了多少英雄數值的變異： pca_10d = PCA ( 10 , random_state = random_state ) pca_10d . fit ( Z ) np . round ( pca_10d . explained_variance_ratio_ , 2 ) array([0.41, 0.2 , 0.09, 0.08, 0.05, 0.05, 0.04, 0.03, 0.02, 0.02]) 端看你的應用情境，累加的變異解釋率也可以幫助我們決定該把原始的 $N$ 維數據降到幾維（即決定 $K$）以維持足夠的數據變異，進而讓降維後的表徵具有足夠的代表性。在我們的例子裡頭，前兩個主成分就已經解釋了 100 多位英雄數值中近 $6$ 成的差異（$0.41 + 0.2$），是一個相當不錯的降維結果。 這可是你把任意兩個原始屬性（比方說生命與物理攻擊）作為數據的 2 維成分表徵無法得到的美麗結果。如果我們將 PCA 降維後的結果結合每個英雄所屬的主要類型的話，一切會變得更加有趣。每位英雄都可以被歸類在以下 6 個類別： df [ '類型' ] . unique () array(['鬥士', '法師', '刺客', '坦克', '射手', '輔助'], dtype=object) 你可以猜一猜，這是我們剛剛對第一主成分的解讀： 第一主成分代表著「遠攻」或是「魔力型」英雄。x 軸的值越大會讓攻擊距離越長、魔力相關屬性皆有所提升；其代價是生命與防禦相關的數值降低。這些是被俗稱為「脆皮」的血少攻高類型的英雄。 依照這個敘述，假設我們將每位英雄投影到第一主成分所得到的特徵值 $l_1$ 描繪在水平 x 軸上，你覺得哪種類型的英雄會有比較大的 x 值？如果你有些電玩遊戲經驗，應該可以想像以下這兩類型的英雄相當符合第一主成分的描述： 射手（Marksman）：定位為遠程攻擊，生命值少，玻璃大砲 法師（Mage）：大多也為遠程攻擊，魔力為主要傷害來源 我們可以看看這個猜想有多準確。下面依序展示每個英雄類型的 2 維主成分表徵分布。前兩類型即為射手與法師： 您的瀏覽器不支援影片標籤，請留言通知我：S 我們可以看到跟其他類型的英雄相比，射手以及法師英雄的確普遍具有較大的 x 值，代表它們相當符合第一主成分的特性：遠程攻擊、魔法傷害高。透過幾行程式碼，在沒有介紹任何英雄的情況下我們就能有效率地發掘出顯著且有趣的英雄特性，這正是 PCA 的強大之處！跟這兩類型英雄相反，你也可以發現動畫中第四個類型：鬥士（Fighter）普遍擁有較小的 x 值。這代表它們魔力較低但擁有較高的生命以及防禦力。 如果你是此遊戲的玩家，也能透過英雄頭像感受一下結果。我相信你會同意從 PCA 得到的發現不只相當有趣，還符合我們對這些英雄特性的理解。這例子也告訴我們領域知識（Domain Knowledge）的重要。以這邊的例子而言，所謂的領域知識自然是你對此遊戲以及英雄特性的理解。如果你完全沒玩過此遊戲， PCA 能幫助你快速地了解這些英雄屬性的本質；但領域知識能幫助你驗證數據得出的結果是否符合常理並讓你找到更多有趣的洞見。 這也是我們在 如何用 30 秒了解台灣發展與全球趨勢 就已經看過的重要數據概念： 不靠數據無法了解世界，但光靠數據也無法了解世界。 ─ 漢斯・羅斯林，《真確》 我鼓勵你認真思考如何將 PCA 應用到自己感興趣或是熟悉的數據之上，並嘗試利用自己的世界觀以及領域知識，解讀 PCA 帶給你的分析結果。相信我，只要結合領域知識以及數據分析能力，你將獲得專屬於自己的全新洞見。 讓我再簡單介紹一個 PCA 案例。這次直接用 PCA 解析你剛剛已經看過的英雄頭像圖片： 您的瀏覽器不支援影片標籤，請留言通知我：S 你可以自行前往 英雄聯盟的開發者頁面 或是 右鍵下載 我為你準備好的 lol_champion_images.npy 。下載後可以這樣將所有的英雄頭像讀取為 4 維 NumPy 陣列： \"\"\" 包含所有英雄頭像圖片的陣列。 維度分別為 (n_champions, height, width, channel) \"\"\" images = np . load ( \"lol_champion_images.npy\" ) print ( \"images.shape:\" , images . shape ) flatten_images = np . reshape ( images , ( images . shape [ 0 ], - 1 )) print ( \"flatten_images.shape:\" , flatten_images . shape ) images.shape: (147, 120, 120, 3) flatten_images.shape: (147, 43200) 你可以看到一張小小的英雄頭像就有整整 43200 個像素（維度），剛剛的 11 維英雄數據跟它比完全是小巫見大巫。但我們完全可以依樣畫葫蘆，用 PCA 將這些處在超高維空間的圖片向量維降並得到不錯的結果，而這是代數觀點上的一大勝利。著有線代經典 《Introduction to Linear Algebra》 的著名數學家 Gilbert Strang 曾說過： 線性代數真的是一門很棒的學問。我的意思是，如果所有東西都是線性的，還有什麼事情會出錯呢？ ─ Gilbert Strang 現在馬上讓我們用 PCA 將高維的英雄頭像做線性降維： from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # 將有 43,200 像素的圖片降到 108 維度 random_state = 9527 pca = PCA ( n_components = 108 , random_state = random_state ) # 我們可以建構一個標準化 -> 降維的 pipeline pipe = make_pipeline ( StandardScaler (), pca ) transformed_images = pca . fit_transform ( flatten_images ) print ( \"transformed_images.shape:\" , transformed_images . shape ) transformed_images.shape: (147, 108) 程式碼非常直覺，且圖片的維度一下子減少了許多。你可以想像我們將這些高維圖片中最重要的資訊投影到了低維空間，類似的降維有很多應用，比方說提升數據傳輸效率、萃取關鍵資訊等等。 我們也可以看看這 108 個主成分解釋了多少數據的變異： \"\"\" 計算主成分累積的疊加變異解釋率 \"\"\" cum_explained_var_ratio = np . cumsum ( pca . explained_variance_ratio_ ) plt . plot ( cum_explained_var_ratio ) plt . xlabel ( '# principal components' ) plt . ylabel ( 'cumulative explained variance' ); 你可以看到，我們只用了原本的 0.25% 的維度數目，就幾乎完整地解釋了這些英雄頭像的變異： 108. / 43200 0.0025 這可是很驚人的結果！而前 30 個主成分就幾乎解釋了 70% 的數據變異。且因為每個主成分自身就代表著一個在 43200 維度裡頭的高維向量，我們可以把它們也視為一個個的圖片視覺化出來： \"\"\" 將前 30 個英雄頭像圖片的主成分 rescale 並繪出 \"\"\" # 為了方便視覺解讀 rescale 主成分 from sklearn.preprocessing import minmax_scale scaled_comps = minmax_scale ( pca . components_ , axis = 1 ) # 繪製前 30 個主成分 fig , axes = plt . subplots ( 3 , 10 , figsize = ( 12 , 4 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i , ax in enumerate ( axes . flat ): c = scaled_comps [ i ] ax . imshow ( c . reshape ( 120 , 120 , 3 )) 你可以看到這裡的每個主成分都捕捉到了某種形式的頭像資訊。這也是你隨機選擇 30 個像素無法做到的事情。你也可以看到第一主成分包含了所有英雄頭像皆有的黑色邊框；而我個人覺得第一排從左數來第二與第六主成分都相當貼近英雄頭像，你覺得呢？ 我們也可以將新得到的 108 維主成分表徵重新投影回原來 43200 維的像素空間看看重建成果： \"\"\" 將新得到的 108 維成分表徵重新投影回原高維空間並比較前後結果 \"\"\" reconstructed_images = pca . inverse_transform ( transformed_images ) reconstructed_images = minmax_scale ( reconstructed_images , axis = 1 ) shape = ( 120 , 120 , 3 ) for i in range ( 3 ): fig , ax = plt . subplots ( 2 , 10 , figsize = ( 12 , 6 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace =- 0.75 , wspace = 0.1 )) for j in range ( 10 ): idx = i * 10 + j ax [ 0 , j ] . imshow ( images [ idx ] . reshape ( shape )) ax [ 1 , j ] . imshow ( reconstructed_images [ idx ] . reshape ( shape )) ax [ 0 , 0 ] . set_ylabel ( '43,200-Dim \\n Image' ) ax [ 1 , 0 ] . set_ylabel ( '108-Dim \\n Reconstruction' ); 投影後的結果雖不完美，但也已經相當不錯了。畢竟我們只用了 0.25% 的維度數目來重新表達這些圖片。原圖跟投影後的圖片之間的差距，就是我們之前講過的重建錯誤。在這個例子中為 $RE_{108}$。 好啦，透過英雄聯盟的案例分析，我想你現在也能如法炮製並實際應用 PCA 在你自己感興趣的數據上面啦！我們這趟漫長的 PCA 之旅也即將邁入尾聲。下節讓我簡單總結一下我們一路走來學了些什麼。 在萬物皆向量的時代，如何瞭解事物本質？ 首先，由衷地感謝你一路跟隨我走到這裡。我得承認，我在本文開頭要求你須先具備的基礎知識並不算少，但如果你有好好聽從我的建議奠定好基礎並一步步跟上來，我相信這趟 PCA 旅程也會讓你收穫滿滿。 閱讀完本文，你現在應該已經能夠： 用 PCA 降維並直觀理解為何將數據投影到主成分上能最小化重建錯誤 用 PCA 去關聯並了解如何進行基底變更以將數據重新表示成主成分表徵 直觀理解共變異數的物理意義並了解如何透過座標轉換使共變異數為零 了解為何數據的共變異數矩陣的 Eigenvectors 為 PCA 找出的主成分 理解為何 $\\mathbf{B}_{standard}$ 並非萬能以及如何找出適當的基底 $\\mathbf{B}_{pc}$ 重新表述數據 透過 NumPy 以及 scikit-learn 將 PCA 應用在自己想分析的數據之上 直觀理解並能欣賞 PCA 背後的線性代數與統計概念 我希望你也能感受到現在自己的腦袋裡多了不少有用的知識。PCA 的核心精神是為手邊的數據選擇一個最好的觀察視角，給予數據全新且最具意義的表徵（representation）。在這個萬物皆能被以一個數值向量表示並解析的時代，這個核心精神能幫助你解構任何數據，瞭解事物的本質。這也是我撰寫此文希望能讓更多人掌握此精神的動力之一。 您的瀏覽器不支援影片標籤，請留言通知我：S t-SNE 也可以用來視覺化高維數據，但需小心解讀 雖然本文篇幅有限無法詳述，在熟悉 PCA 這個線性降維技巧之後，你已經可以開始了解其他（非）線性的降維技術了。比如知名的 t-SNE 、 UMAP 、 NMF 以及 Autoencoder 。希望你離去之後能夠實際嘗試應用 PCA 來分析自己或是企業的數據，並將得到的洞見與我分享。另外如果這篇文章有幫助到你，還請不吝花個幾秒鐘分享給對機器學習或是數據分析有興趣的朋友閱讀，幫我將這些知識傳播給更多人。 那就這樣啦！我們下次見。 或許當年發明 PCA 的皮爾森也沒料想到，在一百年後 AI 滿天飛的年代，如此簡單的主成分分析仍然佔有它自己的一席之地。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/essence-of-principal-component-analysis.html","loc":"https://leemeng.tw/essence-of-principal-component-analysis.html"},{"title":"給所有人的深度學習入門：直觀理解神經網路與線性代數","text":"這是篇幫助你直觀理解神經網路的科普文。讀完本文，你將能夠深刻地體會神經網路與線性代數之間的緊密關係，奠定 AI 之旅的基礎。 （小提醒：因本文圖片與動畫皆為黑色背景，強烈推薦用左下按鈕以 Dark Mode 閱讀本文） 這是個對人工智慧（ A rtificial I ntelligence, AI）趨之若鶩的時代。此領域近年的蓬勃發展很大一部份得歸功於 深度學習 以及 神經網路 的研究。現行的深度學習框架（framework）也日漸成熟，讓任何人都可以使用 TensorFlow 或 PyTorch 輕鬆建立神經網路，解決各式各樣的問題。 舉例而言，你在 30 秒內就可訓練出一個能夠辨識數字的神經網路： # 此例使用 TensorFlow，但各大深度學習框架的實現邏輯基本上類似 import tensorflow as tf # 載入深度學習 Hello World: MNIST 數字 dataset mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # 建立一個約有 10 萬個參數的「小型」神經網路 # 在現在模型參數動輒上千萬、上億的年代，此神經網路不算大 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # 選擇損失函數、optimizer model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # 訓練模型 model . fit ( x_train , y_train , epochs = 5 ) # 訓練後的 NN 在測試集上可得到近 98% 正確辨識率 model . evaluate ( x_test , y_test ) # 實際測試結果 # loss: 0.0750 - accuracy: 0.9763 是的，扣除註解不到 15 行就可以把讀取數據、訓練 model 以及推論全部搞定。這邊秀出程式碼只是想讓你感受一下現在透過框架建立一個神經網路有多麽地「簡單」。事實上，這也是絕大多數線上課程以及教學文章會教你的東西。對此數字辨識應用有興趣的讀者稍後也可自行參考 TensorFlow 的 Colab 筆記本 。 我等等要秀給你看的任何一個神經網路都要比這個 model 還簡單個一萬倍（以參數量而言），但觀察並理解這些「簡單」神經網路，將成為你的 AI 旅程中最有趣且實用的經驗之一。 一些有用的背景知識 文中有很多動畫帶你直觀理解神經網路（ N eural N etwork, 後與 NN 交替使用）與 線性代數（Linear Algebra） 之間的緊密關係。以下知識能幫助你更容易地掌握本文內容： 能讀懂文章開頭建立 NN 的 Python 程式碼 了解線上課程都會教的 超基本 NN 概念 何謂 全連接層（Fully Connected Layer） 常見的 activation functions 如 ReLU 基本的線性代數概念如矩陣相乘、向量空間 別擔心，這些是 nice-to-have。我等等會盡量囉唆點，讓你就算空手而來也能滿載而歸。 您的瀏覽器不支援影片標籤，請留言通知我：S 一個簡單 NN 嘗試解決二元分類的過程 另外值得一提的是，本文主要展示 已經訓練好 的 NN，不會特別說明 訓練一個 NN 的細節。 深度學習框架操作容易，但你真的了解神經網路嗎？ 再次回到文章開頭的 MNIST 例子。 我們剛剛使用 TensorFlow 高層次 API Keras 建立了一個神經網路 model ： # 在 TensorFlow 裡全連接層被稱作 Dense # 因為這些層之間的神經元「緊密」連接 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) 就跟你在多數教學文章裡頭會看到的一樣，現在要使用深度學習框架建立神經網路十分容易，只要當作疊疊樂一個個 layer 疊上去就好了。下圖則將此 model 用視覺上更容易理解的方式呈現： 輸入是 28*28 = 784 維的圖片像素，輸出則是 10 個數字類別的簡單 2-layers NN 先不看神經網路最左邊的輸入層。右側兩 layers 的每個神經元（neuron）都跟 前一層的每個 神經元相連，所以被稱之為 全連接層（ F ully C onnected Layer，後簡稱 FC） 。而因為相連 緊密 的特性，TensorFlow 將它們稱作 Dense layer。 我們可以透過 summary 函式輕鬆計算這個神經網路有多少參數： model . summary () Model: \"sequential_8\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 784) 0 _________________________________________________________________ dense_9 (Dense) (None, 128) 100480 _________________________________________________________________ dropout_1 (Dropout) (None, 128) 0 _________________________________________________________________ dense_10 (Dense) (None, 10) 1290 ================================================================= Total params: 101,770 Trainable params: 101,770 Non-trainable params: 0 _________________________________________________________________ 儘管參數量超過 10 萬，此 model 是個在深度學習領域裡只能被歸類在 Hello World 等級的可憐 NN。畢竟這世界很瘋狂， 我們以前討論過的 BERT 以及 GPT-2 都是現在 NLP 界的知名語言模型，而它們可都是擁有 上億 參數的強大 NN。這些模型的大小可有 model 的 100 倍之大。 但先別管 BERT 或 GPT-2 了，就算是這個 Hello World 等級的 NN，你真的覺得你對它的運作機制有足夠的理解嗎？ 講白點，儘管現在路上隨便拉個人都會使用 TensorFlow 或是 PyTorch 來建立神經網路，許多人（包含剛入門的我）對最基本的神經網路的運作方式都沒有足夠 直觀 的理解。 構成本文的關鍵三要素：矩陣運算、二元分類以及神經網路 為了讓你能夠直觀且正確地理解神經網路，我將透過二元分類（Binary Classification）任務說明其與線性代數之間的緊密關係。前言很長，但如果你想要了解神經網路的本質，或是想要為自己之後的 AI 學習之旅打下良好基礎，那我會建議你繼續往下閱讀：） 用二元分類連結神經網路 & 線性代數 不只出現在深度學習領域， 二元分類 是 機器學習（Machine Learning） 領域裡一個十分基本的任務，其目標是把一個集合裡的所有數據點（data points）依照某種分類規則劃分成 兩 個族群或類別（classes）。經典的例子有我們之前看過的 貓狗圖像辨識 。在這篇文章裡，我假設所有數據點 最多只有兩個維度（dimensions） 。 如果你讀過之前的文章，可能會覺得這假設是在「羞辱」我們。畢竟我們已用過神經網路來： 執行假新聞偵測（BERT） 生成新金庸小說（GPT-2） 把英文翻成中文（Transformer） 生成新海誠動畫（CartoonGAN） 這些任務的複雜程度要比 二維 二元分類多了幾個數量級（當然也非常有趣。你稍後可以點擊相關連結深入了解）。但這篇文章之所以選擇二維二元分類作為目標任務正是因其簡單明暸，我們將能以此窺探神經網路的本質（essence）。 以下是一個包含兩條曲線的資料集： 您的瀏覽器不支援影片標籤，請留言通知我：S 包含兩類別的二維雙曲線資料集 在此雙曲線資料集裡，兩條曲線分別來自不同類別，各自包含了 100 個數據點 $x$。每個數據點 $x$ 都可以很自然地以二維 $\\left (x_{coord}, y_{coord} \\right )$ 座標來呈現。右下角也標注了兩曲線所屬類別：黃點的標籤 $y = 0$，藍點則為 $1$。 另外，圖中也描繪了此向量空間中的基底向量： x 軸上藍色的 $\\vec{i}$ y 軸上紅色的 $\\vec{j}$ 等等你會看到，這兩個基底向量（basis vector）能夠幫助我們了解神經網路的運作方式。至於要如何分類這個資料集呢？回想一下 之前 AI For Everyone 的重要概念 ： 目前多數的機器學習以及 AI 應用本質上都是讓電腦學會一個映射函數，幫我們將輸入的數據 x 轉換到理想的輸出 y。 ─ Andrew Ng 套用相同概念，要處理這個分類任務，我們要問的問題就變成「給定所有藍點與黃點 $x = \\left (x_{coord}, y_{coord} \\right )$，我們能不能找出一個 $x$ 的函數 $f(x)$，將這些二維數據 $x$ 完美地 轉換 到它們各自所屬的一維標籤 $y$ 呢？」 換句話說，我們想要找出一個函數 $f(x)$，使得以下式子成立： \\begin{align} f(x) & = f(\\begin{bmatrix} x_{coord} \\\\ y_{coord} \\end{bmatrix}) \\\\ & = f(\\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix}) \\\\ & = y \\end{align} 如果我們能找到符合這個條件的 $f(x)$，就能在一瞬間預測出某個數據點 $x_{i}$ 比較可能是哪個類別了。我們有非常多種 model $f(x)$ 的方法，但在線性代數的世界裡，我們可以用矩陣運算的形式來定義一個 $f(x)$： \\begin{align} f(x) & = W x + b \\\\ & = \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix} x + b \\\\ & = \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} + b \\\\ & = y \\end{align} 我希望你至少還記得國中、高中或是大學裡任何一位數學老師的諄諄教誨。 在上面的式子裡： $x$ 是一個二維 column vector $W$ 是一個 1 × 2 的 權重矩陣（weight matrix） $b$ 為偏差（bias），是一個純量（scalar） 如果我們先暫時忽略 $b$，事實上 $f(x)$ 對輸入 $x$ 做的 轉換 跟深度學習領域裡時常會使用到的全連接層（FC）的運算是完全相同的。換句話說，使用一層 FC 的 NN 基本上就是在做矩陣運算（假設激勵函式為線性、偏差為 0）。因此用矩陣運算定義的 $f(x)$ 與神經網路之間有非常美好的對應關係： 您的瀏覽器不支援影片標籤，請留言通知我：S 1 × 2 矩陣運算與全連接層的對應關係 沒錯，透過矩陣運算，我們剛剛建立了這世上最簡單的 1-Layer 神經網路！而之所以是 1-Layer，原因在於 NN 的第一層為原始數據（raw data），第二層才是我們新定義的神經網路（一層 FC）。矩陣運算跟 FC 的對應關係家喻戶曉，但這應該是你第一次看到兩者共舞。 從 線性代數 的角度來看，我們透過矩陣運算將二維的 $x$ 轉換成一維的 $y$；而以 神經網路 的角度檢視，我們則是將以二維向量表示（represent）的數據點 $x$ 透過與權重 $W$ 進行加權總和後得到新的一維表徵（representation）$y$。這是常有人說神經網路在做 表徵學習（Representation Learning） 的原因。 另外值得一提的是矩陣 $W$ 裡的每個參數 $w_{mn}$ 實際上就對應到 NN 某一層中第 $n$ 個神經元（neuron）連到其下一層中第 $m$ 個神經元的 邊（edge） 。別擔心， 在本文的最後面 還會有動畫幫助你記憶此對應關係。 就跟文章開頭看到的一樣，要使用 TensorFlow 定義這個 1-Layer NN 也十分容易： import tensorflow as tf model = tf . keras . models . Sequential () # 將 2 維 input 轉成 1 維 output model . add ( tf . keras . layers . Dense ( 1 , use_bias = False , input_shape = ( 2 , ))) model . summary () Model: \"sequential_6\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ 扣除偏差後整個神經網路的確只剩 2 個參數。為了讓你加深印象，讓我們將一些數字代入 $W$ 與 $x$： \\begin{align} W = \\begin{bmatrix} 1 & 5 \\end{bmatrix} \\\\ x = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} \\end{align} 接著再看一次剛剛的運算過程： 您的瀏覽器不支援影片標籤，請留言通知我：S 1 × 2 矩陣運算與全連接層的對應關係（實際數值） 這邊我刻意讓 $x_{1} = x_{2} = 3$。你可以清楚地看到不同大小的 $w_{mn}$ 可以讓不同維度、但相同值的 $x_{n}$ 給與最終輸出 $y_{m}$ 不同程度（權重）的影響力，這也是 $W$ 之所以被稱之為權重（weights）的原因。 你現在知道最基本的矩陣運算、神經網路以及兩者之間的緊密關係了。更美妙的是，如果你已經了解 線性代數的本質 ，就會知道函數 $f(x) = W x + b = y$ 事實定義了兩個簡單轉換來將二維的輸入 $x$ 依序轉換成一維輸出 $y$： 線性轉換：$W$ 位移：$b$ 你多年前可能也已看過 線性轉換（linear transformation, or linear map） 的正式數學定義： 這是所有線性轉換都具備的 可加性（additivity） 與 齊次性質（homogeneity） 。不過別擔心，在本文裡你不需了解這些定義也能直觀地理解線性轉換。用比較不嚴謹的說法，線性轉換會對其作用的向量空間（vector space）做 旋轉 、 伸縮 等「簡單」轉換。 要直觀瞭解這個概念，讓我們再次將幾個數字代入 $f(x)$ 的參數裡頭： \\begin{align} f(x) & = \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} + b \\\\ & = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} + (-2) \\end{align} 這次假設 $w_{11} = w_{12} = 1, b = -2$。另外別忘了我們剛學的，這個函數 $f(x)$ 可以被表示成到一個特定的 1-Layer NN。我們可以觀察這個 NN（即 $f(x)$）如何轉換二維空間裡頭的 $x$： 您的瀏覽器不支援影片標籤，請留言通知我：S 利用 1-Layer NN 將二維輸入轉換成一維輸出 這個 NN 做的轉換十分簡單，但是隱含了不少重要概念。 首先，以 神經網路 的角度解讀 $f(x)$ 的話，此二維向量空間裡頭的任一數據點 $ x = \\left (x_{coord}, y_{coord} \\right )$ 都對應到左上 NN 第一層（輸入層）中的 2 個神經元。權重 $W$ 則以 邊 的方式呈現，負責將第一層中 2 個神經元的值依照不同比重 送去啟動 （activate）下一層的神經元。啟動下層神經元後，NN 只要再將一個偏差值 $b$ 加到該神經元即完成此 layer 的轉換。 以 線性代數 的視角檢視 $f(x)$ 的話，裡頭的矩陣 $W$ 則定義了一個線性轉換。此轉換說明了原向量空間 $V_{original}$ 裡的 2 個基底向量 $\\vec{i}$、$\\vec{j}$ 在 轉換後 的一維空間 $V_{transformed}$ 中的目標位置。$w_{11} = 1$ 讓原 x 軸上的 $\\vec{i}$ 保持在「原位」；$w_{12} = 1$ 則將 $\\vec{j}$ 放到 $V_{transformed}$裡與 $\\vec{i}$ 一樣的位置。 此例中 $V_{transformed}$ 是一個跟 x 軸重疊的一維數線（Number line），你在動畫最後面也可以看到 $\\vec{i}$ 跟 $\\vec{j}$ 在此數線上的相同位置。為了讓轉換過後的 $\\vec{j}$ 能在指定的位置，y 軸順時針旋轉並轉換到了 $V_{transformed}$ 之上。你也能看到原二維空間裡頭的每個數據點 $x$ 都跟著 $\\vec{j}$ 一起被轉換成該數線上的一個值，而不再是二維座標。透過 $W$ 被轉換到一維空間以後，該數線上的每個數據點只要再被加上偏差 $b$ 就完成兩步驟的轉換。 我幫你用兩種角度檢視 $f(x)$，但這邊最重要的啟示是： 你可以用神經網路或是線性代數的角度來解讀 $f(x)$ 的作用，但兩者殊途同歸：它們實際上都是利用 $f(x)$ 將原始數據 $x$ 進行一系列幾何轉換後輸出理想的 $y$。 這也是為何 我們之前說神經網路本質上是一個映射函數 的原因。 怎樣的神經網路才是好的分類器？ 我們剛剛看到，1-Layer NN 可以透過一個 FC（線性轉換 $W$ + 位移 $b$）將二維輸入 $x$ 轉換成一維輸出 $y$。不過由於我們剛剛是隨意設定參數 $W$ 及 $b$，如果以分類器（classifier）的標準去評價該 NN 的話，其表現實在是令人不敢恭維。 讓我們快速回顧一下剛剛的 NN 做了怎樣的轉換，並計算分類準確率： 您的瀏覽器不支援影片標籤，請留言通知我：S 用 1-Layer NN 做二元分類（參數隨意設定） 我盡量讓所有動畫 speak for themselves，但希望你不介意我再嘮叨一下。 我們一樣透過 $f(x)$ 將兩組分別以黃色及藍色表示的二維數據點 $x$ 轉成一維的 $y$。轉換之後我們能找出一個 $y$ 值（此例中 $y = 1$），畫條垂直線將兩組輸出 $y$ 分開。這條分類界線（classification boundary）能使我們分對最多的 $x$。 我們可以用此分界線定出一個分類規則：任何數據點 $x$ 只要它的 $f(x) = y$ 在這條線的左邊，我們就預測其來自黃色曲線，反之則為藍色曲線。很明顯的，黃色在此界線的左側則是因其標籤數值（label）比藍線小。 溫馨提醒，這是我們剛剛用來轉換 $x$ 的神經網路 $f(x)$： \\begin{align} f(x) & = \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} + b \\\\ & = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} + (-2) \\end{align} 將 $W$ 設為 $[1, 1]$、$b$ 設定為 $-2$ 以後，$f(x)$ 函數對輸入 $x$ 做的轉換不至於錯得離譜，但也就只能把觀測到的 200 個數據點 $x$ 裡頭的 68 個點正確分類，其準確率（accuracy）只有 66%。很明顯的，要透過 $f(x)$ 來解決當前的二元分類任務，我們要問的問題變成： 這組參數的選擇是最佳解嗎？能不能找到一組最好的參數 $W$ 與 $b$，使得 $f(x)$ 最後的分類準確率最大？ 答案當然是肯定的 Yes，只要適當地運用以下數學概念與技巧就能做到： 微積分（Calculus） 梯度下降（Gradient Descent） 反向傳播演算法（Back Propagation） 這也是深度學習核心之一的 學習 過程。在此例的 $f(x)$ 裡頭，其參數 $W$ 與 $b$ 的實際數值會在模型的訓練過程中不斷地被修正以提升分類準確率，所以一般會被稱作可訓練的參數 $\\theta$。事實上，我們前面定義 $f(x)$ 時，並不只是定義一個特定的函數，而是一整個函數空間 $f_{\\theta}(x)$。此空間裡頭的每個函數雖然 架構 上都是先對 $x$ 做線性轉換再進行位移，但因其參數值 $\\theta$ 皆有所不同，實際上做的轉換也就有所差異。這也就意味著： 我們可以透過矩陣運算的形式定義一個神經網路架構 $f_{\\theta}(x)$，再透過微積分與反向傳播等方式，求得此架構裡能最能有效解決任務的參數 $\\theta$。 這是為何有人會跟你說要 真的 學好 深度學習 ， 線性代數 以及 微積分 很重要的原因。前者讓你定義出一個具有解決複雜問題「潛力」的數據轉換架構（神經網路），後者則讓你實際找出每個轉換步驟所需的細節（參數權重）。當然，就算你完全不懂後者，深度學習框架也能幫你搞定一切： # 文章開頭的 MNIST 範例程式碼片段 # 定義一個能夠解決問題的模型架構 model = ... # 設置學習實際參數所需的東西：損失函數、optimizer model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) # 訓練模型，取得最好的參數 theta model . fit ( x_train , y_train , epochs = 5 ) # 打完收工 不幸的是，深度學習框架的方便也成為很多人不明就裡的原因之一。 如本文開頭所述，我的目標是讓你可以透過線性代數來直觀地了解 已訓練好 的 NN 如何轉換數據並達成任務目標。因此底下展示的都是我先幫你訓練好的 NN（即我們已經知道最好的 $\\theta$ 為何）。你等等可以參考我在文後附上的連結，深入了解如何實際訓練一個 NN，找出一組好的參數 $\\theta$。 事不宜遲，讓我們馬上看看一個訓練好的 1-Layer NN 能把這個二元分類任務做到多好： 您的瀏覽器不支援影片標籤，請留言通知我：S 完美分類兩曲線的 1-Layer NN 簡單而完美。事實證明，只要使用適合的參數 $\\theta$，用矩陣形式定義的這個 1-Layer NN 能夠完美地將這兩條曲線分開。另外，如果你再稍微仔細觀察最初的雙曲線並結合前面談到的矩陣概念，就會發現 $W$ 這個線性轉換是非常有意思的： \\begin{align} W & = \\begin{bmatrix} w_{11} & w_{12} \\end{bmatrix} \\\\ & = \\begin{bmatrix} 0.01 & 1.67 \\end{bmatrix} \\end{align} 你可以用底下動畫再次觀察轉換前的二維向量空間。 你會發現黃藍雙曲線本身雖然看似複雜，其實只要用一條幾乎跟 x 軸重疊的 水平線 就能將兩者切開。這也就意味著二維數據點 $x = \\left (x_{coord}, y_{coord} \\right )$ 裡頭的第一個維度 $x_{coord}$ 的值幾乎不會影響 分類 兩曲線的決策。此 NN 學會將 $w_{11}$ 設成一個很小的值 $0.01$，讓原空間中 x 軸上的值都壓縮到幾乎等於 0。檢視動畫左上的 NN 示意圖，你會發現代表 $w_{11}$ 的 第一條邊 幾乎沒有將輸入層中第一個神經元的值 $x_{coord}$ 傳給輸出神經元。這也就代表 $x_{coord}$ 不太影響最後的 $y$ 值。 您的瀏覽器不支援影片標籤，請留言通知我：S 方便你對照，這邊再次展示轉換前的黃藍雙曲線 而因為一個數據點的第二個維度 $y_{coord}$ 才是用來判斷其所屬類別的重要信號（$y_{coord}$ 越大，藍線機率越大），且理想上轉換後的藍點標籤 $y$ 要大於黃點的 $y$，因此這個 NN 將 $w_{12}$ 設為一個相對 $w_{11}$ 大的正值，將原空間 $V_{original}$ 的 y 軸順時針旋轉，讓所有藍點的 $y$ 皆大於黃點的 $y$。很有意思，對吧？你可以把矩陣 $W$ 裡頭的值換掉，並測試看看自己是否能在腦海中想像其對應的線性轉換過程。 由淺入深：解決看似不可能的分類任務 如果有反覆觀看前面動畫並停下來動腦思考，我相信你現在已經能夠直觀地理解並想像一個 1-Layer NN 是怎麼解決二元分類問題的。這是一個很不錯的開始。在這小節，我想提升分類任務的難度，讓你再多點對神經網路的直觀感受。 比方說我們可以將剛剛 1-Layer NN 完美分類的兩條曲線拉「近」點： 您的瀏覽器不支援影片標籤，請留言通知我：S 提升分類任務的遊戲難度：讓兩條曲線靠近點 很明顯地，你現在再也無法找到一條平行線來將兩者切開了。相較於我們前面看過的簡單雙曲線，現在這兩條曲線靠得更近，也讓神經網路更難將它們切開了。讓我把這個新的資料集稱作困難雙曲線。 因為我們前面看過的 1-Layer NN 只透過 FC 做一組簡單的線性轉換與位移，我並不認為它能在困難雙曲線上得到多好的結果。不過眼見為憑，還是讓我們看一下 1-Layer NN 在困難雙曲線上的表現： 您的瀏覽器不支援影片標籤，請留言通知我：S 刁難 1-Layer NN：讓它處理更難的分類問題 不需特別計算準確率，你一眼就能看出 1-layer NN 在這資料集上表現地實在是力不從心。兩條曲線在被轉換之後仍然有非常多的重疊部分。換句話說，此 NN 做的（線性）轉換並無法將兩個不同類別的數據點 $x$ 完全 分開。 以線性代數的角度來看，這個 NN 就是透過矩陣 $W$ 對 整個 二維空間做 一致 的旋轉與伸縮，無法針對性地將兩條緊靠的雙曲線在空間中拉扯開來。不過別擔心，深度學習領域裡的人最愛玩疊疊樂了。讓我們多疊一層全連接層（FC），看看新的 2-Layers NN 如何處理這個問題： 您的瀏覽器不支援影片標籤，請留言通知我：S 2-Layers NN 解決困難雙曲線分類問題的過程 這個 2-Layers NN 的表現如你預期嗎？還是讓你失望了？在找戰犯之前，讓我先說明基本概念。 雖然這是本文第一次展示 2-Layers NN，動畫應該已經把所有東西都交代得很清楚了。一個 FC 就對應到一組矩陣 $W$ 以及偏差 $b$，因此一個有兩層 FCs 的神經網路 $g(x)$ 實際上就是對 $x$ 做兩次矩陣運算（與加上偏差）： \\begin{align} g(x) & = W_{2}(W_{1} x + b_{1}) + b_{2} \\end{align} 這邊 $W_{i}$ 與 $b_{i}$ 分別代表第 $i$ 個 FC 的權重矩陣與偏差。 \\begin{align} \\text{1-Layer NN} = f(x) & = W x + b \\end{align} 2-Layers NN 裡頭的矩陣 $W_{1}$ 跟 1-Layer NN 裡頭的矩陣 $W$ 都代表著 第一個 FC 做的線性轉換。但跟 1-Layer NN 不同的地方在於，$W_{1}$ 並不是直接把原始數據點 $x$ 轉換成一維的 $y$，而是先將 $x$ 轉到另個形式的二維表徵 $h$，接著再讓下一層 FC 將 $h$ 轉換至一維的 $y$。像是這種只存在輸入與輸出之間的表徵一般被稱作隱藏表徵（hidden representation），而生成這些表徵的 FC 層自然就被稱作隱藏層（hidden layer）。 而建構這種數據轉換架構的背後精神就是 表徵學習（Representation Learning） ： 透過對輸入 $x$ 做一連串簡單的幾何轉換，將此原始表徵 $x$ 逐漸轉換成能夠用來解決我們問題的隱藏表徵 $h$，最後輸出成目標結果 $y$。 雖然這邊的 2-Layers NN 表現不佳，但這概念可是你在剛接觸深度學習時值得畫上三顆星的重點之一。 回到剛剛的 2 × 2 的 $W_{1}$ 矩陣。跟之前 1 × 2 矩陣一樣，這次讓我們看看 $W_{1}$ 跟 FC 之間的關係： 您的瀏覽器不支援影片標籤，請留言通知我：S 2 × 2 矩陣 $W_{1}$ 的運算與全連接層之間的關係 你可能注意到矩陣 $W_{1}$ 裡頭每一行（column）顏色皆不相同。 矩陣 $W_{1}$ 定義了一個線性轉換，而裡頭的每個 column 則定義了原空間 $V_{original}$ 中的各個基底向量在新空間 $V_{transformed}$ 中的位置。而以神經網路的角度來看，矩陣 $W_{1}$ 中的每一行則對應到某個神經元連到下一層所有神經元的權重值。 另外值得一提的是，FC 跟矩陣的緊密關係事實上也部分解釋了為何神經網路常以一層層（Layered）形式出現的原因。雖然這邊的 NN 只有兩層，在深度學習領域裡我們時常會想對原始數據 $x$ 進行幾十、幾百次的轉換，而由一連串矩陣組成的運算非常容易平行化，因此在建立深度神經網路（Deep Neural Network）時，疊很多以矩陣運算為基礎的 Layers 是一個非常主流的做法。 您的瀏覽器不支援影片標籤，請留言通知我：S 線性整流函數 ReLU 可以讓神經網路對數據做更複雜的轉換 至於為何剛剛的 2-Layers NN 無法解決困難雙曲線？ 在了解矩陣運算以及線性轉換以後，一切都變得很容易解釋。結合前面所學： 一層 FC 對應到一矩陣運算（加偏差） 一個權重矩陣對應到一個線性轉換 一個線性轉換會對作用空間做旋轉伸縮 就可以得知我們的 2-Layers NN $g(x)$ 事實上就是對原空間裡頭的數據點 $x$ 做連續兩次的旋轉與伸縮（當然，動畫已經告訴我們這件事情了）。很直觀地，不管神經網路做幾次這樣的轉換，最後的效果都可以被 單一 線性轉換取代。換句話說，對困難雙曲線連續做好幾次線性轉換是不會得到比單一線性轉換更好的結果的。你可以拉回去看看 2-Layers NN 如何「瞎忙」。 解法也很直覺。我們可以找一個非線性函數如 ReLU 當作 FC 層的激勵函數（activation function），讓 FC 層跟其他層之間有 非線性 的轉換，讓神經網路掌握超越線性轉換的能力。 以下就是將 ReLU 加入 2-Layers NN 並解決困難雙曲線分類的過程： 您的瀏覽器不支援影片標籤，請留言通知我：S 這個轉換過程超美的，不是嗎？這個神經網路只有 9 個參數，卻是我看過最美麗的神經網路之一。 透過線性轉換與非線性轉換的交替使用，升級後的 2-Layers NN $h(x)$ 成功地將困難雙曲線任務解開，辦到前一代 $g(x)$ 做不到的事情，而兩者只差在一個 ReLU 函數： \\begin{align} g(x) & = W_{2}(W_{1} x + b_{1}) + b_{2} \\\\ h(x) & = W_{2} relu(W_{1} x + b_{1}) + b_{2} \\end{align} 再次提醒，激勵函數 $a(z) = relu(z)$ 一般是被 各別 套用到在計算完 $W(x) + b = z$ 的神經元之上，不需做矩陣運算，因此幾乎沒有計算成本（好啦，還是要看是哪個函數）。在深度學習領域裡，使用非線性函數來提升神經網路的處理能力是件稀鬆平常的事。除了常用的 ReLU 之外，其他知名的函數包含了 Sigmoid 、 Tanh 以及 Leaky ReLU 等等。 到此為止，你也已經了解非線性函數之於神經網路的重要性了。我們的旅程也將進入尾聲。 結語：往下一站出發 呼！不知不覺就到了這趟旅程的終點了！結束地真快，你說是吧？ 就算你一開始什麼都不懂，在閱讀本文以後你應該已經能夠： 了解最基本的矩陣運算、神經網路以及兩者之間的緊密關係 想像線性轉換如何對向量空間做旋轉、伸縮等基本轉換 理解並想像一個 1-Layer NN 怎麼解決二元分類問題 了解如何透過矩陣運算的形式定義一個神經網路架構 𝑓𝜃(𝑥) 使用神經網路或是線性代數的角度來解讀 𝑓𝜃(𝑥) 的作用 明白神經網路就是將原始數據 𝑥 進行一系列幾何轉換後輸出理想的 𝑦 的映射函數 瞭解如何透過非線性的轉換，讓神經網路掌握更進階的能力 自由地連結矩陣運算以及神經網路的概念並在兩者之間切換 沒錯，你可能會驚訝於自己學了那麼多。更美妙的是透過大量動畫，很多概念應該都已經深深地烙印在你的腦海中，你甚至不需要背什麼東西。在這個 AI Hype 時代，我們沒有談最新的 AI 論文，也沒有用深度學習框架做什麼酷炫的應用，但我相信這樣的文章才是大部分人以及生活在 AI 時代的下一代所需要的。 您的瀏覽器不支援影片標籤，請留言通知我：S 2 × 3 矩陣與全連接層的對應關係 如果你想要學習更多線性代數以及微積分的相關知識，強力推薦 3Blue1Brown 的 Youtube 頻道 ，本文所有動畫也都是用該頻道作者 Grant Sanderson 開源的 Manim 製作的。另外如果你想要深入了解深度學習， 由淺入深的深度學習資源整理 是一個好的開始。 如果你已經有點 Python 基礎，想要看深度學習還能拿來做些什麼，可以參考之前的文章： 寫給所有人的自然語言處理與深度學習入門指南 AI 如何找出你的喵：直觀理解卷積神經網路 。 進擊的 BERT：NLP 界的巨人之力與遷移學習 直觀理解 GPT-2 語言模型並生成金庸武俠小說 淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中 用 CartoonGAN 及 TensorFlow 2 生成新海誠與宮崎駿動畫 底下是我為你做的最後一個動畫。你可以用它來測試自己在本文的所學，並熟悉矩陣與神經網路之間的關係。注意矩陣行與列中的參數 $w_{mn}$ 是怎麼對應到神經網路的邊（edge）的。 您的瀏覽器不支援影片標籤，請留言通知我：S 更多矩陣與神經網路的對應關係 在這個 AI 新聞滿天飛的時代，我希望這篇 入門 文章可以幫助更多人直觀地理解其背後的核心動力：神經網路並開始自己的 AI 之旅。如果這篇文章有幫助到你，希望你能發揮舉手之勞，幫忙 分享本文 ，以讓更多、更多的人可以加入這個行列。 也希望你喜歡這首神經網路與線性代數的雙重奏，我們下次見。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html","loc":"https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html"},{"title":"直觀理解 GPT-2 語言模型並生成金庸武俠小說","text":"這是一篇以「介紹 GPT-2」之名，行「推坑金庸」之實的自然語言處理文章。 嗨，會點擊進來，我想你應該至少看過武俠小說泰斗 金庸 的一部著作吧！ 這篇文章將簡單介紹 OpenAI 在今年提出的知名 語言模型 GPT-2 ，並展示一個能夠用來生成金庸風格文本的小型 GPT-2。在讀完本文之後，你也能使用我的 Colab 筆記本 來生成屬於你自己的金庸小說。文中也將透過視覺化工具 BertViz 讓你能夠直觀地感受 GPT-2 等 基於 Transformer 架構的 NLP 模型 如何利用 注意力機制（Attention Mechanism） 來生成文本。 您的瀏覽器不支援影片標籤，請留言通知我：S 本文的 Colab 筆記本讓你可以自己生成金庸橋段並可視化結果 如果你想要直觀地了解 自然語言處理（ N atural L anguage P rocessing, NLP） 以及 深度學習 可以如何被用來生成金庸小說，這篇應該很適合你。 前置知識 如果你已讀過我寫的幾篇 NLP 文章，我相信你可以非常輕鬆地理解本文提及的 GPT-2 概念： 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南 讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部 淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中 進擊的 BERT：NLP 界的巨人之力與遷移學習 基本上排越後面越進階。別擔心，我還是會用最平易近人的方式介紹 GPT-2！但你之後可能會想要回來參考這些文章。不管如何，現在先讓我們開始這趟 GPT-2 之旅吧！ 先睹為快：看看 GPT-2 生成的金庸橋段 首先，你可以把本文想成是 如何用 TensorFlow 2 及 TensorFlow.js 寫天龍八部 的升級版本（是的，我太愛金庸所以要寫第兩篇文章）。本文跟該篇文章的最大差異在於： 模型升級 ：當初我們使用輕量的 長短期記憶 LSTM 作為語言模型，這篇則使用更新、更強大的 GPT-2 數據增加 ：當初我們只讓 LSTM「閱讀」一本 《天龍八部》 ，這篇則用了整整 14 部金庸武俠小說來訓練 GPT-2 強調概念 ：當初我們用 TensorFlow 一步步實作 LSTM，這篇則會專注在 GPT-2 的運作原理並額外提供 Colab 筆記本 供你之後自己生成文本以及視覺化結果 用來訓練 GPT-2 的金庸武俠小說：第一排由左到右：飛雪連天射白鹿；第二排：笑書神俠倚碧鴛 飛雪連天射白鹿，笑書神俠倚碧鴛。 ─ 金庸作品首字詩 從《飛狐外傳》、《倚天屠龍記》、《笑傲江湖》到《鴛鴦刀》，這 14 部經典的金庸著作你讀過幾本呢？哪一本是你的最愛呢？還記得多少橋段呢？ 在實際介紹 GPT-2 之前，讓我們先看看將這些作品讀過上百遍的 GPT-2 會生成出怎麼樣的橋段。你可以從底下的這些生成例子感受一下 GPT-2 的語言能力以及腦補技巧： <!-- https://www.w3schools.com/w3css/w3css_slideshow.asp --> .w3-content, .w3-auto { margin-left: auto; margin-right: auto } .w3-content { max-width: 980px } .w3-display-container:hover .w3-display-hover { display: block } .w3-display-container:hover span.w3-display-hover { display: inline-block } .w3-display-container { position: relative } .w3-button:hover { color: #000!important; background-color: inherit; } .w3-button { border: none; display: inline-block; padding: 3px 3px; vertical-align: middle; overflow: hidden; text-decoration: none; color: inherit; background-color: white; text-align: center; cursor: pointer; white-space: nowrap } .w3-button { -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; height: 2.4rem; } .w3-button:disabled { cursor: not-allowed; opacity: 0.3 } .w3-display-left { position: absolute; top: 50%; left: 0%; transform: translate(0%, -50%); -ms-transform: translate(-0%, -50%) } .w3-display-right { position: absolute; top: 50%; right: 0%; transform: translate(0%, -50%); -ms-transform: translate(0%, -50%) } .mySlides {display:none;} ❮ ❯ 點擊左右箭頭可查看 GPT-2 生成不同金庸著作的橋段 var slideIndex = 1; showDivs(slideIndex); function plusDivs(n) { showDivs(slideIndex += n); } function showDivs(n) { var i; var x = document.getElementsByClassName(\"mySlides\"); if (n > x.length) {slideIndex = 1} if (n < 1) {slideIndex = x.length} for (i = 0; i < x.length; i++) { x[i].style.display = \"none\"; } x[slideIndex-1].style.display = \"block\"; } 沒錯，很ㄎㄧㄤ！但這些文本不是我自己吸麻瞎掰出來的。（事實上，GPT-2 比我厲害多了）在用 14 部金庸武俠小說訓練完 GPT-2 之後，我從這些小說中隨意抽取一段文字作為 前文脈絡 ，接著就讓它自己腦補後續橋段。你可以從左上角得知模型是在生成哪部武俠小說。 這些文本當然不完美，但跟 我們當初用 LSTM 生成《天龍八部》 的結果相比，已有不少進步： 生成的文本更加通順、語法也顯得更為自然 記憶能力好，能夠持續生成跟前文相關的文章而不亂跳人物 值得一提的是，如果你讀過《天龍八部》，就會知道第一個例子的前文脈絡（context）是 段譽 與 王語嫣 墜入井內最終兩情相悅的橋段。儘管每次生成的結果會因為隨機抽樣而有所不同，GPT-2 在看了這段前文後為我們生成了一本超級放閃的言情小說，儘管放閃的兩人貌似跟我們預期地不太一樣。且在該平行時空底下，貌似 慕容復 也到了井裡（笑 你可以跟當初 LSTM 的生成結果比較，感受 GPT-2 進步了多少： ❮ ❯ 點擊左右箭頭切換 LSTM 與 GPT-2 的生成橋段 var slideIndex = 1; showDivs1(slideIndex); function plusDivs1(n) { showDivs1(slideIndex += n); } function showDivs1(n) { var i; var x = document.getElementsByClassName(\"mySlides1\"); if (n > x.length) {slideIndex = 1} if (n < 1) {slideIndex = x.length} for (i = 0; i < x.length; i++) { x[i].style.display = \"none\"; } x[slideIndex-1].style.display = \"block\"; } 雖然很有金庸架勢，細看會發現 LSTM 的用詞不太自然（比如說 四具屍體匆匆忙逼 、 伸掌在膝頭褲子 ）。且明明前文脈絡提到的是 段譽 與 王語嫣 ，LSTM 開頭馬上出現不相干的 南海鱷神 、接著跳到 虛竹 、然後又扯到 蕭峰 ... 無庸置疑， LSTM 是你在 NLP 路上必學的重要神經網路。不過很明顯地，跟當初用詞不順且不斷跳 tune 的 LSTM 相比，新的 GPT-2 的生成結果穩定且流暢許多（當然，訓練文本及參數量的差異不小）。在看過生成結果以後，讓我們看看 GPT-2 實際上是個怎麼樣的語言模型。 GPT-2：基於 Transformer 的巨大語言模型 GPT-2 的前身是 GPT ，其全名為 G enerative P re- T raining 。在 GPT-2 的論文 裡頭，作者們首先從網路上爬了將近 40 GB，名為 WebText（開源版） 的文本數據，並用此龐大文本訓練了數個以 Transformer 架構為基底的 語言模型（language model） ，讓這些模型在讀進一段文本之後，能夠預測下一個字（word）。 給定前 t 個在字典裡的詞彙，語言模型要去估計第 t + 1 個詞彙的機率分佈 P 如今無人不知、無人不曉的神經網路架構 Transformer 在 2017 年由至今已超過 3,000 次引用的論文 Attention Is All You Need 提出，是一個不使用循環神經網路、 卷積神經網路 並完全仰賴 注意力機制 的 Encoder-Decoder 模型 。在 前置知識一節 提過的 神經機器翻譯 & Transformer 一文裡已經用了大量動畫帶你理解並實作自注意力機制及 Transformer，這邊就不再贅述了。 基本上只要了解 Transformer 架構，你馬上就懂 GPT-2 了。因為該語言模型的本體事實上就是 Transformer 裡的 Decoder ： GPT-2 與兩知名模型 ELMo 與 BERT 使用的參數量比較 （ 圖片來源 ） 更精準地說，GPT-2 使用的 Transformer Decoder 是 原始 Transformer 論文 的 小變形 （比方說沒有了關注 Encoder 的 Decoder-Encoder Attention Layer），但 序列生成（Sequence Generation） 的概念是完全相同的。 架構本身沒什麼特別。但 GPT-2 之所以出名，是因為它訓練模型時所使用的數據以及參數量都是前所未有地 龐大 ： 訓練數據 ：使用從 800 萬個網頁爬來的 40 GB 高品質文本。把金庸 14 部著作全部串起來也不過 50 MB。WebText 的數據量是金庸著作的 800 倍。想像一下光是要看完這 14 部著作 一遍 所需花費的時間就好。 模型參數 ：15 億參數，是已經相當巨大、擁有 3.4 億參數的 BERT-Large 語言代表模型的 4.5 倍之多。BERT-Large 使用了 24 層 Transformer blocks，GPT-2 則使用了 48 層。 這可是有史以來最多參數的語言模型。而 GPT-2 獨角獸（unicorn）的形象則是因為當初作者在 論文 裡生成文本時，給了 GPT-2 一段關於「住在安地斯山脈，且會說英文的一群獨角獸」作為前文脈絡，而 GPT-2 接著生成的結果有模有樣、頭頭是道，讓許多人都驚呆了： GPT-2 作者用跟本文生成金庸橋段一樣的方式讓模型生成獨角獸文章 （ 圖片來源 ） 你可以前往在 由淺入深的深度學習資源整理 就已經介紹過的 Talk to Transformer 生成上例的獨角獸文章、復仇者聯盟劇本或是任何其他類型的 英文 文章。 我懂，對非英文母語的我們來說，其實很難深切地感受 GPT-2 生成的文章到底有多厲害。這也是為何我決定要使用金庸小說來訓練一個中文模型並介紹 GPT-2，因為這樣你比較能夠實際感受並了解模型生成的文本。 官方釋出的 GPT-2 能夠輸出中文字 ，但因為大部分文本都是透過 Reddit 爬下來的英文文章，因此是沒有辦法做到如同本文的中文生成的。 讓 GPT-2 在社群上被熱烈討論的另個原因是作者等人 當初在部落格上展示驚人的生成結果後表示 ： 因為顧慮到這項技術可能會遭到惡意運用，我們目前並不打算釋出已訓練好的模型。但為了促進研究我們將釋出規模小很多的模型供研究者參考。 ─ OpenAI, 2019/02 此言一出，一片譁然。看看隔壁棚幾乎可以說是以開源為志向的 Google BERT ！網路上有人甚至嘲諷 OpenAI 一點都不 open，而是 CloseAI；也有人說 OpenAI 只是為了炒話題，GPT-2 並沒有那麼厲害；當然也有人認為作者們的論文已經有足夠的學術貢獻，並非一定得釋出模型。 不過至少目前看來 OpenAI 只是採取相對謹慎的態度在釋出模型。該團隊在今年 2 月將最小的 124M GPT-2 Small（1.2 億參數）與 論文 一起釋出，並在 5 月釋出 355M 的 GPT-2 Medium。而就在 不久前的 8 月釋出了有 7.74 億參數的 GPT-2 Large ，其模型大小是 1558M GPT-2 完全體的一半。 作者們實驗的 4 種不同大小的 GPT-2 模型，774M 版本在上個月被釋出 （ 圖片來源 ） 一群人歡欣鼓舞，迫不及待地把玩最新玩具 GPT-2 Large。剛剛提到的 Talk to Transformer 事實上就已經是在使用最新的 GPT-2 Large 了，手腳很快。 其他相關應用多如牛毛。比方說之前介紹過的 This Waifu Does Not Exist 在使用 GAN 生成動漫頭像的同時也利用 GPT-2 隨機生成一段動漫劇情；而 TabNine 則是一個加拿大團隊利用 GPT-2 做智慧 auto-complete 的開發工具，志在讓工程師們減少不必要的打字，甚至推薦更好的寫法： 您的瀏覽器不支援影片標籤，請留言通知我：S TabNine 透過 GPT-2 讓工程師更有效率地開發程式（以 Java 為例） 由強大的深度學習模型驅動，可以想像未來（現在已經是了！）會有更多如 TabNine 的應用影響我們的工作與生活。而這也是為何你最好花點時間 follow 深度學習以及 AI 發展趨勢。當然，你也可以選擇在文末訂閱此部落格，只是我不敢保證文章的更新速度（笑 GPT-2 公布時在多個 language modeling 任務取得 SOTA 結果，因此所有人都在引頸期盼著 OpenAI 將最大、擁有 15 億參數的 GPT-2 模型釋出。而該團隊也表示 他們會先觀察人們怎麼使用 774M GPT-2，並持續考慮開源的可能性 。 在有了 BERT 之後，不少研究者開始垂涎著後來發表的 GPT-2 不過別走開！GPT-2 的故事還沒有結束。可別以為 OpenAI 會僅僅滿足於能夠生成獨角獸文章的一個語言模型。 重頭戲現在才要開始。 論文作者：GPT-2 能做的可不只是生成文本 要了解 GPT-2，先看其前身 GPT。 我們前面就已經提過， GPT 的全名是 G enerative P re- T raining。 G enerative（生成）指的是上節看到的 language modeling，將爬來的文本餵給 GPT，並要求它預測（生成）下一個字； P re- T raining 則是最近 NLP 界非常流行的 兩階段 遷移學習的第一階段： 無監督式學習（Unsupervised Learning） 。相關概念我們在 進擊的 BERT：NLP 界的巨人之力與遷移學習 就已經非常詳細地探討過了，但為了幫助你理解，讓我很快地再次簡單說明。 近年 NLP 界十分流行的兩階段遷移學習會先蒐集大量文本（無需任何標注數據），並以無監督的方式訓練一個 通用 NLP 模型，接著再微調（Fine-tune）該模型以符合 特定 任務的需求。常見的 NLP 任務有文章分類、自然語言推論、問答以及閱讀理解等等。 Google 的語言代表模型 BERT 則是 Transformer 中的 Encoder （ 圖片來源 ） 值得一提的是， OpenAI 提出的 GPT 跟 Google 的語言代表模型 BERT 都信奉著兩階段遷移學習：利用大量文本訓練出一個 通用 、具有高度自然語言理解能力的 NLP 模型。有了一個這樣的通用模型之後，之後就能透過簡單地微調 同一個 模型來解決各式各樣的 NLP 任務，而無需每次都為不同任務設計特定的神經網路架構，省時省力有效率。 兩者的差別則在於進行無監督式訓練時選用的訓練目標以及使用的模型有所不同： GPT 選擇 Transformer 裡的 Decoder ，訓練目標為一般的語言模型，預測下個字 BERT 選擇 Transformer 裡的 Encoder ，訓練目標則為克漏字填空以及下句預測 我們這邊不會細談，但基本上不同模型架構適合的訓練目標就會有所不同。不管如何，兩者都使用了 Transformer 架構的一部份。而這主要是因為 Transformer 裡頭的 自注意力機制（Self-Attention Mechanism） 十分有效且相當適合平行運算。GPT(-2) 的 Transformer Decoder 裡頭疊了很多層 Decoder blocks，以下則是某一層 Decoder block 透過自注意力機制處理一段文字的示意圖： 訓練好的 Transformer Decoder 在處理某詞彙時能關注前方相關的其他詞彙，進而為該詞彙的 representation 融入語境資訊 （ 圖片來源 ） 給定一段文本： <s> a robot must obey the orders given it ... 你可以很輕易地看出 it 指代 前面出現過的 robot 。而這是因為你懂得去 關注 （pay attention to）前文並修正當前詞彙 it 的語意。在給定相同句子時， 傳統詞嵌入（Word Embeddings） 方法是很難做到這件事情的。所幸，透過強大的自注意力機制，我們可以讓模型學會 關注 上文以決定每個詞彙所代表的語意。 以上例而言，訓練好的 GPT 可以在看到 it 時知道該去關注前面的 a 及 robot ，並進而調整 it 在當下代表的意思（即修正該詞彙的 vector representation）。而被融入前文語意的新 representation 就是所謂的 Contextual Word Representation 。 我們可以再看一個 BERT 文章 裡出現過的自注意力例子： BERT 用自注意力機制理解句中的「他」究竟代表什麼意思 （ 圖片來源 ） 關於自注意力機制（Self-Attention），有個值得記住的重要概念：在 GPT 之後問世的 BERT 是同時關注 整個 序列來修正一個特定詞彙的 representation，讓該詞彙的 repr. 同時隱含 上下文 資訊；而 GPT 是一個由左到右的常見語言模型，會額外透過 遮罩技巧（Masking） 來確保模型只會關注到某詞彙 以左 的 上文 資訊。 在原始的 Transformer 架構裡頭就包含了 Encoder 與 Decoder，分別使用左側與右側的自注意力機制。BERT 跟 GPT 其實只是各選一邊來用 （ 圖片來源 ） 再次提醒，如果你想要深入了解如何實際用 TensorFlow 來實作遮罩，並將左側的自注意力機制變成右側的遮罩版本，可以參考 之前的 Transformer 文章 。假設你仍無法理解我在胡扯些什麼的話，只要記得： Transformer 的自注意力機制讓我們可以用更有意義、具備當下語境的方式來表達一段文字裡頭的每個詞彙，進而提升模型的自然語言理解能力。 我在 下一節 還會用些額外的例子讓你能更直觀地理解這個概念。 了解 GPT 後 GPT- 2 就容易了解了，因為 GPT- 2 基本上就是 GPT 第二代：一樣是 Transformer Decoder 架構，但使用的數據及模型大小都直接霸氣地乘上 10 倍 。有了如此龐大的數據與模型，在做完第一階段的無監督式訓練以後，GPT-2 的作者們決定做些瘋狂的事情：不再遵循兩階段遷移學習，直接做 zero-shot learning！這也就意味著直接把只看過 WebText 的 GPT-2 帶上考場，「裸測」它在多個跟 WebText 無關的 NLP 任務上的表現。而實驗結果如下： 由左至右分別為閱讀理解、翻譯、摘要以及問答任務 乍看之下你可能會覺得這張圖沒什麼。畢竟就算是最大、最右側的 GPT-2 模型（1542M）在多數特定的 NLP 任務上還是比不過專門為那些任務設計的神經網路架構（比方說閱讀理解的 DrQA + PGNet）。但 GPT-2 的作者們認為他們最大的貢獻在於展現了用大量無標註數據訓練巨大語言模型的潛力：數大就是美！除了摘要（Summarization）任務之外，基本上只要模型參數越大，zero-shot 的結果就越好。 且因為在模型參數越來越大時，訓練 / 測試集的結果仍然都持續進步且表現水準相仿，作者們認為就算是最大的 GPT-2 模型也還 underfit 他們爬的 WebText 數據集，還有進步空間。 不同大小的 GPT-2 在 WebText 上表現 （ 圖片來源 ） 令某些研究者興奮的是，這實驗結果隱含的一個訊息是「或許只要用夠多的文本訓練夠大的語言模型，就能讓該模型在沒有監督的情況下完成更多 NLP 任務」。 總而言之，GPT-2 整篇論文的核心思想可以被這樣總結： 給定越多參數以及越多樣、越大量的文本，無監督訓練一個語言模型或許就可讓該模型具備更強的自然語言理解能力，並在沒有任何監督的情況下開始學會解決不同類型的 NLP 任務。 這個概念用一個簡單但合適的比喻就是「觸類旁通」：GPT-2 在被要求預測 WebText 裡頭各式各樣文章的下一個字時，逐漸掌握理解自然語言的能力，最後就算不經過特別的訓練也能做些簡單的問答、翻譯以及閱讀理解任務。現在回頭看看，你從論文標題： Language Models are Unsupervised Multitask Learners 就能理解當初 GPT-2 作者們想要傳達的想法了。他們也希望這些實驗結果能吸引更多研究者往這個方向發展。當然，不是每個人都有能力與資源做這種龐大模型的研究，且我個人事實上比較喜歡如 DistilBERT 等輕量級模型的研究與應用，之後有時間再撰文分享。 好啦！基本上你現在應該已經掌握 GPT-2 的核心概念與思想了。如果你欲罷不能、想要了解更多，我在 最後一節 會附上更多相關連結供你參考。在下一節，讓我們再回去看看金庸 GPT-2。 用 BertViz 觀察 GPT-2 生成文本 取決於超參數設定，GPT-2 是一個有上千萬、甚至上億參數的語言模型，因此有時你很難理解它裡頭究竟在做些什麼。這節我想花點時間說明如何透過 BertViz 工具來視覺化（visualize）本文的金庸 GPT-2 裡頭的自注意力機制，以加深你對 GPT-2 的理解。 讓我們先比較一下金庸 GPT-2 跟論文裡 4 個模型的差距： 本文的金庸 GPT-2 與其他 GPT-2 論文模型的規模比較 （ 圖片來源 ） 是的，金庸 GPT-2 不管是在訓練數據還是模型尺寸都比論文裡最小的 GPT-2 Small 還來得小，因此你不該期待它表現地跟任何使用大型 GPT-2 模型的線上 demo 一樣好。但因為它是讀 中文 文章，非常適合我們理解，是作為教學用途的好夥伴。另外注意金庸 GPT-2 使用了 10 層 Decoder blocks，而我們可以用 BertViz 輕易地視覺化每一層的自注意力機制。 比方說給定一個在金庸原著裡不存在，但我超級想要實現的《天龍八部》劇情： 喬峯帶阿朱回到北方，喬峯對她說：「我們兩人永遠留在這裡！」 透過我為你準備好的 Colab 筆記本及預先訓練好的金庸 GPT-2 ，你只需幾行 Python 程式碼就能視覺化每一層 Decoder block 處理這段文本的結果： from bertviz.pytorch_transformers_attn import GPT2Model gpt2_model = GPT2Model . from_pretrained ( '.' ) text = '喬峯帶阿朱回到北方，喬峯對她說：「我們兩人永遠留在這裡！」' view = 'model' show ( gpt2_model , tokenizer , text , view ) 您的瀏覽器不支援影片標籤，請留言通知我：S BertViz 的 model view 讓你輕鬆「鳥瞰」整個模型。這裡只顯示第 6 - 9 層 blocks（zero-index） 我們在這邊不會細談，但你會發現 上下 每一層 Decoder block 左右 各有 12 個 heads。這就是 之前介紹過的 Multi-head Attention 機制 ，目的是讓模型能夠給予每個詞彙多個不同的 representations 並在不同 representation spaces 裡關注不同位置，增加表達能力。 這些圖的解讀方式是當 GPT-2 看 左側 特定詞彙時，關注 右側 同序列中出現在該詞彙 之前 （包含自己）的其他詞彙。關注的程度則透過線條 粗 細來表示。 而如果我們將 GPT-2 生成這段文本的自注意力機制的變化依照詞彙的生成順序顯示出來的話，會看起來像這樣： # BertViz 的 neuron view 可以看到 key, value 的匹配 view = 'neuron' show ( gpt2_model , tokenizer , text , view ) 您的瀏覽器不支援影片標籤，請留言通知我：S GPT-2 在生成新詞彙時會持續透過自注意力機制關注前文 這邊的重點是前面講過的 Masked Self-Attention：一個傳統、 單向 的語言模型在處理新詞彙時只會、也只能 關注 前面已經被生成的其他詞彙，而不該去看「未來」的詞彙。 不知道你有沒有注意到，在上面的例子中，GPT-2 在處理詞彙「她」時會去關注前面靠近人名的「阿」，這是一件很了不起的事情： GPT-2 透過自注意力機制建立具有語境的 word representations 如同我們在 前面章節 提過的，中文字「她」與「這」本身並沒有太多含義，只有在了解情境之後才能判別它們所代表的意義。而這是具有自注意力機制的 NLP 模型可以想辦法學會的。 我們也可以看看不同層的 Decoder blocks 在關注同樣的文本時有什麼變化： # 實際上只會產生一個可以選擇不同層的 UI，我隨意選了 3 層的截圖結果 text = '喬峯帶阿朱回到北方，喬峯對她說：「我們兩人永遠留在這裡！」' view = 'head' show ( gpt2_model , tokenizer , text , view ) 不同層的 Decoder blocks 關注相同文本的結果 你可以明顯地觀察到底層（第一層）的 Decoder block 在處理詞彙「她」時將注意力放到多個詞彙上以擷取整個前文資訊。而越上層的 block 的注意力越顯集中，到了最後一層的 Decoder block 時就相當專注在較遠的人名附近。 透過這些視覺化結果，你現在應該對 GPT-2 的運作模式有著更直觀的理解了。 故事尾聲 呼！我希望你享受這趟跟我一起探索 GPT-2 以及金庸著作的旅程。 就我所知，這應該是網路上第一篇以中文 GPT-2 為背景，用最白話的方式講解相關概念的中文文章了。我在撰寫本文時嘗試用初學者最容易理解的章節編排方式、省略不必要的艱澀詞彙並避免丟上一長串程式碼。如果你讀完覺得本文內容很簡單，GPT-2 也不過就這樣，或者迫不及待想要知道更多細節，那就達成我撰寫本文的目標了。 跟本篇相關的 NLP 文章 本文為了顧及更多剛剛入門 NLP 的讀者省略了不少技術細節，如長距離依賴（Long-range Dependencies）的探討、自注意力機制的實作細節及研究 GPT-2 是真的學習還是只是記憶等課題。想要深入研究相關概念的讀者， 下節的延伸閱讀 可供你做參考。而如果你想要打好 NLP 基礎以及本文提到的相關知識，我會推薦你回到前面的 前置知識 一節，選擇最合你胃口的 NLP 文章開始閱讀。 正如 統計學家喬治·E·P·博克斯 所說的： 所有模型都是錯的；但有些是有用的。 ─ George E. P. Box 等到哪天有更好的語言模型可以拿來生成金庸武俠小說，你就會再次看到我撰寫相關文章了。 但今天就到這裡啦！我現在還得煩惱該從哪部金庸小說開始複習呢 ... 延伸閱讀：課外參考資源 我在文中講述 GPT-2 時已經附上相當多連結，如果你很好學，事實上應該已經花了不少時間才讀到這裡。這節系統性地列出相關連結供你做延伸閱讀。 教學文章、課程影片 李宏毅教授 2019 機器學習課程的 ELMO, BERT, GPT 影片 The Illustrated GPT-2 (Visualizing Transformer Language Models) Better Language Models and Their Implications CS224n: Natural Language Processing with Deep Learning 論文 Generating Wikipedia by Summarizing Long Sequences Improving language understanding by generative pre-training Language Models are Unsupervised Multitask Learners Visualizing Attention in Transformer-Based Language Representation Models 實作 pytorch-transformers GPT2-Chinese if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html","loc":"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html"},{"title":"資料科學家的 pandas 實戰手冊：掌握 40 個實用數據技巧","text":"故學然後知不足，教然後知困。知不足，然後能自反也；知困，然後能自強也，故曰：教學相長也。 ─ 《禮記．學記》 pandas 是 Python 的一個資料分析函式庫，提供如 DataFrame 等十分容易操作的資料結構，是近年做數據分析時不可或需的工具之一。 雖然已經有滿坑滿谷的教學文章、影片或是線上課程，正是因為 pandas 學習資源之多，導致初學者常常不知如何踏出第一步。在這篇文章裡頭，我以自身作為資料科學家（ D ata S cientist, DS）的工作經驗，將接近 40 個實用的 pandas 技巧由淺入深地分成 6 大類別： 建立 DataFrame 客製化 DataFrame 顯示設定 數據清理 & 整理 取得想要關注的數據 基本數據處理與轉換 簡單匯總 & 分析數據 透過有系統地呈現這些 pandas 技巧，我希望能讓更多想要利用 Python 做資料分析或是想成為 DS 的你，能用最有效率的方式掌握核心 pandas 能力；同時也希望你能將自己認為實用但本文沒有提到的技巧與我分享，達到開頭引文所說的 教學相長 ：） 如果你是使用寬螢幕瀏覽本文，隨時可以點擊左側傳送門瀏覽各章內容： 您的瀏覽器不支援影片標籤，請留言通知我：S 除了 pandas 的操作技巧以外，我在最後一節： 與 pandas 相得益彰的實用工具 裡也介紹了幾個實用函式庫。就算你是 pandas 老手，或許也能從中得到些收穫。當然也非常歡迎與我分享其他厲害工具，我會更新到文章裡頭讓更多人知道。 前言已盡，讓我們開始這趟 pandas 旅程吧！當然，首先你得 import pandas ： import pandas as pd pd . __version__ '0.24.2' 建立 DataFrame pandas 裡有非常多種可以初始化一個 DataFrame 的技巧。以下列出一些我覺得實用的初始化方式。 用 Python dict 建立 DataFrame 使用 Python 的 dict 來初始化 DataFrame 十分直覺。基本上 dict 裡頭的每一個鍵值（key）都對應到一個欄位名稱，而其值（value）則是一個 iterable，代表該欄位裡頭所有的數值。 dic = { \"col 1\" : [ 1 , 2 , 3 ], \"col 2\" : [ 10 , 20 , 30 ], \"col 3\" : list ( 'xyz' ), \"col 4\" : [ 'a' , 'b' , 'c' ], \"col 5\" : pd . Series ( range ( 3 )) } df = pd . DataFrame ( dic ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col 1 col 2 col 3 col 4 col 5 0 1 10 x a 0 1 2 20 y b 1 2 3 30 z c 2 在需要管理多個 DataFrames 時你會想要用更有意義的名字來代表它們，但在資料科學領域裡只要看到 df ，每個人都會預期它是一個 D ata F rame，不論是 Python 或是 R 語言的使用者。 很多時候你也會需要改變 DataFrame 裡的欄位名稱： rename_dic = { \"col 1\" : \"x\" , \"col 2\" : \"10x\" } df . rename ( rename_dic , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x 10x col 3 col 4 col 5 0 1 10 x a 0 1 2 20 y b 1 2 3 30 z c 2 這邊也很直覺，就是給一個將舊欄位名對應到新欄位名的 Python dict 。值得注意的是參數 axis=1 ：在 pandas 裡大部分函式預設處理的軸為列（row）：以 axis=0 表示；而將 axis 設置為 1 則代表你想以行（column）為單位套用該函式。 你也可以用 df.columns 的方式改欄位名稱： df . columns = [ 'x(new)' , '10x(new)' ] + list ( df . columns [ 2 :]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x(new) 10x(new) col 3 col 4 col 5 0 1 10 x a 0 1 2 20 y b 1 2 3 30 z c 2 使用 pd.util.testing 隨機建立 DataFrame 當你想要隨意初始化一個 DataFrame 並測試 pandas 功能時， pd.util.testing 就顯得十分好用： pd . util . testing . makeDataFrame () . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D xPlIXbQqGU 1.086581 -0.002484 -0.335693 0.226988 IAFe6K8mpA -0.547556 -0.290935 -0.014313 -0.301007 OoATGY0k2M 1.017636 0.568835 -0.272382 0.659657 uRN2yGacDw -0.662390 1.929820 -1.206670 0.250626 ElphZli9nK -0.697235 0.942415 -0.894887 0.701790 oiEoCPCXK8 -1.049284 -1.019107 -0.640271 -0.613056 NUrQFrYQw1 0.759355 0.717367 -0.449368 1.889321 oC9iiEBneW 0.665412 -0.391204 -0.974010 0.248326 4hD6Eea7yF -0.862819 2.092149 0.976645 -0.388735 3QD5mMfstw -0.312762 -0.110278 1.162421 -0.335144 head 函式預設用來顯示 DataFrame 中前 5 筆數據。要顯示後面數據則可以使用 tail 函式。 你也可以用 makeMixedDataFrame 建立一個有各種資料型態的 DataFrame 方便測試： pd . util . testing . makeMixedDataFrame () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 0.0 0.0 foo1 2009-01-01 1 1.0 1.0 foo2 2009-01-02 2 2.0 0.0 foo3 2009-01-05 3 3.0 1.0 foo4 2009-01-06 4 4.0 0.0 foo5 2009-01-07 其他函式如 makeMissingDataframe 及 makeTimeDataFrame 在後面的章節都還會看到。 將剪貼簿內容轉換成 DataFrame 你可以從 Excel、Google Sheet 或是網頁上複製表格並將其轉成 DataFrame。 簡單 2 步驟： 複製其他來源的表格 執行 pd.read_clipboard 您的瀏覽器不支援影片標籤，請留言通知我：S 這個技巧在你想要快速將一些數據轉成 DataFrame 時非常方便。當然，你得考量重現性（reproducibility）。 為了讓未來的自己以及他人可以重現你當下的結果，必要時記得另存新檔以供後人使用： df . to_csv ( \"some_data.csv\" ) 讀取線上 CSV 檔 不限於本地檔案，只要有正確的 URL 以及網路連線就可以將網路上的任意 CSV 檔案轉成 DataFrame。 比方說你可以將 Kaggle 著名的 鐵達尼號競賽 的 CSV 檔案從網路上下載下來並轉成 DataFrame： df = pd . read_csv ( 'http://bit.ly/kaggletrain' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 以下則是另個使用 pandas 爬取網路上數據並作分析的真實案例。 我在之前的 Chartify 教學文 中從臺北市的資料開放平台爬取 A1 及 A2 類交通事故數據 後做了簡單匯總： # 你可以用類似的方式爬取任何網路上的公開數據集 base_url = \"https://data.taipei/api/getDatasetInfo/downloadResource?id= {} &rid= {} \" _id = \"2f238b4f-1b27-4085-93e9-d684ef0e2735\" rid = \"ea731a84-e4a1-4523-b981-b733beddbc1f\" csv_url = base_url . format ( _id , rid ) df_raw = pd . read_csv ( csv_url , encoding = 'big5' ) # 複製一份做處理 df = df_raw . copy () # 計算不同區不同性別的死亡、受傷人數 df [ '區序' ] = df [ '區序' ] . apply ( lambda x : '' . join ([ i for i in x if not i . isdigit ()])) df = ( df [ df [ '性別' ] . isin ([ 1 , 2 ])] . groupby ([ '區序' , '性別' ])[[ '死亡人數' , '受傷人數' ]] . sum () . reset_index () . sort_values ( '受傷人數' )) df [ '性別' ] = df [ '性別' ] . apply ( lambda x : '男性' if x == 1 else '女性' ) df = df . reset_index () . drop ( 'index' , axis = 1 ) # 顯示結果 display ( df_raw . head ()) display ( df . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 發生年 發生月 發生日 發生時 發生分 處理別 區序 肇事地點 死亡人數 受傷人數 當事人序 車種 性別 年齡 受傷程度 4天候 7速限 8道路型態 9事故位置 0 107 3 29 15 54 2 01大同區 大同區民權西路108號 0 1 1 B03 2 41.0 3.0 8 50 14.0 9.0 1 107 3 29 15 54 2 01大同區 大同區民權西路108號 0 1 2 C03 2 58.0 2.0 8 50 14.0 9.0 2 107 1 7 17 42 2 01大同區 大同區重慶北路2段與南京西路口 0 1 1 B01 1 59.0 3.0 6 40 4.0 2.0 3 107 1 7 17 42 2 01大同區 大同區重慶北路3段與南京西路口 0 1 2 C03 1 18.0 2.0 6 40 4.0 2.0 4 107 1 14 9 56 2 01大同區 大同區承德路3段與民族西路口 0 1 1 C03 1 20.0 2.0 8 50 4.0 1.0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 區序 性別 死亡人數 受傷人數 0 南港區 女性 2 799 1 萬華區 女性 2 1084 2 松山區 女性 30 1146 3 大同區 女性 3 1336 4 士林區 女性 3 1660 有了匯總過後的 DataFrame，你可以用後面 簡易繪圖並修改預設樣式 章節提到的 pandas plot 函式繪圖。但在這邊讓我用 Chartify 展示結果： import chartify ch = chartify . Chart ( x_axis_type = 'categorical' , y_axis_type = 'categorical' ) ch . plot . heatmap ( data_frame = df , y_column = '性別' , x_column = '區序' , color_column = '受傷人數' , text_column = '受傷人數' , color_palette = 'Reds' , text_format = ' {:,.0f} ' ) ( ch . set_title ( '2017 年台北市各行政區交通意外受傷人數' ) . set_subtitle ( '性別分計' ) . set_source_label ( \"Data.Taipei\" ) . axes . set_xaxis_label ( '性別' ) . axes . set_yaxis_label ( '行政區' ) . show ( 'png' )) 過來人經驗。雖然像這樣利用 pandas 直接從網路上下載並分析數據很方便，有時 host 數據的網頁與機構（尤其是政府機關）會無預期地修改他們網站，導致數據集的 URL 失效（苦主）。為了最大化重現性，我還是會建議將數據載到本地備份之後，再做分析比較實在。 優化記憶體使用量 你可以透過 df.info 查看 DataFrame 當前的記憶體用量： df . info ( memory_usage = \"deep\" ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 322.0 KB 從最後一列可以看出鐵達尼號這個小 DataFrame 只佔了 322 KB。如果你是透過 Jupyter 筆記本 來操作 pandas，也可以考慮用 Variable Inspector 插件來觀察包含 DataFrame 等變數的大小： 您的瀏覽器不支援影片標籤，請留言通知我：S Variable Inspector 這邊使用的 df 不佔什麼記憶體，但如果你想讀入的 DataFrame 很大，可以只讀入特定的欄位並將已知的分類型（categorical）欄位轉成 category 型態以節省記憶體（在分類數目較數據量小時有效）： dtypes = { \"Embarked\" : \"category\" } cols = [ 'PassengerId' , 'Name' , 'Sex' , 'Embarked' ] df = pd . read_csv ( 'http://bit.ly/kaggletrain' , dtype = dtypes , usecols = cols ) df . info ( memory_usage = \"deep\" ) <class 'pandas.core.frame.DataFrame'> RangeIndex: 891 entries, 0 to 890 Data columns (total 4 columns): PassengerId 891 non-null int64 Name 891 non-null object Sex 891 non-null object Embarked 889 non-null category dtypes: category(1), int64(1), object(2) memory usage: 134.9 KB 透過減少讀入的欄位數並將 object 轉換成 category 欄位，讀入的 df 只剩 135 KB。只需剛剛的 40 % 記憶體用量。 另外如果你想在有限的記憶體內處理巨大 CSV 檔案，也可以透過 chunksize 參數來限制一次讀入的列數（rows）： from IPython.display import display # chunksize=4 表示一次讀入 4 筆樣本 reader = pd . read_csv ( 'dataset/titanic-train.csv' , chunksize = 4 , usecols = cols ) # 秀出前兩個 chunks for _ , df_partial in zip ( range ( 2 ), reader ): display ( df_partial ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Name Sex Embarked 0 1 Braund, Mr. Owen Harris male S 1 2 Cumings, Mrs. John Bradley (Florence Briggs Th... female C 2 3 Heikkinen, Miss. Laina female S 3 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female S .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Name Sex Embarked 4 5 Allen, Mr. William Henry male S 5 6 Moran, Mr. James male Q 6 7 McCarthy, Mr. Timothy J male S 7 8 Palsson, Master. Gosta Leonard male S 讀入並合併多個 CSV 檔案成單一 DataFrame 很多時候因為企業內部 ETL 或是數據處理的方式（比方說 利用 Airflow 處理批次數據 ），相同類型的數據可能會被分成多個不同的 CSV 檔案儲存。 假設在本地端 dataset 資料夾內有 2 個 CSV 檔案，分別儲存鐵達尼號上不同乘客的數據： pd . read_csv ( \"dataset/passenger1.csv\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 5 0 3 Allen male 35.0 0 0 373450 8.0500 NaN S 1 6 0 3 Moran male NaN 0 0 330877 8.4583 NaN Q 2 7 0 1 McCarthy male 54.0 0 0 17463 51.8625 E46 S 另外一個 CSV 內容： pd . read_csv ( \"dataset/passenger2.csv\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 13 0 3 Saundercock male 20.0 0 0 A/5. 2151 8.0500 NaN S 1 14 0 3 Andersson male 39.0 1 5 347082 31.2750 NaN S 2 15 0 3 Vestrom female 14.0 0 0 350406 7.8542 NaN S 注意上面 2 個 DataFrames 的內容雖然分別代表不同乘客，其格式卻是一模一樣。這種時候你可以使用 pd.concat 將分散在不同 CSV 的乘客數據合併成單一 DataFrame，方便之後處理： from glob import glob files = glob ( \"dataset/passenger*.csv\" ) df = pd . concat ([ pd . read_csv ( f ) for f in files ]) df . reset_index ( drop = True ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 5 0 3 Allen male 35.0 0 0 373450 8.0500 NaN S 1 6 0 3 Moran male NaN 0 0 330877 8.4583 NaN Q 2 7 0 1 McCarthy male 54.0 0 0 17463 51.8625 E46 S 3 13 0 3 Saundercock male 20.0 0 0 A/5. 2151 8.0500 NaN S 4 14 0 3 Andersson male 39.0 1 5 347082 31.2750 NaN S 5 15 0 3 Vestrom female 14.0 0 0 350406 7.8542 NaN S 你還可以使用 reset_index 函式來重置串接後的 DataFrame 索引。 前面說過很多 pandas 函式 預設 的 axis 參數為 0 ，代表著以 列（row） 為單位做特定的操作。在 pd.concat 的例子中則是將 2 個同樣格式的 DataFrames 依照 列 串接起來。 有時候同一筆數據的不同特徵值（features）會被存在不同檔案裡頭。以鐵達尼號的數據集舉例： pd . read_csv ( \"dataset/feature_set1.csv\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name 0 1 0 3 Braund 1 2 1 1 Cumings 2 3 1 3 Heikkinen 3 4 1 1 Futrelle 4 5 0 3 Allen 除了乘客名稱以外，其他如年齡以及性別等特徵值則被存在另個 CSV 裡頭： pd . read_csv ( \"dataset/feature_set2.csv\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 female 35.0 1 0 113803 53.1000 C123 S 4 male 35.0 0 0 373450 8.0500 NaN S 假設這 2 個 CSV 檔案裡頭 同列 對應到同個乘客，則你可以很輕鬆地用 pd.concat 函式搭配 axis=1 將不同 DataFrames 依照 行（column） 串接： files = glob ( \"dataset/feature_set*.csv\" ) pd . concat ([ pd . read_csv ( f ) for f in files ], axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen male 35.0 0 0 373450 8.0500 NaN S 客製化 DataFrame 顯示設定 雖然 pandas 會盡可能地將一個 DataFrame 完整且漂亮地呈現出來，有時候你還是會想要改變預設的顯示方式。這節列出一些常見的使用情境。 完整顯示所有欄位 有時候一個 DataFrame 裡頭的欄位太多， pandas 會自動省略某些中間欄位以保持頁面整潔： df = pd . util . testing . makeCustomDataframe ( 5 , 25 ) df 但如果你無論如何都想要顯示所有欄位以方便一次查看，可以透過 pd.set_option 函式來改變 display.max_columns 設定： pd . set_option ( \"display.max_columns\" , None ) df 注意 ... 消失了。另外你也可以使用 T 來轉置（transpose）當前 DataFrame，垂直顯示所有欄位： # 注意轉置後 `head(15)` 代表選擇前 15 個欄位 df . T . head ( 15 ) 這個測試用的 DataFrame 欄位裡頭的 C 正代表著 column。你可以在 pandas 官方文件裡查看其他常用的顯示設定 。 減少顯示的欄位長度 這邊你一樣可以透過 pd.set_option 函式來限制鐵達尼號資料集裡頭 Name 欄位的顯示長度： from IPython.display import display print ( \"display.max_colwidth 預設值：\" , pd . get_option ( \"display.max_colwidth\" )) # 使用預設設定來顯示 DataFrame df = pd . read_csv ( 'http://bit.ly/kaggletrain' ) display ( df . head ( 3 )) print ( \"注意 Name 欄位的長度被改變了：\" ) # 客製化顯示（global） pd . set_option ( \"display.max_colwidth\" , 10 ) df . head ( 3 ) display.max_colwidth 預設值： 50 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 注意 Name 欄位的長度被改變了： .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund... male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cuming... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikki... female 26.0 0 0 STON/O... 7.9250 NaN S 改變浮點數顯示位數 除了欄位長度以外，你常常會想要改變浮點數（float）顯示的小數點位數： pd . set_option ( \"display.precision\" , 1 ) df . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund... male 22.0 1 0 A/5 21171 7.2 NaN S 1 2 1 1 Cuming... female 38.0 1 0 PC 17599 71.3 C85 C 2 3 1 3 Heikki... female 26.0 0 0 STON/O... 7.9 NaN S 你會發現 Fare 欄位現在只顯示小數點 後一位 的數值了。另外注意剛剛設定的 max_colwidth 是會被套用到所有 DataFrame 的。因此這個 DataFrame 的 Name 欄位顯示的寬度還跟上個 DataFrame 相同：都被縮減了。 想要將所有調整過的設定初始化，可以執行： pd . reset_option ( \"all\" ) 其他常用的 options 包含： max_rows max_columns date_yearfirst 等等。執行 pd.describe_option() 可以顯示所有可供使用的 options，但如果你是在 Jupyter 筆記本內使用 pandas 的話，我推薦直接在 set_option 函式的括號裡輸入 Shift + tab 顯示所有選項： 您的瀏覽器不支援影片標籤，請留言通知我：S 為特定 DataFrame 加點樣式 pd.set_option 函式在你想要把某些顯示設定套用到 所有 DataFrames 時很好用。不過很多時候你會想要讓不同 DataFrame 有不同的顯示設定或樣式（styling）。 比方說針對下面這個只有 10 筆數據的 DataFrame，你想要跟上一節一樣把 Fare 欄位弄成只有小數點後一位，但又不想影響到其他 DataFrame 或是其他欄位： # 隨機抽樣 10 筆數據來做 styling df_sample = df . sample ( n = 10 , random_state = 9527 ) . drop ( 'Name' , axis = 1 ) df_sample . Age . fillna ( int ( df . Age . mean ()), inplace = True ) df_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S 182 183 0 3 male 9.0 4 2 347077 31.3875 NaN S 376 377 1 3 female 22.0 0 0 C 7077 7.2500 NaN S 659 660 0 1 male 58.0 0 2 35273 113.2750 D48 C 431 432 1 3 female 29.0 1 0 376564 16.1000 NaN S 539 540 1 1 female 22.0 0 2 13568 49.5000 B39 C 478 479 0 3 male 22.0 0 0 350060 7.5208 NaN S 791 792 0 2 male 16.0 0 0 239865 26.0000 NaN S 303 304 1 2 female 29.0 0 0 226593 12.3500 E101 Q 70 71 0 2 male 32.0 0 0 C.A. 33111 10.5000 NaN S 這時候你可以使用 pandas Styler 底下的 format 函式來做到這件事情： # 一個典型 chain pandas 函式的例子 ( df_sample . style . format ( ' {:.1f} ' , subset = 'Fare' ) . set_caption ( '★五顏六色の鐵達尼號數據集☆' ) . hide_index () . bar ( 'Age' , vmin = 0 ) . highlight_max ( 'Survived' ) . background_gradient ( 'Greens' , subset = 'Fare' ) . highlight_null () ) 如果你從來沒有用過 df.style ，這應該是你這輩子看過最繽紛的 DataFrame。 從上而下，上述程式碼對此 DataFrame 做了以下 styling： 將 Fare 欄位的數值顯示限制到小數後第一位 添加一個標題輔助說明 隱藏索引（注意最左邊！） 將 Age 欄位依數值大小畫條狀圖 將 Survived 最大的值 highlight 將 Fare 欄位依數值畫綠色的 colormap 將整個 DataFrame 的空值顯示為紅色 pd.DataFrame.style 會回傳一個 Styler。你已經看到除了 format 函式以外，還有很多其他函式可以讓你為 DataFrame 添加樣式。使用 format 函式的最大好處是你不需要用像是 round 等函式去修改 實際 數值，而只是改變 呈現結果 而已。 熟悉 styling 技巧能讓你不需畫圖就能輕鬆與他人分享簡單的分析結果，也能凸顯你想讓他們關注的事物。小提醒：為了讓你能一次掌握常用函式，我把能加的樣式都加了。實務上你應該思考 什麼視覺變數是必要的 ，而不是盲目地添加樣式。 另外值得一提的是 pandas 函式都會回傳處理後的結果，而不是直接修改原始 DataFrame。這讓你可以輕鬆地把多個函式串（chain）成一個複雜的數據處理 pipeline，但又不會影響到最原始的數據： df_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S 182 183 0 3 male 9.0 4 2 347077 31.3875 NaN S 376 377 1 3 female 22.0 0 0 C 7077 7.2500 NaN S 659 660 0 1 male 58.0 0 2 35273 113.2750 D48 C 431 432 1 3 female 29.0 1 0 376564 16.1000 NaN S 539 540 1 1 female 22.0 0 2 13568 49.5000 B39 C 478 479 0 3 male 22.0 0 0 350060 7.5208 NaN S 791 792 0 2 male 16.0 0 0 239865 26.0000 NaN S 303 304 1 2 female 29.0 0 0 226593 12.3500 E101 Q 70 71 0 2 male 32.0 0 0 C.A. 33111 10.5000 NaN S 瞧！原來的 DataFrame 還是挺淳樸的。注意 Fare 欄位裡的小數點並沒有因為剛剛的 styling 而變少，而這讓你在呈現 DataFrame 時有最大的彈性。 數據清理 & 整理 這節列出一些十分常用的數據清理與整理技巧，如處理空值（null value）以及切割欄位。 處理空值 世界總是殘酷，很多時候手上的 DataFrame 裡頭會有不存在的值，如底下一格格額外顯眼的 NaN ： df = pd . util . testing . makeMissingDataframe () . head () df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 6bAs9RrtPf -1.864693 0.395997 -1.057175 1.261987 P4pLVw54w1 0.817813 -0.893529 NaN NaN KkhqmjKghu 0.182929 0.892841 1.487175 -1.218580 S4ppq42BKt 1.113693 0.979332 -0.761886 0.026489 Mu2ryPVR0x 0.387857 0.334798 1.321586 NaN 你可以利用 fillna 函式將 DataFrame 裡頭所有不存在的值設為 0 ： df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 6bAs9RrtPf -1.864693 0.395997 -1.057175 1.261987 P4pLVw54w1 0.817813 -0.893529 0.000000 0.000000 KkhqmjKghu 0.182929 0.892841 1.487175 -1.218580 S4ppq42BKt 1.113693 0.979332 -0.761886 0.026489 Mu2ryPVR0x 0.387857 0.334798 1.321586 0.000000 當然，這個操作的前提是你確定在當前分析的情境下，將不存在的值視為 0 這件事情是沒有問題的。 針對字串欄位，你也可以將空值設定成任何容易識別的值，讓自己及他人明確了解此 DataFrame 的數據品質： df = pd . util . testing . makeMissingCustomDataframe ( nrows = 5 , ncols = 4 , dtype = str ) df . fillna ( \"Unknown\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C0 C_l0_g0 C_l0_g1 C_l0_g2 C_l0_g3 R0 R_l0_g0 R0C0 R0C1 R0C2 R0C3 R_l0_g1 R1C0 R1C1 R1C2 R1C3 R_l0_g2 R2C0 R2C1 R2C2 R2C3 R_l0_g3 R3C0 Unknown R3C2 R3C3 R_l0_g4 Unknown R4C1 R4C2 R4C3 捨棄不需要的行列 給定一個初始 DataFrame： df = pd . util . testing . makeDataFrame () . head () df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 87dl3rTYXR 0.048544 0.133999 0.885555 1.164883 DE0ptDHz2Q 0.551378 0.012621 0.256241 0.197140 jiWMAjniDH -1.275694 -1.473950 -1.602433 -0.286107 45lFb2QT9g 1.348614 2.235074 1.471781 0.449909 OOOtaUhOOp -0.297613 -1.332934 0.194135 0.654267 你可以使用 drop 函式來捨棄不需要的欄位。記得將 axis 設為 1： columns = [ 'B' , 'D' ] df . drop ( columns , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A C 87dl3rTYXR 0.048544 0.885555 DE0ptDHz2Q 0.551378 0.256241 jiWMAjniDH -1.275694 -1.602433 45lFb2QT9g 1.348614 1.471781 OOOtaUhOOp -0.297613 0.194135 同理，你也可以捨棄特定列（row）： df . drop ( 'OOOtaUhOOp' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 87dl3rTYXR 0.048544 0.133999 0.885555 1.164883 DE0ptDHz2Q 0.551378 0.012621 0.256241 0.197140 jiWMAjniDH -1.275694 -1.473950 -1.602433 -0.286107 45lFb2QT9g 1.348614 2.235074 1.471781 0.449909 重置並捨棄索引 很多時候你會想要重置一個 DataFrame 的索引，以方便使用 loc 或 iloc 屬性 來存取想要的數據。 給定一個 DataFrame： df = pd . util . testing . makeDataFrame () . head () df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D LI5Y0d3Ygk 0.704950 0.610508 -1.686467 0.165393 9EkrfcGaKh 1.088294 -0.947646 1.382226 -1.410468 Jf891pQjSh 0.137240 1.490714 1.236335 -0.270623 Q9O2vu4Mg1 -0.375816 2.308924 -1.735557 1.734371 z0xoUxOAWW 1.067996 0.474090 -0.492757 -0.042121 你可以使用 reset_index 函式來重置此 DataFrame 的索引並輕鬆存取想要的部分： df . reset_index ( inplace = True ) df . iloc [: 3 , :] # 豆知識：因為 iloc 是屬性而非函式， # 因此你得使用 [] 而非 () 存取數據 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index A B C D 0 LI5Y0d3Ygk 0.704950 0.610508 -1.686467 0.165393 1 9EkrfcGaKh 1.088294 -0.947646 1.382226 -1.410468 2 Jf891pQjSh 0.137240 1.490714 1.236335 -0.270623 將函式的 inplace 參數設為 True 會讓 pandas 直接修改 df 。一般來說 pandas 裡的函式並不會修改原始 DataFrame，這樣可以保證原始數據不會受到任何函式的影響。 當你不想要原來的 DataFrame df 受到 reset_index 函式的影響，則可以將處理後的結果交給一個新 DataFrame（比方說 df1 ）： df = pd . util . testing . makeDataFrame () . head () df1 = df . reset_index ( drop = True ) display ( df ) display ( df1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D fNMRqfPnw1 0.458255 1.736118 -1.659602 2.325742 xeIJ4tJuxW -1.102030 1.321306 -0.262345 0.864090 RDAJlNsfDS 1.047750 -0.420285 0.757464 0.384514 kIOVU4EU79 -0.843373 0.235789 -1.211142 0.656130 OKuRXq4eEK -1.340040 0.674713 0.768126 -0.500718 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 0.458255 1.736118 -1.659602 2.325742 1 -1.102030 1.321306 -0.262345 0.864090 2 1.047750 -0.420285 0.757464 0.384514 3 -0.843373 0.235789 -1.211142 0.656130 4 -1.340040 0.674713 0.768126 -0.500718 透過這樣的方式，pandas 讓你可以放心地對原始數據做任何壞壞的事情而不會產生任何不好的影響。 將字串切割成多個欄位 在處理文本數據時，很多時候你會想要把一個字串欄位拆成多個欄位以方便後續處理。 給定一個簡單 DataFrame： df = pd . DataFrame ({ \"name\" : [ \"大雄\" , \"胖虎\" ], \"feature\" : [ \"膽小, 翻花繩\" , \"粗魯, 演唱會\" ] }) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name feature 0 大雄 膽小, 翻花繩 1 胖虎 粗魯, 演唱會 你可能會想把這個 DataFrame 的 feature 欄位分成不同欄位，這時候利用 str 將字串取出，並透過 expand=True 將字串切割的結果擴大成（expand）成一個 DataFrame： df [[ '性格' , '特技' ]] = df . feature . str . split ( ',' , expand = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name feature 性格 特技 0 大雄 膽小, 翻花繩 膽小 翻花繩 1 胖虎 粗魯, 演唱會 粗魯 演唱會 注意我們使用 df[columns] = ... 的形式將字串切割出來的 2 個新欄位分別指定成 性格 與 特技 。 將 list 分成多個欄位 有時候一個欄位裡頭的值為 Python list ： df = pd . DataFrame ({ \"name\" : [ \"大雄\" , \"胖虎\" ], \"feature\" : [[ \"膽小\" , \"翻花繩\" ], [ \"粗魯\" , \"演唱會\" ]] }) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name feature 0 大雄 [膽小, 翻花繩] 1 胖虎 [粗魯, 演唱會] 這時則可以使用 tolist 函式做到跟剛剛字串切割相同的效果： cols = [ '性格' , '特技' ] pd . DataFrame ( df . feature . tolist (), columns = cols ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 性格 特技 0 膽小 翻花繩 1 粗魯 演唱會 你也可以使用 apply(pd.Series) 的方式達到一樣的效果： df . feature . apply ( pd . Series ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 膽小 翻花繩 1 粗魯 演唱會 遇到以 Python list 呈現欄位數據的情境不少，這些函式能讓你少抓點頭。 取得想要關注的數據 通常你會需要依照各種不同的分析情境，將整個 DataFrame 裡頭的一部份數據取出並進一步分析。這節內容讓你能夠輕鬆取得想要關注的數據。 基本數據切割 在 pandas 裡頭，切割（Slice）DataFrame 裡頭一部份數據出來做分析是稀鬆平常的事情。讓我們再次以鐵達尼號數據集為例： df = pd . read_csv ( 'http://bit.ly/kaggletrain' ) df = df . drop ( \"Name\" , axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S 你可以透過 loc 以及 : 的方式輕鬆選取從某個起始欄位 C1 到結束欄位 C2 的所有欄位，而無需將中間的欄位一一列出： df . loc [: 3 , 'Pclass' : 'Ticket' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pclass Sex Age SibSp Parch Ticket 0 3 male 22.0 1 0 A/5 21171 1 1 female 38.0 1 0 PC 17599 2 3 female 26.0 0 0 STON/O2. 3101282 3 1 female 35.0 1 0 113803 反向選取行列 透過 Python 常見的 [::-1] 語法，你可以輕易地改變 DataFrame 裡頭所有欄位的排列順序： df . loc [: 3 , :: - 1 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Embarked Cabin Fare Ticket Parch SibSp Age Sex Pclass Survived PassengerId 0 S NaN 7.2500 A/5 21171 0 1 22.0 male 3 0 1 1 C C85 71.2833 PC 17599 0 1 38.0 female 1 1 2 2 S NaN 7.9250 STON/O2. 3101282 0 0 26.0 female 3 1 3 3 S C123 53.1000 113803 0 1 35.0 female 1 1 4 同樣概念也可以運用到列（row）上面。你可以將所有樣本（samples）排序顛倒並選取其中 N 列： df . iloc [:: - 1 , : 5 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age 890 891 0 3 male 32.0 889 890 1 1 male 26.0 888 889 0 3 female NaN 887 888 1 1 female 19.0 886 887 0 2 male 27.0 注意我們同時使用 :5 來選出前 5 個欄位。 條件選取數據 在 pandas 裡頭最實用的選取技巧大概非遮罩（masking）莫屬了。遮罩讓 pandas 將符合特定條件的樣本回傳： male_and_age_over_70 = ( df . Sex == 'male' ) & ( df . Age > 70 ) ( df [ male_and_age_over_70 ] . style . applymap ( lambda x : 'background-color: rgb(153, 255, 51)' , subset = pd . IndexSlice [:, 'Sex' : 'Age' ])) # 跟 df[(df.Sex == 'male') & (df.Age > 70)] 結果相同 #T_68f75918_b060_11e9_8b6c_8c8590794fe2row0_col3 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row0_col4 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row1_col3 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row1_col4 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row2_col3 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row2_col4 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row3_col3 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row3_col4 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row4_col3 { background-color: rgb(153, 255, 51); } #T_68f75918_b060_11e9_8b6c_8c8590794fe2row4_col4 { background-color: rgb(153, 255, 51); } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 96 97 0 1 male 71 0 0 PC 17754 34.6542 A5 C 116 117 0 3 male 70.5 0 0 370369 7.75 nan Q 493 494 0 1 male 71 0 0 PC 17609 49.5042 nan C 630 631 1 1 male 80 0 0 27042 30 A23 S 851 852 0 3 male 74 0 0 347060 7.775 nan S male_and_age_over_70 是我們定義的一個遮罩，可以把同時符合兩個布林判斷式（大於 70 歲、男性）的樣本選取出來。上面註解有相同效果，但當存在多個判斷式時，有個準確說明遮罩意義的變數（上例的 male_and_age_over_70 ）會讓你的程式碼好懂一點。 另外你也可以使用 query 函式來達到跟遮罩一樣的效果： age = 70 df . query ( \"Age > @age & Sex == 'male'\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 96 97 0 1 male 71.0 0 0 PC 17754 34.6542 A5 C 116 117 0 3 male 70.5 0 0 370369 7.7500 NaN Q 493 494 0 1 male 71.0 0 0 PC 17609 49.5042 NaN C 630 631 1 1 male 80.0 0 0 27042 30.0000 A23 S 851 852 0 3 male 74.0 0 0 347060 7.7750 NaN S 在這個例子裡頭，你可以使用 @ 來存取已經定義的 Python 變數 age 的值。 選擇任一欄有空值的樣本 一個 DataFrame 裡常會有多個欄位（column），而每個欄位裡頭都有可能包含空值。 有時候你會想把在 任一 欄位（column）出現過空值的樣本（row）全部取出： df [ df . isnull () . any ( axis = 1 )] . head () \\ . style . highlight_null () #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row0_col9 { background-color: red; } #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row1_col9 { background-color: red; } #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row2_col9 { background-color: red; } #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row3_col4 { background-color: red; } #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row3_col9 { background-color: red; } #T_31d6affa_b061_11e9_8b6c_8c8590794fe2row4_col9 { background-color: red; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22 1 0 A/5 21171 7.25 nan S 2 3 1 3 female 26 0 0 STON/O2. 3101282 7.925 nan S 4 5 0 3 male 35 0 0 373450 8.05 nan S 5 6 0 3 male nan 0 0 330877 8.4583 nan Q 7 8 0 3 male 2 3 1 349909 21.075 nan S 這邊剛好所有樣本的 Cabin 欄位皆為空值。但倒數第 2 個樣本就算其 Cabin 欄不為空值，也會因為 Age 欄為空而被選出。 選取或排除特定類型欄位 有時候你會想選取 DataFrame 裡特定數據類型（字串、數值、時間型態等）的欄位，這時你可以使用 select_dtypes 函式： df . select_dtypes ( include = 'number' ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare 0 1 0 3 22.0 1 0 7.2500 1 2 1 1 38.0 1 0 71.2833 2 3 1 3 26.0 0 0 7.9250 3 4 1 1 35.0 1 0 53.1000 4 5 0 3 35.0 0 0 8.0500 上面我們用一行程式碼就把所有數值欄位取出，儘管我們根本不知道有什麼欄位。而你當然也可以利用 exclude 參數來排除特定類型的欄位： # 建立一個有多種數據形態的 DataFrame df_mix = pd . util . testing . makeMixedDataFrame () display ( df_mix ) display ( df_mix . dtypes ) display ( df_mix . select_dtypes ( exclude = [ 'datetime64' , 'object' ])) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 0.0 0.0 foo1 2009-01-01 1 1.0 1.0 foo2 2009-01-02 2 2.0 0.0 foo3 2009-01-05 3 3.0 1.0 foo4 2009-01-06 4 4.0 0.0 foo5 2009-01-07 A float64 B float64 C object D datetime64[ns] dtype: object .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 0 0.0 0.0 1 1.0 1.0 2 2.0 0.0 3 3.0 1.0 4 4.0 0.0 pandas 裡的函式使用上都很直覺，你可以丟入 1 個包含多個元素的 Python list 或是單一 str 作為參數輸入。 選取所有出現在 list 內的樣本 很多時候針對某一個特定欄位，你會想要取出所有出現在一個 list 的樣本。這時候你可以使用 isin 函式來做到這件事情： tickets = [ \"SC/Paris 2123\" , \"PC 17475\" ] df [ df . Ticket . isin ( tickets )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 43 44 1 2 female 3.0 1 2 SC/Paris 2123 41.5792 NaN C 608 609 1 2 female 22.0 1 2 SC/Paris 2123 41.5792 NaN C 685 686 0 2 male 25.0 1 2 SC/Paris 2123 41.5792 NaN C 701 702 1 1 male 35.0 0 0 PC 17475 26.2875 E24 S 選取某欄位為 top-k 值的樣本 很多時候你會想選取在某個欄位中前 k 大的所有樣本。這時你可以先利用 value_counts 函式找出該欄位前 k 多的值： top_k = 3 top_tickets = df . Ticket . value_counts ()[: top_k ] top_tickets . index Index(['347082', '1601', 'CA. 2343'], dtype='object') 這邊我們以欄位 Ticket 為例。另外你也可以使用 pandas.Series 裡的 nlargest 函式取得相同結果： df . Ticket . value_counts () . nlargest ( top_k ) . index Index(['347082', '1601', 'CA. 2343'], dtype='object') 接著利用上小節看過的 isin 函式就能輕鬆取得 Ticket 欄位值為前 k 大值的樣本： df [ df . Ticket . isin ( top_tickets . index )] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 13 14 0 3 male 39.0 1 5 347082 31.2750 NaN S 74 75 1 3 male 32.0 0 0 1601 56.4958 NaN S 119 120 0 3 female 2.0 4 2 347082 31.2750 NaN S 159 160 0 3 male NaN 8 2 CA. 2343 69.5500 NaN S 169 170 0 3 male 28.0 0 0 1601 56.4958 NaN S 找出符合特定字串的樣本 有時你會想要對一個字串欄位做正規表示式（regular expression），取出符合某個 pattern 的所有樣本。 這時你可以使用 str 底下的 contains 函式： df = pd . read_csv ( 'http://bit.ly/kaggletrain' ) df [ df . Name . str . contains ( \"Mr\\.\" )] . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 5 6 0 3 Moran, Mr. James male NaN 0 0 330877 8.4583 NaN Q 6 7 0 1 McCarthy, Mr. Timothy J male 54.0 0 0 17463 51.8625 E46 S 12 13 0 3 Saundercock, Mr. William Henry male 20.0 0 0 A/5. 2151 8.0500 NaN S 這邊我們將所有 Name 欄位值裡包含 Mr. 的樣本取出。注意 contains 函式接受的是正規表示式，因此需要將 . 轉換成 \\. 。 使用正規表示式選取數據 有時候你會想要依照一些規則來選取 DataFrame 裡頭的值、索引或是欄位，尤其是在處理跟時間序列相關的數據： df_date = pd . util . testing . makeTimeDataFrame ( freq = '7D' ) df_date . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 -0.143945 -0.020783 1.434651 1.044208 2000-01-08 -0.901508 -1.082932 -1.035743 0.334431 2000-01-15 -1.544868 1.183136 -0.913117 -1.199199 2000-01-22 0.604091 -0.233628 -0.348109 1.082138 2000-01-29 0.062112 1.565870 -0.791369 1.017766 2000-02-05 -0.013661 1.015528 -0.420123 -0.513559 2000-02-12 0.404094 0.486909 -0.815937 0.743381 2000-02-19 -0.288886 2.560776 -0.864528 0.727740 2000-02-26 0.975203 -0.551452 0.531635 -0.595716 2000-03-04 -0.045714 0.137413 2.187056 1.164371 假設你想將所有索引在 2000 年 2 月內的樣本取出，則可以透過 filter 函式達成這個目的： df_date . filter ( regex = \"2000-02.*\" , axis = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-02-05 -0.013661 1.015528 -0.420123 -0.513559 2000-02-12 0.404094 0.486909 -0.815937 0.743381 2000-02-19 -0.288886 2.560776 -0.864528 0.727740 2000-02-26 0.975203 -0.551452 0.531635 -0.595716 filter 函式本身功能十分強大，有興趣的讀者可以 閱讀官方文件進一步了解其用法 。 選取從某時間點開始的區間樣本 在處理時間數據時，很多時候你會想要針對某個起始時間挑出前 t 個時間點的樣本。 讓我們再看一次剛剛建立的 DataFrame： df_date . head ( 8 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 -0.143945 -0.020783 1.434651 1.044208 2000-01-08 -0.901508 -1.082932 -1.035743 0.334431 2000-01-15 -1.544868 1.183136 -0.913117 -1.199199 2000-01-22 0.604091 -0.233628 -0.348109 1.082138 2000-01-29 0.062112 1.565870 -0.791369 1.017766 2000-02-05 -0.013661 1.015528 -0.420123 -0.513559 2000-02-12 0.404094 0.486909 -0.815937 0.743381 2000-02-19 -0.288886 2.560776 -0.864528 0.727740 在索引為時間型態的情況下，如果你想要把前 3 週的樣本取出，可以使用 first 函式： df_date . first ( '3W' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 -0.143945 -0.020783 1.434651 1.044208 2000-01-08 -0.901508 -1.082932 -1.035743 0.334431 2000-01-15 -1.544868 1.183136 -0.913117 -1.199199 十分方便的函式。 基本數據處理與轉換 在了解如何選取想要的數據以後，你可以透過這節的介紹來熟悉 pandas 裡一些常見的數據處理方式。這章節也是我認為使用 pandas 處理數據時最令人愉快的部分之一。 對某一軸套用相同運算 你時常會需要對 DataFrame 裡頭的每一個欄位（縱軸）或是每一列（橫軸）做相同的運算。 比方說你想將鐵達尼號資料集內的 Survived 數值欄位轉換成人類容易理解的字串： # 重新讀取鐵達尼號數據 df_titanic = pd . read_csv ( 'http://bit.ly/kaggletrain' ) df_titanic = df_titanic . drop ( \"Name\" , axis = 1 ) # 複製一份副本 DataFrame df = df_titanic . copy () columns = df . columns . tolist ()[: 4 ] # 好戲登場 new_col = '存活' columns . insert ( 1 , new_col ) # 調整欄位順序用 df [ new_col ] = df . Survived . apply ( lambda x : '倖存' if x else '死亡' ) df . loc [: 5 , columns ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId 存活 Survived Pclass Sex 0 1 死亡 0 3 male 1 2 倖存 1 1 female 2 3 倖存 1 3 female 3 4 倖存 1 1 female 4 5 死亡 0 3 male 5 6 死亡 0 3 male 透過 apply 函式，我們把一個匿名函式 lambda 套用到整個 df.Survived Series 之上，並以此建立一個新的 存活 欄位。 值得一提的是當你需要追加新的欄位但又不想影響到原始 DataFrame 時，可以使用 copy 函式複製一份副本另行操作。 對每一個樣本做自定義運算 上小節我們用 apply 函式對 DataFrame 裡頭的 某個 Series 做運算並生成新欄位： df [ new_col ] = df . Survived . apply ( ... 不過你時常會想要把樣本（row）裡頭的 多個 欄位一次取出做運算並產生一個新的值。這時你可以自定義一個 Python function 並將 apply 函式套用到整個 DataFrame 之上： df = df_titanic . copy () display ( df . head ()) # apply custom function 可以說是 pandas 裡最重要的技巧之一 d = { 'male' : '男性' , 'female' : '女性' } def generate_desc ( row ): return f \"一名 {row['Age']} 歲的{d[row['Sex']]}\" df [ '描述' ] = df . apply ( generate_desc , axis = 1 ) df . loc [: 4 , 'Sex' :] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sex Age SibSp Parch Ticket Fare Cabin Embarked 描述 0 male 22.0 1 0 A/5 21171 7.2500 NaN S 一名 22.0 歲的男性 1 female 38.0 1 0 PC 17599 71.2833 C85 C 一名 38.0 歲的女性 2 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 一名 26.0 歲的女性 3 female 35.0 1 0 113803 53.1000 C123 S 一名 35.0 歲的女性 4 male 35.0 0 0 373450 8.0500 NaN S 一名 35.0 歲的男性 此例中 apply 函式將 generate_desc 函式 個別 套用到 DataFrame 裡頭的每一個樣本（row），結合 Sex 及 Age 兩欄位資訊，生成新的 描述 。 當然，將 axis 設置為 0 則可以對每一個欄位分別套用自定義的 Python function。 將連續數值轉換成分類數據 有時你會想把一個連續數值（numerical）的欄位分成多個 groups 以方便對每個 groups 做統計。這時候你可以使用 pd.cut 函式： df = df_titanic . copy () # 為了方便比較新舊欄位 columns = df . columns . tolist () new_col = '年齡區間' columns . insert ( 4 , new_col ) # 將 numerical 轉換成 categorical 欄位 labels = [ f '族群 {i} ' for i in range ( 1 , 11 )] df [ new_col ] = pd . cut ( x = df . Age , bins = 10 , labels = labels ) # 可以排序切割後的 categorical 欄位 ( df . sort_values ( new_col , ascending = False ) . reset_index () . loc [: 5 , columns ] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex 年齡區間 Age SibSp Parch Ticket Fare Cabin Embarked 0 631 1 1 male 族群 10 80.0 0 0 27042 30.0000 A23 S 1 852 0 3 male 族群 10 74.0 0 0 347060 7.7750 NaN S 2 55 0 1 male 族群 9 65.0 0 1 113509 61.9792 B30 C 3 97 0 1 male 族群 9 71.0 0 0 PC 17754 34.6542 A5 C 4 494 0 1 male 族群 9 71.0 0 0 PC 17609 49.5042 NaN C 5 117 0 3 male 族群 9 70.5 0 0 370369 7.7500 NaN Q 如上所示，使用 pd.cut 函式建立出來的每個分類 族群 X 有大小之分，因此你可以輕易地使用 sort_values 函式排序樣本。 df [ new_col ] . dtype CategoricalDtype(categories=['族群 1', '族群 2', '族群 3', '族群 4', '族群 5', '族群 6', '族群 7', '族群 8', '族群 9', '族群 10'], ordered=True) 將 DataFrame 隨機切成兩個子集 有時你會想將手上的 DataFrame 隨機切成兩個獨立的子集。選取其中一個子集來訓練機器學習模型是一個常見的情境。 要做到這件事情有很多種方法，你可以使用 scikit-learn 的 train_test_split 或是 numpy 的 np.random.randn ，但假如你想要純 pandas 解法，可以使用 sample 函式： df_train = df_titanic . sample ( frac = 0.8 , random_state = 5566 ) df_test = df_titanic . drop ( df_train . index ) # 顯示結果，無特殊操作 display ( df_train . head ()) display ( df_test . head ()) print ( '各 DataFrame 大小：' , len ( df_titanic ), len ( df_train ), len ( df_test )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 294 295 0 3 male 24.0 0 0 349233 7.8958 NaN S 199 200 0 2 female 24.0 0 0 248747 13.0000 NaN S 864 865 0 2 male 24.0 0 0 233866 13.0000 NaN S 625 626 0 1 male 61.0 0 0 36963 32.3208 D50 S 638 639 0 3 female 41.0 0 5 3101295 39.6875 NaN S .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 10 11 1 3 female 4.0 1 1 PP 9549 16.7000 G6 S 11 12 1 1 female 58.0 0 0 113783 26.5500 C103 S 12 13 0 3 male 20.0 0 0 A/5. 2151 8.0500 NaN S 14 15 0 3 female 14.0 0 0 350406 7.8542 NaN S 16 17 0 3 male 2.0 4 1 382652 29.1250 NaN Q 各 DataFrame 大小： 891 713 178 這個解法的前提是原來的 DataFrame df_titanic 裡頭的索引是獨一無二的。另外記得設定 random_state 以方便別人重現你的結果。 用 SQL 的方式合併兩個 DataFrames 很多時候你會想要將兩個 DataFrames 依照某個共通的欄位（鍵值）合併成單一 DataFrame 以整合資訊。 比方說給定以下兩個 DataFrames： df_city = pd . DataFrame ({ 'state' : [ '密蘇里州' , '亞利桑那州' , '肯塔基州' , '紐約州' ], 'city' : [ '堪薩斯城' , '鳳凰城' , '路易維爾' , '紐約市' ] }) df_info = pd . DataFrame ({ 'city' : [ '路易維爾' , '堪薩斯城' , '鳳凰城' ], 'population' : [ 741096 , 481420 , 4039182 ], 'feature' : list ( 'abc' )}) display ( df_city ) display ( df_info ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state city 0 密蘇里州 堪薩斯城 1 亞利桑那州 鳳凰城 2 肯塔基州 路易維爾 3 紐約州 紐約市 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } city population feature 0 路易維爾 741096 a 1 堪薩斯城 481420 b 2 鳳凰城 4039182 c DataFrame df_city 包含了幾個美國城市以及其對應的州名（state）；DataFrame df_info 則包含城市名稱以及一些數據。如果你想將這兩個 DataFrames 合併（merge），可以使用非常方便的 merge 函式： pd . merge ( left = df_city , right = df_info , how = \"left\" , # left outer join on = \"city\" , # 透過此欄位合併 indicator = True # 顯示結果中每一列的來源 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state city population feature _merge 0 密蘇里州 堪薩斯城 481420.0 b both 1 亞利桑那州 鳳凰城 4039182.0 c both 2 肯塔基州 路易維爾 741096.0 a both 3 紐約州 紐約市 NaN NaN left_only 沒錯， merge 函式運作方式就像 SQL 一樣，可以讓你透過更改 how 參數來做： left ：left outer join right ：right outer join outer : full outer join inner ： inner join 注意合併後的 DataFrame 的最後一列：因為是 left join，就算右側的 df_info 裡頭並沒有紐約市的資訊，我們也能把該城市保留在 merge 後的結果。你還可以透過 indicator=True 的方式讓 pandas 幫我們新增一個 _merge 欄位，輕鬆了解紐約市只存在左側的 df_city 裡。 merge 函式強大之處在於能跟 SQL 一樣為我們抽象化如何合併兩個 DataFrames 的運算。如果你想要了解 SQL 跟 Python 本質上的差異，可以參考 為何資料科學家需要學習 SQL 一文。 存取並操作每一個樣本 我們前面看過，雖然一般可以直接使用 apply 函式 來對每個樣本作運算，有時候你就是會想用 for 迴圈的方式把每個樣本取出處理。 這種時候你可以用 itertuples 函式： for row in df_city . itertuples ( name = 'City' ): print ( f ' {row.city} 是 {row.state} 裡頭的一個城市' ) 堪薩斯城是密蘇里州裡頭的一個城市 鳳凰城是亞利桑那州裡頭的一個城市 路易維爾是肯塔基州裡頭的一個城市 紐約市是紐約州裡頭的一個城市 顧名思義， itertuples 函式回傳的是 Python namedtuple ，也是一個你應該已經很熟悉的資料型態： from collections import namedtuple City = namedtuple ( 'City' , [ 'Index' , 'state' , 'city' ]) c = City ( 3 , '紐約州' , '紐約市' ) c == row True 簡單匯總 & 分析數據 在對數據做些基本處理以後，你會想要從手上的 DataFrame 匯總或整理出一些有用的統計數據。本節介紹一些常用的數據匯總技巧。 取出某欄位 top k 的值 這你在 選取某欄位為 top-k 值的樣本 小節應該就看過了。 但因為這個使用情境實在太常出現，讓我們再次嘗試將鐵達尼號數據集裡頭 Ticket 欄位最常出現的值取出： df = df_titanic . copy () display ( df . head ()) display ( df . Ticket . value_counts () . head ( 5 ) . reset_index ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index Ticket 0 347082 7 1 1601 7 2 CA. 2343 7 3 3101295 6 4 CA 2144 6 value_counts 函式預設就會把欄位裡頭的值依照出現頻率 由高到低 排序，因此搭配 head 函式就可以把最常出現的 top k 值選出。 一行描述數值欄位 當你想要快速了解 DataFrame 裡所有數值欄位的統計數據（最小值、最大值、平均和中位數等）時可以使用 describe 函式： df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 你也可以用 取得想要關注的數據 一節的技巧來選取自己關心的統計數據： df . describe () . loc [[ 'mean' , 'std' ], 'Survived' : 'Age' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass Age mean 0.383838 2.308642 29.699118 std 0.486592 0.836071 14.526497 找出欄位裡所有出現過的值 針對特定欄位使用 unique 函式即可： df . Sex . unique () array(['male', 'female'], dtype=object) 分組匯總結果 很多時候你會想要把 DataFrame 裡頭的樣本依照某些特性分門別類，並依此匯總各組（group）的統計數據。這種時候你可以用 groupby 函式。 讓我們再次拿出鐵達尼號數據集： df = df_titanic . copy () df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S 你可以將所有乘客（列）依照它們的 Pclass 欄位值分組，並計算每組裡頭乘客們的平均年齡： df . groupby ( \"Pclass\" ) . Age . mean () Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 你也可以搭配剛剛看過的 describe 函式來匯總各組的統計數據： df . groupby ( \"Sex\" ) . Survived . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max Sex female 314.0 0.742038 0.438211 0.0 0.0 1.0 1.0 1.0 male 577.0 0.188908 0.391775 0.0 0.0 0.0 0.0 1.0 你也可以依照多個欄位分組，並利用 size 函式迅速地取得各組包含的樣本數： df . groupby ([ \"Sex\" , 'Pclass' ]) . size () . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pclass 1 2 3 Sex female 94 76 144 male 122 108 347 你也可以用 agg 函式（ agg regate，匯總）搭配 groupby 函式來將每一組樣本依照多種方式匯總： df . groupby ([ \"Sex\" , 'Pclass' ]) . Age . agg ([ 'min' , 'max' , 'count' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } min max count Sex Pclass female 1 2.00 63.0 85 2 2.00 57.0 74 3 0.75 63.0 102 male 1 0.92 80.0 101 2 0.67 70.0 99 3 0.42 74.0 253 透過 unstack 函式能讓你產生跟 pivot_table 函式相同的結果： df . groupby ([ \"Sex\" , 'Pclass' ]) . Age . agg ([ 'min' , 'max' , 'count' ]) . unstack () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } min max count Pclass 1 2 3 1 2 3 1 2 3 Sex female 2.00 2.00 0.75 63.0 57.0 63.0 85 74 102 male 0.92 0.67 0.42 80.0 70.0 74.0 101 99 253 當然，你也可以直接使用 pivot_table 函式來匯總各組數據： df . pivot_table ( index = 'Sex' , columns = 'Pclass' , values = 'Age' , aggfunc = [ 'min' , 'max' , 'count' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } min max count Pclass 1 2 3 1 2 3 1 2 3 Sex female 2.00 2.00 0.75 63.0 57.0 63.0 85 74 102 male 0.92 0.67 0.42 80.0 70.0 74.0 101 99 253 依照背景不同，每個人會有偏好的 pandas 使用方式。選擇對你來說最直覺又好記的方式吧！ 結合原始數據與匯總結果 不管是上節的 groupby 搭配 agg 還是 pivot_table ，匯總結果都會以另外一個全新的 DataFrame 表示。 有時候你會想直接把各組匯總的結果放到原本的 DataFrame 裡頭，方便比較原始樣本與匯總結果的差異。這時你可以使用 transform 函式： df = df_titanic . copy () df [ 'Avg_age' ] = df . groupby ( \"Sex\" ) . Age . transform ( \"mean\" ) df [ 'Above_avg_age' ] = df . apply ( lambda x : 'yes' if x . Age > x . Avg_age else 'no' , axis = 1 ) # styling ( df . loc [: 4 , 'Sex' :] . style . highlight_max ( subset = [ 'Avg_age' ]) . applymap ( lambda x : 'background-color: rgb(153, 255, 51)' , subset = pd . IndexSlice [[ 0 , 4 ], [ 'Age' , 'Above_avg_age' ]]) ) #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row0_col1 { background-color: rgb(153, 255, 51); } #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row0_col8 { background-color: yellow; } #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row0_col9 { background-color: rgb(153, 255, 51); } #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row4_col1 { background-color: rgb(153, 255, 51); } #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row4_col8 { background-color: yellow; } #T_51aa6300_b014_11e9_8b6c_8c8590794fe2row4_col9 { background-color: rgb(153, 255, 51); } Sex Age SibSp Parch Ticket Fare Cabin Embarked Avg_age Above_avg_age 0 male 22 1 0 A/5 21171 7.25 nan S 30.7266 no 1 female 38 1 0 PC 17599 71.2833 C85 C 27.9157 yes 2 female 26 0 0 STON/O2. 3101282 7.925 nan S 27.9157 no 3 female 35 1 0 113803 53.1 C123 S 27.9157 yes 4 male 35 0 0 373450 8.05 nan S 30.7266 yes 此例將所有乘客依照性別 Sex 分組之後，計算各組的平均年齡 Age ，並利用 transform 函式將各組結果插入對應的乘客（列）裡頭。你會發現兩名男乘客跟平均男性壽命 Avg_age 欄位相比正好一上一下，這差異則反映到 Above_avg_age 欄位裡頭。 對時間數據做匯總 給定一個跟時間相關的 DataFrame： df_date = pd . util . testing . makeTimeDataFrame ( freq = 'Q' ) . head ( 10 ) * 10 df_date .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-03-31 -20.465499 -5.402825 4.790400 -6.949846 2000-06-30 -3.573217 2.091491 12.725446 -10.215842 2000-09-30 -3.377268 3.600146 13.607341 6.309234 2000-12-31 -11.181567 -16.471324 8.659892 5.619725 2001-03-31 7.148111 8.887097 -0.117279 -1.628799 2001-06-30 9.765045 -1.160198 4.898406 12.054339 2001-09-30 -8.067456 -9.746304 -12.500050 -8.970445 2001-12-31 -3.300273 -11.148755 -13.194654 12.852938 2002-03-31 -6.421518 12.420474 -3.546766 20.694348 2002-06-30 11.967359 9.522958 1.557142 -2.778456 你可以利用 resample 函式來依照不同時間粒度匯總這個時間 DataFrame： ( df_date . resample ( 'Y' ) . A . max () . reset_index () . rename ({ 'index' : 'year' }, axis = 1 ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year A 0 2000-12-31 -3.377268 1 2001-12-31 9.765045 2 2002-12-31 11.967359 此例中將不同年份（ Y ear）的樣本分組，並從每一組的欄位 A 中選出最大值。你可以查看 官方 resampling 說明文件 來了解還有什麼時間粒度可以選擇（分鐘、小時、月份等等）。 另外小細節是你可以利用 numpy 的 broadcasting 運算 輕鬆地將 DataFrame 裡的所有數值做操作（初始 df_date 時的用到的 * 10 ）。 簡易繪圖並修改預設樣式 在 Python 世界裡有很多數據視覺化工具供你選擇，比方說經典的 Matplotlib 以及在 淺談神經機器翻譯一文 中被我拿來視覺化矩陣運算的 Seaborn 。你也可以使用前面看過的 Chartify 搭配 pandas 畫出美麗圖表。 但有時，你只需要 pandas 內建的 plot 函式就能輕鬆地將一個 DataFrame 轉成統計圖： # 用 `plot` 一行畫圖。前兩行只是改變預設樣式 import matplotlib.pyplot as plt plt . style . use ( \"ggplot\" ) df . groupby ( \"Pclass\" ) . Survived . count () . plot ( \"barh\" ); 我們都是視覺動物，pandas 的 plot 函式讓你在進行 探索型數據分析（ E xploratory D ata A nalysis, EDA） 、試著快速了解手上數據集時十分方便。我們在 淺談資料視覺化以及 ggplot2 實踐 一文也已經探討過類似概念： 在做 EDA 時只需要包含最重要且你自己能夠解讀的視覺變數，重點是效率 （ 圖片來源 ） 另外 pandas 底層預設使用 Matplotlib 繪圖，而用過 Matplotlib 的人都知道其初始的繪圖樣式實在不太討喜。你可以透過 plt.style.available 查看所有可供使用的繪圖樣式（style），並將喜歡的樣式透過 plt.style.use() 套用到所有 DataFrames 的 plot 函式： plt . style . available ['seaborn-dark', 'seaborn-darkgrid', 'seaborn-ticks', 'fivethirtyeight', 'seaborn-whitegrid', 'classic', '_classic_test', 'fast', 'seaborn-talk', 'seaborn-dark-palette', 'seaborn-bright', 'seaborn-pastel', 'grayscale', 'seaborn-notebook', 'ggplot', 'seaborn-colorblind', 'seaborn-muted', 'seaborn', 'Solarize_Light2', 'seaborn-paper', 'bmh', 'tableau-colorblind10', 'seaborn-white', 'dark_background', 'seaborn-poster', 'seaborn-deep'] 與 pandas 相得益彰的實用工具 前面幾個章節介紹了不少 pandas 的使用技巧與操作概念，這節則介紹一些我認為十分適合跟 pandas 一起搭配使用的數據工具 / 函式庫。在說明每個工具的功能時，我都會使用你已經十分熟悉的鐵達尼號數據集作為範例 DataFrame： df = df_titanic . copy () df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 male 35.0 0 0 373450 8.0500 NaN S tqdm ：了解你的數據處理進度 tqdm 是一個十分強大的 Python 進度條工具，且有 整合 pandas 。此工具可以幫助我們了解 DataFrame apply 函式的進度。回想一下我們在 對某一軸套用相同運算 一節做的一個簡單 apply 運算： df [ '存活' ] = df . Survived . apply ( lambda x : '倖存' if x else '死亡' ) df . loc [: 5 , 'Survived' : '存活' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked 存活 0 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S 死亡 1 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C 倖存 2 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 倖存 3 1 1 female 35.0 1 0 113803 53.1000 C123 S 倖存 4 0 3 male 35.0 0 0 373450 8.0500 NaN S 死亡 5 0 3 male NaN 0 0 330877 8.4583 NaN Q 死亡 在這個不到 1,000 筆的 DataFrame 做這樣的簡單運算不用一秒鐘，但實務上你可能常常需要對幾十萬、幾百萬筆數據分別做複雜的運算，這時候了解執行進度就是一件非常重要的事情。首先透過 Anaconda 安裝 tqdm： conda install -c conda-forge tqdm 如果你是在 Juypter 筆記本裡執行 pandas，可以用以下的方式將 tqdm 進度條整合到 apply 函式： from tqdm import tqdm_notebook tqdm_notebook () . pandas () clear_output () # 只需將 `apply` 替換成 `progress_apply` df [ '存活' ] = df . Survived . progress_apply ( lambda x : '倖存' if x else '死亡' ) df . loc [: 5 , 'Survived' : '存活' ] 透過使用 progress_apply 函式，我們可以得到跟使用 apply 函式一樣的結果，附贈進度條。相信我，在你 apply 的函式很複雜且樣本數很大時，你會很感謝有進度條的存在。你可以查看 tqdm repo 了解更多使用案例： swifter ：加速你的數據處理 swifter 函式庫能以最有效率的方式執行 apply 函式。同樣可以使用 Anaconda 安裝： conda install -c conda-forge swifter 接著讓我建立一個有 100 萬筆樣本的 DataFrame ，測試 swifter 與原版 apply 函式的效能差異： import swifter df = pd . DataFrame ( pd . np . random . rand ( 1000000 , 1 ), columns = [ 'x' ]) % timeit -n 10 df['x2'] = df['x'].apply(lambda x: x**2) % timeit -n 10 df['x2'] = df['x'].swifter.apply(lambda x: x**2) 240 ms ± 5.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 5 ms ± 565 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) 在這個簡單的平方運算中，swifter 版的 apply 函式在我的 mac 上的效率是原始 apply 函式的 48 倍。而要使用 swifter 你也只需要加上 swifter 關鍵字即可，何樂而不為？ 喔別擔心，剛剛的 tqdm 進度條仍然會繼續顯示： df = pd . DataFrame ( pd . np . random . rand ( 100000 , 1 ), columns = [ 'x' ]) df [ 'x2' ] = df [ 'x' ] . swifter . apply ( lambda x : x ** 2 ) 魚與熊掌兩者皆得。 qgrid ：即時排序、篩選及編輯你的 DataFrame qgrid 是一個能讓你在 Jupyter 筆記本裡使用簡單 UI 瀏覽並操作 DataFrame 的 widget。一樣透過 Anaconda 安裝： conda install -c tim_shawver qgrid jupyter nbextension enable --py --sys-prefix widgetsnbextension 實際的使用方式也非常簡單： import qgrid qgrid . set_grid_option ( 'maxVisibleRows' , 7 ) q = qgrid . show_grid ( df_titanic ) q 您的瀏覽器不支援影片標籤，請留言通知我：S 你可以簡單瀏覽 DataFrame 內容，也能依照欄位值篩選及排序樣本，甚至編輯裡頭的值。 如果你想要取得當前狀態的 DataFrame，也能使用 qgrid 裡頭的 get_changed_df 函式： # 取得更改後的 DataFrame df_changed = q . get_changed_df () # styling ( df_changed . iloc [: 8 , :] . style . applymap ( lambda x : 'color:red' , subset = pd . IndexSlice [ 829 , 'SibSp' ]) ) 注意我剛剛在 qgrid 裡頭將 0 改成 2 ，而這個變動被反映到新的 DataFrame df_changed 裡頭。 pandas-profiling ：你的一鍵 EDA 神器 廢話不多說，先附上安裝指令： conda install -c conda-forge pandas-profiling 當碰到新的資料集時，你常會需要做 EDA 來探索並了解手上的數據。常見的基礎步驟有： 隨機抽樣 DataFrame 並用肉眼觀察樣本 了解樣本數有多少、各類型的變數有多少 了解 DataFrame 裡有多少空值 看數值欄位的分佈，欄位之間的相關係數 看分類型欄位的有多少不同的值，分佈為何 等等。這些在你熟悉本文的 pandas 技巧以後都應該跟吃飯喝水一樣，但很多時候就是例行公事。 這時你可以使用 pandas-profiling 讓它自動產生簡單報表，總結 DataFrame 裡的數據性質： import pandas_profiling df = df_titanic . copy () # 一行報表：將想觀察的 DataFrame 丟進去就完工了 pandas_profiling . ProfileReport ( df ) 您的瀏覽器不支援影片標籤，請留言通知我：S 有了自動生成的數據分析報表，你不再需要自己寫繁瑣的程式碼就能「健檢」手上的數據集。這讓你能對數據更快地展開 domain-specific 的研究與分析，大大提升效率。 結語：實際運用所學 呼！本文的 pandas 旅程到此告一段落啦！ 我想在其他地方你應該是找不到跟本文一樣囉哩八唆的 pandas 教學文章了。本文雖長，但涵蓋的都是我認為十分實用的 pandas 使用手法，希望你有從中學到些東西，並開始自己的數據處理與分析之旅。如果你之後想要複習，可以點開頁面左側的章節傳送門，測試自己還記得多少技巧。 接下來最重要的是培養你自己的 pandas 肌肉記憶 ：重複應用你在本文學到的東西，分析自己感興趣的任何數據並內化這些知識。 Now it's your show time pandas 是一個非常強大的資料處理工具，本文雖長，仍無法涵蓋所有東西。不過你現在已經學會作為一個資料科學家所需要的所有基礎 pandas 手法。除了多加練習找手感以外，我能給你的最後一個建議就是在 Jupyter 筆記本裡頭多加利用 Shift + tab 查看 pandas 函式的 documentation，很多時候你會有新的發現。 如果你有任何其他 pandas 技巧，也請不吝留言與我分享！我懂的技巧不多，而現在輪到你教我了：） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/practical-pandas-tutorial-for-aspiring-data-scientists.html","loc":"https://leemeng.tw/practical-pandas-tutorial-for-aspiring-data-scientists.html"},{"title":"進擊的 BERT：NLP 界的巨人之力與遷移學習","text":"這是一篇 BERT 科普文，帶你直觀理解並實際運用現在 NLP 領域的巨人之力。 如果你還有印象，在 自然語言處理（NLP）與深度學習入門指南 裡我使用了 LSTM 以及 Google 的語言代表模型 BERT 來分類中文假新聞。而最後因為 BERT 本身的強大，我不費吹灰之力就在 該 Kaggle 競賽 達到 85 % 的正確率，距離第一名 3 %，總排名前 30 %。 當初我是使用 TensorFlow 官方釋出的 BERT 進行 fine tuning，但使用方式並不是那麼直覺。最近適逢 PyTorch Hub 上架 BERT ，李宏毅教授的 機器學習課程 也推出了 BERT 的教學影片 ，我認為現在正是你了解並 實際運用 BERT 的最佳時機！ 這篇文章會簡單介紹 BERT 並展示如何使用 BERT 做 遷移學習（Transfer Learning） 。我在文末也會提供一些有趣的研究及應用 ，讓你可以進一步探索變化快速的 NLP 世界。 如果你完全不熟 NLP 或是壓根子沒聽過什麼是 BERT，我強力建議你之後找時間（或是現在！）觀看李宏毅教授說明 ELMo 、BERT 以及 GPT 等模型的影片，淺顯易懂： 李宏毅教授講解目前 NLP 領域的最新研究是如何讓機器讀懂文字的（我超愛這截圖） 我接下來會花點篇幅闡述 BERT 的基礎概念。如果你已經十分熟悉 BERT 而且迫不及待想要馬上將 BERT 應用到自己的 NLP 任務上面，可以直接跳到 用 BERT fine tune 下游任務 一節。 BERT：理解上下文的語言代表模型 一個簡單的 convention，等等文中會穿插使用的： 代表 representation repr. repr. 向量 指的都是一個可以用來 代表 某詞彙（在某個語境下）的多維連續向量（continuous vector）。 現在在 NLP 圈混的，應該沒有人會說自己不曉得 Transformer 的 經典論文 Attention Is All You Need 以及其知名的 自注意力機制（Self-attention mechanism） 。 BERT 全名為 B idirectional E ncoder R epresentations from T ransformers，是 Google 以無監督的方式利用大量無標註文本「煉成」的 語言代表模型 ，其架構為 Transformer 中的 Encoder。 我在 淺談神經機器翻譯 & 用 Transformer 英翻中 一文已經鉅細靡遺地解說過所有 Transformer 的相關概念，這邊就不再贅述。 BERT 其實就是 Transformer 中的 Encoder，只是有很多層 （ 圖片來源 ） BERT 是傳統語言模型的一種變形，而 語言模型（ L anguage M odel, LM） 做的事情就是在給定一些詞彙的前提下， 去估計下一個詞彙出現的機率分佈。在 讓 AI 給我們寫點金庸 裡的 LSTM 也是一個語言模型 ，只是跟 BERT 差了很多個數量級。 給定前 t 個在字典裡的詞彙，語言模型要去估計第 t + 1 個詞彙的機率分佈 P 為何會想要訓練一個 LM？因為有種種好處： 好處 1：無監督數據無限大。不像 ImageNet 還要找人標注數據，要訓練 LM 的話網路上所有文本都是你潛在的資料集（BERT 預訓練使用的數據集共有 33 億 個字，其中包含維基百科及 BooksCorpus ） 好處 2：厲害的 LM 能夠學會語法結構、解讀語義甚至 指代消解 。透過特徵擷取或是 fine-tuning 能更有效率地訓練下游任務並提升其表現 好處 3：減少處理不同 NLP 任務所需的 architecture engineering 成本 一般人很容易理解前兩點的好處，但事實上第三點的影響也十分深遠。以往為了解決不同的 NLP 任務，我們會為該任務設計一個最適合的神經網路架構並做訓練。以下是一些簡單例子： 一般會依照不同 NLP 任務的性質為其貼身打造特定的模型架構 在這篇文章裡頭我不會一一介紹上述模型的運作原理，在這邊只是想讓你了解不同的 NLP 任務通常需要不同的模型，而設計這些模型並測試其 performance 是非常耗費成本的（人力、時間、計算資源）。 如果有一個能直接處理各式 NLP 任務的通用架構該有多好？ 隨著時代演進，不少人很自然地有了這樣子的想法，而 BERT 就是其中一個將此概念付諸實踐的例子。 BERT 論文 的作者們使用 Transfomer Encoder、大量文本以及兩個預訓練目標，事先訓練好一個可以套用到多個 NLP 任務的 BERT 模型，再以此為基礎 fine tune 多個下游任務。 這就是近來 NLP 領域非常流行的 兩階段 遷移學習： 先以 LM Pretraining 的方式預先訓練出一個對自然語言有一定「理解」的通用模型 再將該模型拿來做特徵擷取或是 fine tune 下游的（監督式）任務 兩階段遷移學習在 BERT 下的應用：使用預先訓練好的 BERT 對下游任務做 fine tuning 上面這個示意圖最重要的概念是預訓練步驟跟 fine-tuning 步驟所用的 BERT 是 一模一樣 的。當你學會使用 BERT 就能用同個架構訓練多種 NLP 任務，大大減少自己設計模型的 architecture engineering 成本，投資報酬率高到爆炸。 壞消息是，天下沒有白吃的午餐。 要訓練好一個有 1.1 億參數的 12 層 BERT-BASE 得用 16 個 TPU chips 跑上整整 4 天， 花費 500 鎂 ；24 層的 BERT-LARGE 則有 3.4 億個參數，得用 64 個 TPU chips（約 7000 鎂）訓練。喔對，別忘了多次實驗得把這些成本乘上幾倍。 最近也有 NLP 研究者呼籲大家把訓練好的模型開源釋出 以減少重複訓練對環境造成的影響。 好消息是，BERT 作者們有開源釋出訓練好的模型，只要使用 TensorFlow 或是 PyTorch 將已訓練好的 BERT 載入，就能省去預訓練步驟的所有昂貴成本。好 BERT 不用嗎？ 雖然一般來說我們只需要用訓練好的 BERT 做 fine-tuning，稍微瞭解預訓練步驟的內容能讓你直觀地理解它在做些什麼。 BERT 在預訓練時需要完成的兩個任務 （ 圖片來源 ） Google 在預訓練 BERT 時讓它 同時 進行兩個任務： 克漏字填空（ 1953 年被提出的 Cloze task ，學術點的說法是 M asked L anguage M odel, MLM） 判斷第 2 個句子在原始文本中是否跟第 1 個句子相接（ N ext S entence P rediction, NSP） 對上通天文下知地理的鄉民們來說，要完成這兩個任務簡單到爆。只要稍微看一下 前後文 就能知道左邊克漏字任務的 [MASK] 裡頭該填 退了 ；而 醒醒吧 後面接 你沒有妹妹 也十分合情合理。 讓我們馬上載入 PyTorch Hub 上的 BERT 模型 體驗看看。首先我們需要安裝一些簡單的函式庫： （2019/10/07 更新：因應 HuggingFace 團隊最近將 GitHub 專案大翻新並更名成 transformers ，本文已直接 import 該 repo 並使用新的方法調用 BERT。底下的程式碼將不再使用該團隊在 PyTorch Hub 上 host 的模型。感謝網友 Hsien 提醒） %%bash pip install transformers tqdm boto3 requests regex -q 接著載入中文 BERT 使用的 tokenizer： import torch from transformers import BertTokenizer from IPython.display import clear_output PRETRAINED_MODEL_NAME = \"bert-base-chinese\" # 指定繁簡中文 BERT-BASE 預訓練模型 # 取得此預訓練模型所使用的 tokenizer tokenizer = BertTokenizer . from_pretrained ( PRETRAINED_MODEL_NAME ) clear_output () print ( \"PyTorch 版本：\" , torch . __version__ ) PyTorch 版本： 1.4.0 為了讓你直觀了解 BERT 運作，本文使用包含繁體與簡體中文的預訓練模型。 你可以在 Hugging Face 團隊的 repo 裡看到所有可從 PyTorch Hub 載入的 BERT 預訓練模型。截至目前為止有以下模型可供使用： bert-base-chinese bert-base-uncased bert-base-cased bert-base-german-cased bert-base-multilingual-uncased bert-base-multilingual-cased bert-large-cased bert-large-uncased bert-large-uncased-whole-word-masking bert-large-cased-whole-word-masking 這些模型的參數都已經被訓練完成，而主要差別在於： 預訓練步驟時用的文本語言 有無分大小寫 層數的不同 預訓練時遮住 wordpieces 或是整個 word 除了本文使用的中文 BERT 以外，常被拿來應用與研究的是英文的 bert-base-cased 模型。 現在讓我們看看 tokenizer 裡頭的字典資訊： vocab = tokenizer . vocab print ( \"字典大小：\" , len ( vocab )) 字典大小： 21128 如上所示，中文 BERT 的字典大小約有 2.1 萬個 tokens。沒記錯的話，英文 BERT 的字典則大約是 3 萬 tokens 左右。我們可以瞧瞧中文 BERT 字典裡頭紀錄的一些 tokens 以及其對應的索引： import random random_tokens = random . sample ( list ( vocab ), 10 ) random_ids = [ vocab [ t ] for t in random_tokens ] print ( \" {0:20}{1:15} \" . format ( \"token\" , \"index\" )) print ( \"-\" * 25 ) for t , id in zip ( random_tokens , random_ids ): print ( \" {0:15}{1:10} \" . format ( t , id )) token index ------------------------- ##荘 18834 ##尉 15259 詬 6278 32gb 11155 荨 5787 ##狙 17376 兹 1074 ##诈 19457 蠣 6112 gp 13228 BERT 使用當初 Google NMT 提出的 WordPiece Tokenization ，將本來的 words 拆成更小粒度的 wordpieces，有效處理 不在字典裡頭的詞彙 。中文的話大致上就像是 character-level tokenization，而有 ## 前綴的 tokens 即為 wordpieces。 以詞彙 fragment 來說，其可以被拆成 frag 與 ##ment 兩個 pieces，而一個 word 也可以獨自形成一個 wordpiece。wordpieces 可以由蒐集大量文本並找出其中常見的 pattern 取得。 另外有趣的是ㄅㄆㄇㄈ也有被收錄： indices = list ( range ( 647 , 657 )) some_pairs = [( t , idx ) for t , idx in vocab . items () if idx in indices ] for pair in some_pairs : print ( pair ) ('ㄅ', 647) ('ㄆ', 648) ('ㄇ', 649) ('ㄉ', 650) ('ㄋ', 651) ('ㄌ', 652) ('ㄍ', 653) ('ㄎ', 654) ('ㄏ', 655) ('ㄒ', 656) 讓我們利用中文 BERT 的 tokenizer 將一個中文句子斷詞看看： text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\" tokens = tokenizer . tokenize ( text ) ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( text ) print ( tokens [: 10 ], '...' ) print ( ids [: 10 ], '...' ) [CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。 ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ... [101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... 除了一般的 wordpieces 以外，BERT 裡頭有 5 個特殊 tokens 各司其職： [CLS] ：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr. [SEP] ：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔 [UNK] ：沒出現在 BERT 字典裡頭的字會被這個 token 取代 [PAD] ：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算 [MASK] ：未知遮罩，僅在預訓練階段會用到 如上例所示， [CLS] 一般會被放在輸入序列的最前面，而 zero padding 在之前的 Transformer 文章裡已經有非常詳細的介紹 。 [MASK] token 一般在 fine-tuning 或是 feature extraction 時不會用到，這邊只是為了展示預訓練階段的克漏字任務才使用的。 現在馬上讓我們看看給定上面有 [MASK] 的句子，BERT 會填入什麼字： \"\"\" 這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測 \"\"\" from transformers import BertForMaskedLM # 除了 tokens 以外我們還需要辨別句子的 segment ids tokens_tensor = torch . tensor ([ ids ]) # (1, seq_len) segments_tensors = torch . zeros_like ( tokens_tensor ) # (1, seq_len) maskedLM_model = BertForMaskedLM . from_pretrained ( PRETRAINED_MODEL_NAME ) clear_output () # 使用 masked LM 估計 [MASK] 位置所代表的實際 token maskedLM_model . eval () with torch . no_grad (): outputs = maskedLM_model ( tokens_tensor , segments_tensors ) predictions = outputs [ 0 ] # (1, seq_len, num_hidden_units) del maskedLM_model # 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來 masked_index = 5 k = 3 probs , indices = torch . topk ( torch . softmax ( predictions [ 0 , masked_index ], - 1 ), k ) predicted_tokens = tokenizer . convert_ids_to_tokens ( indices . tolist ()) # 顯示 top k 可能的字。一般我們就是取 top 1 當作預測值 print ( \"輸入 tokens ：\" , tokens [: 10 ], '...' ) print ( '-' * 50 ) for i , ( t , p ) in enumerate ( zip ( predicted_tokens , probs ), 1 ): tokens [ masked_index ] = t print ( \"Top {} ( {:2} %)： {} \" . format ( i , int ( p . item () * 100 ), tokens [: 10 ]), '...' ) 輸入 tokens ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ... -------------------------------------------------- Top 1 (82%)：['[CLS]', '等', '到', '潮', '水', '來', '了', '，', '就', '知'] ... Top 2 (11%)：['[CLS]', '等', '到', '潮', '水', '濕', '了', '，', '就', '知'] ... Top 3 ( 2%)：['[CLS]', '等', '到', '潮', '水', '過', '了', '，', '就', '知'] ... Google 在訓練中文 BERT 鐵定沒看 批踢踢 ，還無法預測出我們最想要的那個 退 字。而最接近的 過 的出現機率只有 2%，但我會說以語言代表模型以及自然語言理解的角度來看這結果已經不差了。BERT 透過關注 潮 與 水 這兩個字，從 2 萬多個 wordpieces 的可能性中選出 來 作為這個情境下 [MASK] token 的預測值 ，也還算說的過去。 這是 BertViz 視覺化 BERT 注意力的結果，我等等會列出安裝步驟讓你自己玩玩。值得一提的是，以上是第 8 層 Encoder block 中 Multi-head attention 裡頭某一個 head 的自注意力結果。並不是每個 head 都會關注在一樣的位置。透過 multi-head 自注意力機制，BERT 可以讓不同 heads 在不同的 representation subspaces 裡學會關注不同位置的不同 repr.。 學會填克漏字讓 BERT 更好地 model 每個詞彙在不同語境下該有的 repr.，而 NSP 任務則能幫助 BERT model 兩個句子之間的關係，這在 問答系統 QA 、 自然語言推論 NLI 或是後面我們會看到的 假新聞分類任務 都很有幫助。 這樣的 word repr. 就是近年十分盛行的 contextual word representation 概念。跟以往沒有蘊含上下文資訊的 Word2Vec、GloVe 等無語境的詞嵌入向量有很大的差異。用稍微學術一點的說法就是： Contextual word repr. 讓同 word type 的 word token 在不同語境下有不同的表示方式；而傳統的詞向量無論上下文，都會讓同 type 的 word token 的 repr. 相同。 直覺上 contextual word representation 比較能反映人類語言的真實情況，畢竟同個詞彙的含義在不同情境下相異是再正常不過的事情。在不同語境下給同個詞彙相同的 word repr. 這件事情在近年的 NLP 領域裡頭顯得越來越不合理。 為了讓你加深印象，讓我再舉個具體的例子： 情境 1： 胖虎叫大雄去買漫畫，回來慢了就打他。 情境 2： 妹妹說胖虎是「胖子」，他聽了很不開心。 很明顯地，在這兩個情境裡頭「他」所代表的語義以及指稱的對象皆不同。如果仍使用沒蘊含上下文 / 語境資訊的詞向量，機器就會很難正確地「解讀」這兩個句子所蘊含的語義了。 現在讓我們跟隨 這個 Colab 筆記本 安裝 BERT 的視覺化工具 BertViz ，看看 BERT 會怎麼處理這兩個情境： # 安裝 BertViz import sys ! test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo if not 'bertviz_repo' in sys . path : sys . path += [ 'bertviz_repo' ] # import packages from transformers import BertTokenizer , BertModel from bertviz import head_view # 在 jupyter notebook 裡頭顯示 visualzation 的 helper def call_html (): import IPython display ( IPython . core . display . HTML ( ''' <script src=\"/static/components/requirejs/require.js\"></script> <script> requirejs.config({ paths: { base: '/static/base', \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\", jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min', }, }); </script> ''' )) clear_output () Setup 以後就能非常輕鬆地將 BERT 內部的注意力機制視覺化出來： # 記得我們是使用中文 BERT model_version = 'bert-base-chinese' model = BertModel . from_pretrained ( model_version , output_attentions = True ) tokenizer = BertTokenizer . from_pretrained ( model_version ) # 情境 1 的句子 sentence_a = \"胖虎叫大雄去買漫畫，\" sentence_b = \"回來慢了就打他。\" # 得到 tokens 後丟入 BERT 取得 attention inputs = tokenizer . encode_plus ( sentence_a , sentence_b , return_tensors = 'pt' , add_special_tokens = True ) token_type_ids = inputs [ 'token_type_ids' ] input_ids = inputs [ 'input_ids' ] attention = model ( input_ids , token_type_ids = token_type_ids )[ - 1 ] input_id_list = input_ids [ 0 ] . tolist () # Batch index 0 tokens = tokenizer . convert_ids_to_tokens ( input_id_list ) call_html () # 交給 BertViz 視覺化 head_view ( attention , tokens ) # 注意：執行這段程式碼以後只會顯示下圖左側的結果。 # 為了方便你比較，我把情境 2 的結果也同時附上 這是 BERT 裡第 9 層 Encoder block 其中一個 head 的注意力結果。 圖中的線條代表該 head 在更新「他」（左側）的 repr. 時關注其他詞彙（右側）的注意力程度。越粗代表關注權重（attention weights）越高。很明顯地這個 head 具有一定的 指代消解（Coreference Resolution） 能力，能正確地關注「他」所指代的對象。 要處理指代消解需要對自然語言有不少理解，而 BERT 在沒有標注數據的情況下透過自注意力機制、深度雙向語言模型以及「閱讀」大量文本達到這樣的水準，是一件令人雀躍的事情。 當然 BERT 並不是第一個嘗試產生 contextual word repr. 的語言模型。在它之前最知名的例子有剛剛提到的 ELMo 以及 GPT ： ELMo、GPT 以及 BERT 都透過訓練語言模型來獲得 contextual word representation ELMo 利用獨立訓練的雙向兩層 LSTM 做語言模型並將中間得到的隱狀態向量串接當作每個詞彙的 contextual word repr.；GPT 則是使用 Transformer 的 Decoder 來訓練一個中規中矩，從左到右的 單向 語言模型。你可以參考我另一篇文章： 直觀理解 GPT-2 語言模型並生成金庸武俠小說 來深入了解 GPT 與 GPT-2。 BERT 跟它們的差異在於利用 MLM（即克漏字）的概念及 Transformer Encoder 的架構，擺脫以往語言模型只能從單個方向（由左到右或由右到左）估計下個詞彙出現機率的窘境，訓練出一個 雙向 的語言代表模型。這使得 BERT 輸出的每個 token 的 repr. Tn 都同時蘊含了前後文資訊，真正的 雙向 representation。 跟以往模型相比，BERT 能更好地處理自然語言，在著名的問答任務 SQuAD2.0 也有卓越表現： SQuAD 2.0 目前排行榜的前 5 名有 4 個有使用 BERT 我想我又犯了解說癖，這些東西你可能在看這篇文章之前就全懂了。但希望這些對 BERT 的 high level 介紹能幫助更多人直覺地理解 BERT 的強大之處以及為何值得學習它。 假如你仍然似懂非懂，只需記得： BERT 是一個強大的語言代表模型，給它一段文本序列，它能回傳一段相同長度且蘊含上下文資訊的 word repr. 序列，對下游的 NLP 任務很有幫助。 有了這樣的概念以後，我們接下來要做的事情很簡單，就是將自己感興趣的 NLP 任務的文本丟入 BERT ，為文本裡頭的每個 token 取得有語境的 word repr.，並以此 repr. 進一步 fine tune 當前任務，取得更好的結果。 用 BERT fine tune 下游任務 我們在 給所有人的 NLP 入門指南 碰過的 假新聞分類任務 將會是本文拿 BERT 來做 fine-tuning 的例子。選擇這個任務的最主要理由是因為中文數據容易理解，另外網路上針對兩個句子做分類的例子也較少。 就算你對假新聞分類沒興趣也建議繼續閱讀。因為本節談到的所有概念完全可以被套用到其他語言的文本以及不同的 NLP 任務之上。因此我希望接下來你能一邊閱讀一邊想像如何用同樣的方式把 BERT 拿來處理你自己感興趣的 NLP 任務。 給定假新聞 title1，判斷另一新聞 title2 跟 title1 的關係（同意、反對或無關） （ 圖片來源 ） fine tune BERT 來解決新的下游任務有 5 個簡單步驟： 準備原始文本數據 將原始文本轉換成 BERT 相容的輸入格式 在 BERT 之上加入新 layer 成下游任務模型 訓練該下游任務模型 對新樣本做推論 對，就是那麼直覺。而且你應該已經看出步驟 1、4 及 5 都跟訓練一般模型所需的步驟無太大差異。跟 BERT 最相關的細節事實上是步驟 2 跟 3： 如何將原始數據轉換成 BERT 相容 的輸入格式？ 如何在 BERT 之上建立 layer(s) 以符合下游任務需求？ 事不宜遲，讓我們馬上以假新聞分類任務為例回答這些問題。 我在之前的文章已經說明過 ，這個任務的輸入是兩個句子，輸出是 3 個類別機率的多類別分類任務（multi-class classification task），跟 NLP 領域裡常見的 自然語言推論（Natural Language Inference） 具有相同性質。 1. 準備原始文本數據 為了最大化再現性（reproducibility）以及幫助有興趣的讀者深入研究，我會列出所有的程式碼，你只要複製貼上就能完整重現文中所有結果並生成能提交到 Kaggle 競賽的預測檔案。你當然也可以選擇直接閱讀，不一定要下載數據。 因為 Kaggle 網站本身的限制，我無法直接提供數據載點。如果你想要跟著本文練習以 BERT fine tune 一個假新聞的分類模型，可以先 前往該 Kaggle 競賽下載資料集 。下載完數據你的資料夾裡應該會有兩個壓縮檔，分別代表訓練集和測試集： import glob glob . glob ( \"*.csv.zip\" ) ['train.csv.zip', 'test.csv.zip'] 接著就是我實際處理訓練資料集的程式碼。再次申明，你只需稍微瀏覽註解並感受一下處理邏輯即可，no pressure。 因為競賽早就結束，我們不必花費時間衝高分數。比起衝高準確度，讓我們做點有趣的事情：從 32 萬筆訓練數據裡頭隨機抽樣 1 % 來讓 BERT 學怎麼分類假新聞。 我們可以看看 BERT 本身的語言理解能力對只有少量標註數據的任務有什麼幫助： \"\"\" 前處理原始的訓練數據集。 你不需了解細節，只需要看註解了解邏輯或是輸出的數據格式即可 \"\"\" import os import pandas as pd # 解壓縮從 Kaggle 競賽下載的訓練壓縮檔案 os . system ( \"unzip train.csv.zip\" ) # 簡單的數據清理，去除空白標題的 examples df_train = pd . read_csv ( \"train.csv\" ) empty_title = (( df_train [ 'title2_zh' ] . isnull ()) \\ | ( df_train [ 'title1_zh' ] . isnull ()) \\ | ( df_train [ 'title2_zh' ] == '' ) \\ | ( df_train [ 'title2_zh' ] == '0' )) df_train = df_train [ ~ empty_title ] # 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU MAX_LENGTH = 30 df_train = df_train [ ~ ( df_train . title1_zh . apply ( lambda x : len ( x )) > MAX_LENGTH )] df_train = df_train [ ~ ( df_train . title2_zh . apply ( lambda x : len ( x )) > MAX_LENGTH )] # 只用 1% 訓練數據看看 BERT 對少量標註數據有多少幫助 SAMPLE_FRAC = 0.01 df_train = df_train . sample ( frac = SAMPLE_FRAC , random_state = 9527 ) # 去除不必要的欄位並重新命名兩標題的欄位名 df_train = df_train . reset_index () df_train = df_train . loc [:, [ 'title1_zh' , 'title2_zh' , 'label' ]] df_train . columns = [ 'text_a' , 'text_b' , 'label' ] # idempotence, 將處理結果另存成 tsv 供 PyTorch 使用 df_train . to_csv ( \"train.tsv\" , sep = \" \\t \" , index = False ) print ( \"訓練樣本數：\" , len ( df_train )) df_train . head () 事情變得更有趣了。因為我們在抽樣 1 % 的數據後還將過長的樣本去除，實際上會被拿來訓練的樣本數只有 2,657 筆，佔不到參賽時可以用的訓練數據的 1 %，是非常少量的數據。 我們也可以看到 unrelated 的樣本佔了 68 %，因此我們用 BERT 訓練出來的分類器最少最少要超過多數決的 68 % baseline 才行： df_train . label . value_counts () / len ( df_train ) unrelated 0.679338 agreed 0.294317 disagreed 0.026346 Name: label, dtype: float64 接著我也對最後要預測的測試集做些非常基本的前處理，方便之後提交符合競賽要求的格式。你也不需了解所有細節，只要知道我們最後要預測 8 萬筆樣本： os . system ( \"unzip test.csv.zip\" ) df_test = pd . read_csv ( \"test.csv\" ) df_test = df_test . loc [:, [ \"title1_zh\" , \"title2_zh\" , \"id\" ]] df_test . columns = [ \"text_a\" , \"text_b\" , \"Id\" ] df_test . to_csv ( \"test.tsv\" , sep = \" \\t \" , index = False ) print ( \"預測樣本數：\" , len ( df_test )) df_test . head () ratio = len ( df_test ) / len ( df_train ) print ( \"測試集樣本數 / 訓練集樣本數 = {:.1f} 倍\" . format ( ratio )) 測試集樣本數 / 訓練集樣本數 = 30.2 倍 因為測試集的樣本數是我們迷你訓練集的 30 倍之多，後面你會看到反而是推論需要花費比較久的時間，模型本身一下就訓練完了。 2. 將原始文本轉換成 BERT 相容的輸入格式 處理完原始數據以後，最關鍵的就是了解如何讓 BERT 讀取這些數據以做訓練和推論。這時候我們需要了解 BERT 的輸入編碼格式。 這步驟是本文的精華所在，你將看到在其他只單純說明 BERT 概念的文章不會提及的所有實務細節。以下是 原論文 裡頭展示的成對句子編碼示意圖： 加入 PyTorch 使用細節的 BERT 成對句子編碼示意圖 第二條分隔線 之上 的內容是論文裡展示的例子。圖中的每個 Token Embedding 都對應到前面提過的一個 wordpiece，而 Segment Embeddings 則代表不同句子的位置，是學出來的。Positional Embeddings 則跟其他 Transformer 架構中出現的位置編碼同出一轍。 實際運用 PyTorch 的 BERT 時最重要的則是在第二條分隔線 之下 的資訊。我們需要將原始文本轉換成 3 種 id tensors ： tokens_tensor ：代表識別每個 token 的索引值，用 tokenizer 轉換即可 segments_tensor ：用來識別句子界限。第一句為 0，第二句則為 1。另外注意句子間的 [SEP] 為 0 masks_tensor ：用來界定自注意力機制範圍。1 讓 BERT 關注該位置，0 則代表是 padding 不需關注 論文裡的例子並沒有說明 [PAD] token，但實務上每個 batch 裡頭的輸入序列長短不一，為了讓 GPU 平行運算我們需要將 batch 裡的每個輸入序列都補上 zero padding 以保證它們長度一致。另外 masks_tensor 以及 segments_tensor 在 [PAD] 對應位置的值也都是 0，切記切記。 有了這些背景知識以後，要實作一個 Dataset 並將原始文本轉換成 BERT 相容的格式就變得十分容易了： \"\"\" 實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。 此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors： - tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP] - segments_tensor：可以用來識別兩個句子界限的 binary tensor - label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None \"\"\" from torch.utils.data import Dataset class FakeNewsDataset ( Dataset ): # 讀取前處理後的 tsv 檔並初始化一些參數 def __init__ ( self , mode , tokenizer ): assert mode in [ \"train\" , \"test\" ] # 一般訓練你會需要 dev set self . mode = mode # 大數據你會需要用 iterator=True self . df = pd . read_csv ( mode + \".tsv\" , sep = \" \\t \" ) . fillna ( \"\" ) self . len = len ( self . df ) self . label_map = { 'agreed' : 0 , 'disagreed' : 1 , 'unrelated' : 2 } self . tokenizer = tokenizer # 我們將使用 BERT tokenizer # 定義回傳一筆訓練 / 測試數據的函式 def __getitem__ ( self , idx ): if self . mode == \"test\" : text_a , text_b = self . df . iloc [ idx , : 2 ] . values label_tensor = None else : text_a , text_b , label = self . df . iloc [ idx , :] . values # 將 label 文字也轉換成索引方便轉換成 tensor label_id = self . label_map [ label ] label_tensor = torch . tensor ( label_id ) # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP] word_pieces = [ \"[CLS]\" ] tokens_a = self . tokenizer . tokenize ( text_a ) word_pieces += tokens_a + [ \"[SEP]\" ] len_a = len ( word_pieces ) # 第二個句子的 BERT tokens tokens_b = self . tokenizer . tokenize ( text_b ) word_pieces += tokens_b + [ \"[SEP]\" ] len_b = len ( word_pieces ) - len_a # 將整個 token 序列轉換成索引序列 ids = self . tokenizer . convert_tokens_to_ids ( word_pieces ) tokens_tensor = torch . tensor ( ids ) # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句 segments_tensor = torch . tensor ([ 0 ] * len_a + [ 1 ] * len_b , dtype = torch . long ) return ( tokens_tensor , segments_tensor , label_tensor ) def __len__ ( self ): return self . len # 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞 trainset = FakeNewsDataset ( \"train\" , tokenizer = tokenizer ) 這段程式碼不難，我也很想硬掰些台詞撐撐場面，但該說的重點都寫成註解給你看了。如果你想要把自己手上的文本轉換成 BERT 看得懂的東西，那徹底理解這個 Dataset 的實作邏輯就非常重要了。 現在讓我們看看第一個訓練樣本轉換前後的格式差異： # 選擇第一個樣本 sample_idx = 0 # 將原始文本拿出做比較 text_a , text_b , label = trainset . df . iloc [ sample_idx ] . values # 利用剛剛建立的 Dataset 取出轉換後的 id tensors tokens_tensor , segments_tensor , label_tensor = trainset [ sample_idx ] # 將 tokens_tensor 還原成文本 tokens = tokenizer . convert_ids_to_tokens ( tokens_tensor . tolist ()) combined_text = \"\" . join ( tokens ) # 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果 print ( f \"\"\"[原始文本] 句子 1： {text_a} 句子 2： {text_b} 分類 ： {label} -------------------- [Dataset 回傳的 tensors] tokens_tensor ： {tokens_tensor} segments_tensor： {segments_tensor} label_tensor ： {label_tensor} -------------------- [還原 tokens_tensors] {combined_text} \"\"\" ) [原始文本] 句子 1：苏有朋要结婚了，但网友觉得他还是和林心如比较合适 句子 2：好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！ 分類 ：unrelated -------------------- [Dataset 回傳的 tensors] tokens_tensor ：tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8024, 852, 5381, 1351, 6230, 2533, 800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394, 6844, 102, 1962, 7318, 6057, 5310, 2042, 5314, 679, 2042, 3184, 4638, 4912, 2269, 2803, 5709, 4413, 8024, 948, 7450, 4638, 4912, 2269, 2957, 3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]) segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) label_tensor ：2 -------------------- [還原 tokens_tensors] [CLS]苏有朋要结婚了，但网友觉得他还是和林心如比较合适[SEP]好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！[SEP] 好啦，我很雞婆地幫你把處理前後的差異都列了出來，你現在應該了解我們定義的 trainset 回傳的 tensors 跟原始文本之間的關係了吧！如果你之後想要一行行解析上面我定義的這個 Dataset ，強烈建議安裝在 Github 上已經得到超過 1 萬星的 PySnooper ： ! pip install pysnooper - q import pysnooper class FakeNewsDataset ( Dataset ): ... @pysnooper . snoop () # 加入以了解所有轉換過程 def __getitem__ ( self , idx ): ... 加上 @pysnooper.snoop() 、重新定義 FakeNewsDataset 、初始化一個新的 trainset 並將第一個樣本取出即可看到這樣的 logging 訊息： 使用 PySnooper 讓你輕鬆了解怎麼將原始文本變得「 BERT 相容」 有了 Dataset 以後，我們還需要一個 DataLoader 來回傳成一個個的 mini-batch。畢竟我們不可能一次把整個數據集塞入 GPU，對吧？ 痾 ... 你剛剛應該沒有打算這麼做吧？ 除了上面的 FakeNewsDataset 實作以外，以下的程式碼是你在想將 BERT 應用到自己的 NLP 任務時會需要徹底搞懂的部分： \"\"\" 實作可以一次回傳一個 mini-batch 的 DataLoader 這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`， 回傳訓練 BERT 時會需要的 4 個 tensors： - tokens_tensors : (batch_size, max_seq_len_in_batch) - segments_tensors: (batch_size, max_seq_len_in_batch) - masks_tensors : (batch_size, max_seq_len_in_batch) - label_ids : (batch_size) \"\"\" from torch.utils.data import DataLoader from torch.nn.utils.rnn import pad_sequence # 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是 # 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors： # - tokens_tensor # - segments_tensor # - label_tensor # 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors def create_mini_batch ( samples ): tokens_tensors = [ s [ 0 ] for s in samples ] segments_tensors = [ s [ 1 ] for s in samples ] # 測試集有 labels if samples [ 0 ][ 2 ] is not None : label_ids = torch . stack ([ s [ 2 ] for s in samples ]) else : label_ids = None # zero pad 到同一序列長度 tokens_tensors = pad_sequence ( tokens_tensors , batch_first = True ) segments_tensors = pad_sequence ( segments_tensors , batch_first = True ) # attention masks，將 tokens_tensors 裡頭不為 zero padding # 的位置設為 1 讓 BERT 只關注這些位置的 tokens masks_tensors = torch . zeros ( tokens_tensors . shape , dtype = torch . long ) masks_tensors = masks_tensors . masked_fill ( tokens_tensors != 0 , 1 ) return tokens_tensors , segments_tensors , masks_tensors , label_ids # 初始化一個每次回傳 64 個訓練樣本的 DataLoader # 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵 BATCH_SIZE = 64 trainloader = DataLoader ( trainset , batch_size = BATCH_SIZE , collate_fn = create_mini_batch ) 加上註解，我相信這應該是你在整個網路上能看到最平易近人的實作了。這段程式碼是你要實際將 mini-batch 丟入 BERT 做訓練以及預測的關鍵，務必搞清楚每一行在做些什麼。 有了可以回傳 mini-batch 的 DataLoader 後，讓我們馬上拿出一個 batch 看看： data = next ( iter ( trainloader )) tokens_tensors , segments_tensors , \\ masks_tensors , label_ids = data print ( f \"\"\" tokens_tensors.shape = {tokens_tensors.shape} {tokens_tensors} ------------------------ segments_tensors.shape = {segments_tensors.shape} {segments_tensors} ------------------------ masks_tensors.shape = {masks_tensors.shape} {masks_tensors} ------------------------ label_ids.shape = {label_ids.shape} {label_ids} \"\"\" ) tokens_tensors.shape = torch.Size([64, 63]) tensor([[ 101, 5722, 3300, ..., 0, 0, 0], [ 101, 4255, 3160, ..., 8013, 102, 0], [ 101, 711, 2506, ..., 8013, 102, 0], ..., [ 101, 671, 2157, ..., 0, 0, 0], [ 101, 1380, 677, ..., 0, 0, 0], [ 101, 2458, 1853, ..., 0, 0, 0]]) ------------------------ segments_tensors.shape = torch.Size([64, 63]) tensor([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 1, 1, 0], [0, 0, 0, ..., 1, 1, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) ------------------------ masks_tensors.shape = torch.Size([64, 63]) tensor([[1, 1, 1, ..., 0, 0, 0], [1, 1, 1, ..., 1, 1, 0], [1, 1, 1, ..., 1, 1, 0], ..., [1, 1, 1, ..., 0, 0, 0], [1, 1, 1, ..., 0, 0, 0], [1, 1, 1, ..., 0, 0, 0]]) ------------------------ label_ids.shape = torch.Size([64]) tensor([2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0]) 建立 BERT 用的 mini-batch 時最需要注意的就是 zero padding 的存在了。你可以發現除了 lable_ids 以外，其他 3 個 tensors 的每個樣本的最後大都為 0，這是因為每個樣本的 tokens 序列基本上長度都會不同，需要補 padding。 到此為止我們已經成功地將原始文本轉換成 BERT 相容的輸入格式了。這節是本篇文章最重要，也最需要花點時間咀嚼的內容。在有這些 tensors 的前提下，要在 BERT 之上訓練我們自己的下游任務完全是一塊蛋糕。 3. 在 BERT 之上加入新 layer 成下游任務模型 我從 李宏毅教授講解 BERT 的投影片 中擷取出 原論文 提到的 4 種 fine-tuning BERT 情境，並整合了一些有用資訊： 在 4 種 NLP 任務上 fine-tuning BERT 的例子 （ 圖片來源 ） 資訊量不少，但我假設你在 前面教授的 BERT 影片 或是其他地方已經看過類似的圖。 首先，我們前面一直提到的 fine-tuning BERT 指的是在 預訓練完 的 BERT 之上加入新的線性分類器（Linear Classifier），並利用下游任務的目標函式 從頭 訓練分類器並 微調 BERT 的參數。這樣做的目的是讓整個模型（BERT + Linear Classifier）能一起最大化當前下游任務的目標。 圖中紅色小字則是該任務類型常被拿來比較的資料集，比方說 MNLI 及 SQuAD v1.1 。 不過現在對我們來說最重要的是圖中的藍色字體。多虧了 HuggingFace 團隊 ，要用 PyTorch fine-tuing BERT 是件非常容易的事情。每個藍色字體都對應到一個可以處理下游任務的 模型 ，而這邊說的模型指的是 已訓練的 BERT + Linear Classifier 。 按圖索驥，因為假新聞分類是一個成對句子分類任務，自然就對應到上圖的左下角。 FINETUNE_TASK 則為 bertForSequenceClassification： # 載入一個可以做中文多分類任務的模型，n_class = 3 from transformers import BertForSequenceClassification PRETRAINED_MODEL_NAME = \"bert-base-chinese\" NUM_LABELS = 3 model = BertForSequenceClassification . from_pretrained ( PRETRAINED_MODEL_NAME , num_labels = NUM_LABELS ) clear_output () # high-level 顯示此模型裡的 modules print ( \"\"\" name module ----------------------\"\"\" ) for name , module in model . named_children (): if name == \"bert\" : for n , _ in module . named_children (): print ( f \" {name} : {n} \" ) else : print ( \" {:15} {} \" . format ( name , module )) name module ---------------------- bert:embeddings bert:encoder bert:pooler dropout Dropout(p=0.1, inplace=False) classifier Linear(in_features=768, out_features=3, bias=True) 沒錯，一行程式碼就初始化了一個可以用 BERT 做文本多分類的模型 model 。我也列出了 model 裡頭最 high level 的模組，資料流則從上到下，通過： BERT 處理各種 embeddings 的模組 在 神經機器翻譯 就已經看過的 Transformer Encoder 一個 pool [CLS] token 在所有層的 repr. 的 BertPooler Dropout 層 回傳 3 個類別 logits 的線性分類器 classifier 而 classifer 就只是將從 BERT 那邊拿到的 [CLS] token 的 repr. 做一個線性轉換而已，非常簡單。我也將我們實際使用的分類模型 BertForSequenceClassification 實作簡化一下供你參考： class BertForSequenceClassification ( BertPreTrainedModel ): def __init__ ( self , config , num_labels = 2 , ... ): super ( BertForSequenceClassification , self ) . __init__ ( config ) self . num_labels = num_labels self . bert = BertModel ( config , ... ) # 載入預訓練 BERT self . dropout = nn . Dropout ( config . hidden_dropout_prob ) # 簡單 linear 層 self . classifier = nn . Linear ( config . hidden_size , num_labels ) ... def forward ( self , input_ids , token_type_ids = None , attention_mask = None , labels = None , ... ): # BERT 輸入就是 tokens, segments, masks outputs = self . bert ( input_ids , token_type_ids , attention_mask , ... ) ... pooled_output = self . dropout ( pooled_output ) # 線性分類器將 dropout 後的 BERT repr. 轉成類別 logits logits = self . classifier ( pooled_output ) # 輸入有 labels 的話直接計算 Cross Entropy 回傳，方便！ if labels is not None : loss_fct = CrossEntropyLoss () loss = loss_fct ( logits . view ( - 1 , self . num_labels ), labels . view ( - 1 )) return loss # 有要求回傳注意矩陣的話回傳 elif self . output_attentions : return all_attentions , logits # 回傳各類別的 logits return logits 這樣應該清楚多了吧！我們的分類模型 model 也就只是在 BERT 之上加入 dropout 以及簡單的 linear classifier，最後輸出用來預測類別的 logits。 這就是兩階段遷移學習強大的地方：你不用再自己依照不同 NLP 任務從零設計非常複雜的模型，只需要站在巨人肩膀上，然後再做一點點事情就好了。 你也可以看到整個分類模型 model 預設的隱狀態維度為 768。如果你想要更改 BERT 的超參數，可以透過給一個 config dict 來設定。以下則是分類模型 model 預設的參數設定： model . config BertConfig { \"architectures\": [ \"BertForMaskedLM\" ], \"attention_probs_dropout_prob\": 0.1, \"bos_token_id\": 0, \"directionality\": \"bidi\", \"do_sample\": false, \"eos_token_ids\": 0, \"finetuning_task\": null, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"id2label\": { \"0\": \"LABEL_0\", \"1\": \"LABEL_1\" }, \"initializer_range\": 0.02, \"intermediate_size\": 3072, \"is_decoder\": false, \"label2id\": { \"LABEL_0\": 0, \"LABEL_1\": 1 }, \"layer_norm_eps\": 1e-12, \"length_penalty\": 1.0, \"max_length\": 20, \"max_position_embeddings\": 512, \"model_type\": \"bert\", \"num_attention_heads\": 12, \"num_beams\": 1, \"num_hidden_layers\": 12, \"num_labels\": 3, \"num_return_sequences\": 1, \"output_attentions\": false, \"output_hidden_states\": false, \"output_past\": true, \"pad_token_id\": 0, \"pooler_fc_size\": 768, \"pooler_num_attention_heads\": 12, \"pooler_num_fc_layers\": 3, \"pooler_size_per_head\": 128, \"pooler_type\": \"first_token_transform\", \"pruned_heads\": {}, \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_k\": 50, \"top_p\": 1.0, \"torchscript\": false, \"type_vocab_size\": 2, \"use_bfloat16\": false, \"vocab_size\": 21128 } Dropout、LayerNorm、全連接層數以及 mutli-head attentions 的 num_attention_heads 等超參數我們也都已經在之前的 Transformer 文章看過了，這邊就不再贅述。 目前 PyTorch Hub 上有 8 種模型以及一個 tokenizer 可供使用，依照用途可以分為： 基本款： bertModel bertTokenizer 預訓練階段 bertForMaskedLM bertForNextSentencePrediction bertForPreTraining Fine-tuning 階段 bertForSequenceClassification bertForTokenClassification bertForQuestionAnswering bertForMultipleChoice 粗體是本文用到的模型。如果你想要完全 DIY 自己的模型，可以載入純 bertModel 並參考上面看到的 BertForSequenceClassification 的實作。當然建議盡量不要重造輪子。如果只是想要了解其背後實作邏輯，可以參考 pytorch-transformers 。 有了 model 以及我們在前一節建立的 trainloader ，讓我們寫一個簡單函式測試現在 model 在訓練集上的分類準確率： \"\"\" 定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式 之後也可以用來生成上傳到 Kaggle 競賽的預測結果 2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors 丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace 更新 repo 程式碼並改變參數順序時影響到我們的結果。 \"\"\" def get_predictions ( model , dataloader , compute_acc = False ): predictions = None correct = 0 total = 0 with torch . no_grad (): # 遍巡整個資料集 for data in dataloader : # 將所有 tensors 移到 GPU 上 if next ( model . parameters ()) . is_cuda : data = [ t . to ( \"cuda:0\" ) for t in data if t is not None ] # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱 tokens_tensors , segments_tensors , masks_tensors = data [: 3 ] outputs = model ( input_ids = tokens_tensors , token_type_ids = segments_tensors , attention_mask = masks_tensors ) logits = outputs [ 0 ] _ , pred = torch . max ( logits . data , 1 ) # 用來計算訓練集的分類準確率 if compute_acc : labels = data [ 3 ] total += labels . size ( 0 ) correct += ( pred == labels ) . sum () . item () # 將當前 batch 記錄下來 if predictions is None : predictions = pred else : predictions = torch . cat (( predictions , pred )) if compute_acc : acc = correct / total return predictions , acc return predictions # 讓模型跑在 GPU 上並取得訓練集的分類準確率 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( \"device:\" , device ) model = model . to ( device ) _ , acc = get_predictions ( model , trainloader , compute_acc = True ) print ( \"classification acc:\" , acc ) device: cuda:0 classification acc: 0.03387278885961611 毫不意外，模型裡新加的線性分類器才剛剛被初始化，整個分類模型的表現低於 68 % 的 baseline 是非常正常的。因為模型是隨機初始化的，你的執行結果可能跟我有點差距，但應該不會超過 68 %。 另外我們也可以算算整個分類模型以及裡頭的簡單分類器有多少參數： def get_learnable_params ( module ): return [ p for p in module . parameters () if p . requires_grad ] model_params = get_learnable_params ( model ) clf_params = get_learnable_params ( model . classifier ) print ( f \"\"\" 整個分類模型的參數量：{sum(p.numel() for p in model_params)} 線性分類器的參數量：{sum(p.numel() for p in clf_params)} \"\"\" ) 整個分類模型的參數量：102269955 線性分類器的參數量：2307 新增的 classifier 的參數量在 BERT 面前可說是滄海一粟。而因為分類模型大多數的參數都是從已訓練的 BERT 來的，實際上我們需要從頭訓練的參數量非常之少，這也是遷移學習的好處。 當然，一次 forward 所需的時間也不少就是了。 4. 訓練該下游任務模型 接下來沒有什麼新玩意了，除了需要記得我們前面定義的 batch 數據格式以外，訓練分類模型 model 就跟一般你使用 PyTorch 訓練模型做的事情相同。 為了避免失焦，訓練程式碼我只保留核心部分： %%time # 訓練模式 model . train () # 使用 Adam Optim 更新整個分類模型的參數 optimizer = torch . optim . Adam ( model . parameters (), lr = 1e-5 ) EPOCHS = 6 # 幸運數字 for epoch in range ( EPOCHS ): running_loss = 0.0 for data in trainloader : tokens_tensors , segments_tensors , \\ masks_tensors , labels = [ t . to ( device ) for t in data ] # 將參數梯度歸零 optimizer . zero_grad () # forward pass outputs = model ( input_ids = tokens_tensors , token_type_ids = segments_tensors , attention_mask = masks_tensors , labels = labels ) loss = outputs [ 0 ] # backward loss . backward () optimizer . step () # 紀錄當前 batch loss running_loss += loss . item () # 計算分類準確率 _ , acc = get_predictions ( model , trainloader , compute_acc = True ) print ( '[epoch %d ] loss: %.3f , acc: %.3f ' % ( epoch + 1 , running_loss , acc )) [epoch 1] loss: 32.120, acc: 0.803 [epoch 2] loss: 19.275, acc: 0.845 [epoch 3] loss: 14.135, acc: 0.903 [epoch 4] loss: 10.738, acc: 0.868 [epoch 5] loss: 8.326, acc: 0.905 [epoch 6] loss: 8.947, acc: 0.930 CPU times: user 1min 41s, sys: 46 s, total: 2min 27s Wall time: 2min 27s 哇嗚！我們成功地 Fine-tune BERT 了！ 儘管擁有 1 億參數的分類模型十分巨大，多虧了小訓練集的助攻（？），幾個 epochs 的訓練過程大概在幾分鐘內就結束了。從準確率看得出我們的分類模型在非常小量的訓練集的表現已經十分不錯，接著讓我們看看這個模型在真實世界，也就是 Kaggle 競賽上的測試集能得到怎麼樣的成績。 5. 對新樣本做推論 這邊我們要做的事情很單純，就只是用訓練過後的分類模型 model 為測試集裡的每個樣本產生預測分類。執行完以下程式碼，我們就能得到一個能直接繳交到 Kaggle 競賽的 csv 檔案： %%time # 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大 testset = FakeNewsDataset ( \"test\" , tokenizer = tokenizer ) testloader = DataLoader ( testset , batch_size = 256 , collate_fn = create_mini_batch ) # 用分類模型預測測試集 predictions = get_predictions ( model , testloader ) # 用來將預測的 label id 轉回 label 文字 index_map = { v : k for k , v in testset . label_map . items ()} # 生成 Kaggle 繳交檔案 df = pd . DataFrame ({ \"Category\" : predictions . tolist ()}) df [ 'Category' ] = df . Category . apply ( lambda x : index_map [ x ]) df_pred = pd . concat ([ testset . df . loc [:, [ \"Id\" ]], df . loc [:, 'Category' ]], axis = 1 ) df_pred . to_csv ( 'bert_1_prec_training_samples.csv' , index = False ) df_pred . head () CPU times: user 2min 11s, sys: 49.5 s, total: 3min Wall time: 3min 1s ! ls bert*.csv bert_1_prec_training_samples.csv 我們前面就說過測試集是訓練集的 30 倍，因此光是做推論就得花不少時間。廢話不多說，讓我將生成的預測結果上傳到 Kaggle 網站，看看會得到怎麼樣的結果： 在不到 1 % 的數據 Fine-tuing BERT 可以達到 80 % 測試準確率 測試集是訓練集的 30 倍大，overfitting 完全是可預期的。不過跟我們一開始多數決的 68 % baseline 相比，以 BERT fine tune 的分類模型在測試集達到 80 %，整整上升了 12 %。雖然這篇文章的重點一直都不在最大化這個假新聞分類任務的準確率，還是別忘了我們只用了不到原來競賽 1 % 的數據以及不到 5 分鐘的時間就達到這樣的結果。 讓我們忘了準確率，看看 BERT 本身在 fine tuning 之前與之後的差異。以下程式碼列出模型成功預測 disagreed 類別的一些例子： predictions = get_predictions ( model , trainloader ) df = pd . DataFrame ({ \"predicted\" : predictions . tolist ()}) df [ 'predicted' ] = df . predicted . apply ( lambda x : index_map [ x ]) df1 = pd . concat ([ trainset . df , df . loc [:, 'predicted' ]], axis = 1 ) disagreed_tp = (( df1 . label == 'disagreed' ) & \\ ( df1 . label == df1 . predicted ) & \\ ( df1 . text_a . apply ( lambda x : True if len ( x ) < 10 else False ))) df1 [ disagreed_tp ] . head () 其實用肉眼看看這些例子，以你對自然語言的理解應該能猜出要能正確判斷 text_b 是反對 text_a ，首先要先關注「謠」、「假」等代表反對意義的詞彙，接著再看看兩個句子間有沒有含義相反的詞彙。 讓我們從中隨意選取一個例子，看看 fine tuned 後的 BERT 能不能關注到該關注的位置。再次出動 BertViz 來視覺化 BERT 的注意權重： # 觀察訓練過後的 model 在處理假新聞分類任務時關注的位置 # 去掉 `state_dict` 即可觀看原始 BERT 結果 model_version = 'bert-base-chinese' finetuned_model = BertModel . from_pretrained ( model_version , output_attentions = True , state_dict = model . state_dict ()) # 兩個句子 sentence_a = \"烟王褚时健去世\" sentence_b = \"辟谣：一代烟王褚时健安好！\" # 得到 tokens 後丟入 BERT 取得 attention inputs = tokenizer . encode_plus ( sentence_a , sentence_b , return_tensors = 'pt' , add_special_tokens = True ) token_type_ids = inputs [ 'token_type_ids' ] input_ids = inputs [ 'input_ids' ] attention = finetuned_model ( input_ids , token_type_ids = token_type_ids )[ - 1 ] input_id_list = input_ids [ 0 ] . tolist () # Batch index 0 tokens = tokenizer . convert_ids_to_tokens ( input_id_list ) call_html () head_view ( attention , tokens ) # 這段程式碼會顯示下圖中右邊的結果 我們說過在 BERT 裡頭，第一個 [CLS] 的 repr. 代表著整個輸入序列的 repr.。 左邊是一般預訓練完的 BERT。如果你還記得 BERT 的其中一個預訓練任務 NSP 的話，就會了解這時的 [CLS] 所包含的資訊大多是要用來預測第二句本來是否接在第一句後面。以第 8 層 Encoder block 而言，你會發現大多數的 heads 在更新 [CLS] 時只關注兩句間的 [SEP] 。 有趣的是在看過一些假新聞分類數據以後（右圖），這層的一些 heads 在更新 [CLS] 的 repr. 時會開始關注跟下游任務目標相關的特定詞彙： 闢謠 去世 安好 在 fine tune 一陣子之後， 這層 Encoder block 學會關注兩句之間「衝突」的位置，並將這些資訊更新到 [CLS] 裡頭。有了這些資訊，之後的 Linear Classifier 可以將其轉換成更好的分類預測。考慮到我們只給 BERT 看不到 1 % 的數據，這樣的結果不差。如果有時間 fine tune 整個訓練集，我們能得到更好的成果。 好啦，到此為止你應該已經能直觀地理解 BERT 並開始 fine tuning 自己的下游任務了。如果你要做的是如 SQuAD 問答 等常見的任務，甚至可以用 transformers 準備好的 Python 腳本一鍵完成訓練與推論： # 腳本模式的好處是可以透過改變參數快速進行各種實驗。 # 壞處是黑盒子效應，不過對閱讀完本文的你應該不是個問題。 # 選擇適合自己的方式 fine-tuning BERT 吧！ export SQUAD_DIR = /path/to/SQUAD python run_squad.py \\ --bert_model bert-base-uncased \\ --do_train \\ --do_predict \\ --do_lower_case \\ --train_file $SQUAD_DIR /train-v1.1.json \\ --predict_file $SQUAD_DIR /dev-v1.1.json \\ --train_batch_size 12 \\ --learning_rate 3e-5 \\ --num_train_epochs 2 .0 \\ --max_seq_length 384 \\ --doc_stride 128 \\ --output_dir /tmp/debug_squad/ 用腳本的好處是你不需要知道所有實作細節，只要調整自己感興趣的參數就好。我在 用 CartoonGAN 與 TensorFlow 2 生成新海誠動畫 一文也採用同樣方式，提供讀者一鍵生成卡通圖片的 Python 腳本。 當然，你也可以先試著一步步執行本文列出的程式碼，複習並鞏固學到的東西。最後，讓我們做點簡單的總結。 結語 一路過來，你現在應該已經能夠： 直觀理解 BERT 內部自注意力機制的物理意義 向其他人清楚解釋何謂 BERT 以及其運作的原理 了解 contextual word repr. 及兩階段遷移學習 將文本數據轉換成 BERT 相容的輸入格式 依據下游任務 fine tuning BERT 並進行推論 恭喜！你現在已經具備能夠進一步探索最新 NLP 研究與應用的能力了。 UniLM 用 3 種語言模型作為預訓練目標，可以 fine tune 自然語言生成任務，是值得期待的研究 （ 圖片來源 ） 我還有不少東西想跟你分享，但因為時間有限，在這邊就簡單地條列出來： BERT 的 Encoder 架構很適合做 自然語言理解 NLU 任務，但如文章摘要等 自然語言生成 NLG 的任務就不太 okay。 BertSum 則是一篇利用 BERT 做萃取式摘要並在 CNN/Dailymail 取得 SOTA 的研究，適合想要在 BERT 之上開發自己模型的人參考作法 UniLM 透過「玩弄」注意力遮罩使得其可以在預訓練階段同時訓練 3 種語言模型，讓 fine tune NLG 任務不再是夢想。如果你了解 之前 Transformer 文章 裡說明的遮罩概念，幾秒鐘就能直觀理解上面的 UniLM 架構 最近新的 NLP 王者非 XLNet 莫屬。其表現打敗 BERT 自然不需多言，但 訓練該模型所需的花費 令人不禁思考這樣的大公司遊戲是否就是我們要的未來 有些人認為 BERT 不夠通用，因為 Fine-tuning 時還要依照不同下游任務加入新的 Linear Classifier。有些人提倡使用 Multitask Learning 想辦法弄出更通用的模型，而 decaNLP 是一個知名例子。 PyTorch 的 BERT 雖然使用上十分直覺，如果沒有強大的 GPU 還是很難在實務上使用。你可以嘗試特徵擷取或是 freeze BERT。另外如果你是以個人身份進行研究，但又希望能最小化成本並加快模型訓練效率，我會推薦花點時間學會 在 Colab 上使用 TensorFlow Hub 及 TPU 訓練模型 其他的碎念留待下次吧。 當時在撰寫 進入 NLP 世界的最佳橋樑 一文時我希望能用點微薄之力搭起一座小橋，幫助更多人平順地進入 NLP 世界。作為該篇文章的延伸，這次我希望已經在 NLP 世界闖蕩的你能夠進一步掌握突破城牆的巨人之力，前往更遠的地方。 啊，我想這篇文章就是讓你變成智慧巨人的脊髓液了！我們牆外見。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html","loc":"https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"},{"title":"淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中","text":"pre { overflow-x: auto; word-wrap: break-word; } 那時，全世界的語言都一樣。人們說：『來吧，我們要建一座塔，塔頂通天，為了揚我們的名，免得我們被分散到世界各地。』耶和華說：『看哪！他們成爲一樣的人民、用同樣的語言。如今既蓋起塔來，以後就沒有他們無法完成的事情了。我們下去！在那裏變亂他們的口音，使他們的言語彼此不通。』 ─ 《創世記》第十一章 這是聖經中著名的 巴別塔 橋段，用來解釋為何當今世上有那麼多種語言。當年的上帝或許過於杞人憂天，但近年多虧了 深度學習 ， 機器翻譯 的快速發展讓人不禁覺得，或許巴別塔很快就不再只是虛幻傳說了。 您的瀏覽器不支援影片標籤，請留言通知我：S 以往被視為非常困難的中 -> 英翻譯如今在深度學習的加持下也有不錯的水準 機器翻譯的研究之所以如此重要且迷人，是因為它將有機會讓未來任何人都不受語言的限制，獲得世界上任何他或她想要的資訊與知識。 在這篇文章的前半部分，我們會先花點時間來回顧 神經機器翻譯 裡頭的一些重要概念。接著在具備這些概念以及 其他背景知識 的前提之下，利用最新的 TensorFlow 2 來實作一個可以將英文句子翻譯成中文的神經網路架構： Transformer 。 利用 Transformer 將法文句子翻譯成英文 （ 圖片來源 ） 這是一個非常簡化的示意圖。Transformer 實際上是一種基於自注意力機制的 Seq2Seq 模型 ，近年在 圖像描述 、 聊天機器人 、 語音辨識 以及機器翻譯等各大領域大發異彩。但因為其相對複雜，到現在還是有種現象： 了解 Transformer 相關技術的人已經用了好一陣子且用得很開心，不知道的人還是不知道。 當然這並不僅限於 Transformer，因為深度學習牽涉的研究範圍實在太廣了。透過這篇文章，我希望能幫助你開始了解神經機器翻譯以及 Transformer 的相關知識。 當我們完成實作並訓練出一個 Transformer 以後，除了可以英翻中以外，我們還能清楚地了解其是如何利用強大的 注意力機制 （我們在 Encoder-Decoder 模型 + 注意力機制 一節會仔細探討此概念）來做到精準且自然的翻譯。 Transformer 在將英文句子翻譯成中文時會「關注」需要注意的英文詞彙來生成對應的中文字詞 除了翻譯出來的中文正確無誤以外，從上圖你可以發現很有趣的現象。 給定左側的英文，Transformer 在生成其對應的中文翻譯時都會給每個英文詞彙不同的「注意程度」。小方格越亮則代表模型在生成某中文字時放越多的注意力在左側對應的英文詞彙上。 仔細看你會發現這個已經訓練好的 Transformer 在翻譯： 「必」、「須」時會關注「must」 「希」、「望」時會關注「hope」 「公」、「民」時會關注「citizens」 乍看之下好像稀鬆平常，但事實上我們在訓練模型時並不會告訴它這些詞彙之間的對應關係或是任何語言學的知識。我們就只是餵給它多組相同意思的中英句子，並讓它自己學會怎麼做翻譯。 好黑魔法，不學嗎？ 在英翻中的情境下，神經網路要做的事情就是讀入左側的英文句子，接著生成右側的中文句子（繁中對英文的翻譯資料集稀少，此文將以簡體為例） 訓練資料是多組相同語義的成對中英句子（當然仍需前處理） 一些你需先具備的基礎知識 我在文中會盡量言簡意賅地介紹所有你需要了解的深度學習概念，並附上相關連結供你參考。但就像在 天龍八部 或是眾多武俠小說都有提過的重要準則： 武功修習有先後順序，勿求一步登天。 儘管在 2017 年就已被提出 ，本文即將探討並實作的 Transformer 仍算是相當進階的神經網路架構。因此具備以下的基礎知識能幫助你更順利地理解本文內容： 一點點 卷積神經網路 的概念 清楚理解 循環神經網路 的運算方式 基本的 自然語言處理 知識 基本的 線性代數 如矩陣相乘運算 中研院這篇文章清楚地說明了自然語言處理在中文上的研究與應用 （圖片來源： 研之有物 ） 希望這樣的要求沒把你嚇跑，因為事實上你大約需要好幾倍的相關知識來成功實作 Transformer。 儘管在實作前你會看到一些額外要求 ，本文的前半部分還是相當平易近人的，還請放心閱讀。 當你想要深入了解某些細節的時候，可以參考這節附上的連結或是文內說明概念時附上的圖片來源。 想更深入了解文中講述的各種概念，點擊相關的「圖片來源」就對了。 前言很長，但好戲才在後頭。如果你已經準備好進入神經機器翻譯的世界的話，現在就讓我們正式開始這趟旅程吧！ 機器翻譯近代史 鑑往知來。了解一點機器翻譯的歷史以及 Transformer 是怎麼跑出來的會對實作很有幫助。 機器翻譯（ M achine T ranslation）本身的概念 最早可追溯到 17 世紀 。自從那開始，人們嘗試並研究了各式各樣的方法，寫了一大堆規則、蒐集了數以萬計的翻譯結果來嘗試自動化翻譯。隨著時代演進，我們有了： 基於規則的機器翻譯 RBMT 基於範例的機器翻譯 EBMT 統計機器翻譯 SMT 近年的神經機器翻譯 NMT 近代機器翻譯發展簡史 （ 圖片來源 ） 很多遠古時代的東西我們不會討論，而 NMT 當然是本文的重點。不過在那之前讓我們非常簡短地看一下 SMT。 統計機器翻譯：基於短語的翻譯 機器翻譯的歷史很長，但一直要到 21 世紀初期 統計機器翻譯（ S tatistical M achine T ranslation，簡稱 SMT） 技術成熟以後，機器翻譯的品質才稍微使人滿意。其中最知名的例子當屬 Google 在 2006 年發布的 SMT 翻譯系統 。 不限於 Google，當時不少最先進的 SMT 系統都採用了 基於短語的機器翻譯（Phrase-Based MT） 演算法。PBMT 最大的特色是先將來源語言（Source Language）的句子切成短語或是詞彙，接著大致上獨立地將這些詞彙翻譯成目標語言（Target Language）。 基於短語的 SMT（Phrase-Based SMT） （ 圖片來源 ） PBMT 的翻譯結果相較於早年基於規則（Rule-Based）的手法已經進步很多，但仍然需要大量的 平行語料 、對齊語料來取得較好的結果。且因為是以短語為單位在做翻譯，這些短語拼湊出來的句子仍然不夠自然。 如果你跟我一樣有用過早年的 Google 翻譯，應該還能隱約記得當年那些充斥著「機械感」的翻譯結果。 （如果你有當年 Google 翻譯結果的截圖的話歡迎提供） 神經機器翻譯：Encoder-Decoder 模型 顧名思義，神經機器翻譯 NMT 即代表使用 類神經網路（Neural Network） 來做機器翻譯。 不管是英文、法文還是中文，一個自然語言的句子基本上可以被視為一個有時間順序的序列數據（Sequence Data）。而 我們曾提過 RNN 很適合用來處理有時間關係的序列數據 。給定一個向量序列，RNN 就是回傳一個一樣長度的向量序列作為輸出。 RNN 很適合拿來處理具有時間順序的序列數據（下方的詞在丟入 RNN 前會被轉成詞向量） （ 圖片來源 ） 當我們把來源語言以及目標語言的句子都視為一個獨立的序列以後，機器翻譯事實上就是一個 序列生成（Sequence Generation） 任務：對一個輸入序列（來源語言）做些有意義的轉換與處理以後，輸出一個新的序列（目標語言）。 而在深度學習時代，我們一般會使用以 RNN 為基礎的 Encoder-Decoder 架構（又被稱作 Sequence to Sequence / Seq2Seq 模型） 來做序列生成： 您的瀏覽器不支援影片標籤，請留言通知我：S 一個以 RNN 為基礎的 Encoder-Decoder / Seq2Seq 模型將法文翻譯成英文的步驟 （ 圖片來源 ） Seq2Seq 模型裡頭 Encoder 跟 Decoder 是各自獨立的 RNN。Encoder 把輸入的句子做處理後所得到的隱狀態向量（圖中的 Hidden State#3 ）交給 Decoder 來生成目標語言。 你可以想像兩個語義相同的法英句子雖然使用的語言、語順不一樣，但因為它們有相同的語義，Encoder 在將整個 法文 句子濃縮成一個嵌入空間（Embedding Space）中的向量後，Decoder 能利用隱含在該向量中的語義資訊來重新生成具有相同意涵的 英文 句子。 這樣的模型就像是在模擬人類做翻譯的 兩個主要過程 ： （Encoder）解譯來源文字的文意 （Decoder）重新編譯該文意至目標語言 當然人類在做翻譯時有更多步驟、也會考慮更多東西，但 Seq2Seq 模型的表現已經很不錯了。 有些人閱讀到這裡可能會問： 如果我們利用 Seq2Seq 模型將多種語言的句子都轉換到某個嵌入空間裡頭，該空間會長成什麼樣子呢？是相同語言的句子靠得比較近，還是不同語言但擁有同語義的句子會靠得比較近呢？ 這是一個很好的研究問題。 而如果我們試著把這個問題圖像化，則結果可能長得像這樣： 大哉問：神經網路將句子轉換完所形成的向量空間比較靠近左邊還是右邊？ （ 圖片來源 ） 圖中的點代表不同句子，不同顏色則代表不同語言。如果結果是左邊，代表神經網路並沒有創出一個「語義」空間，而只是把不同語言都投射到該嵌入空間裡頭的不同位置，接著才在該空間裡進行不同語言之間的轉換（中轉英、英轉法 etc.）。 我們比較想要的是右邊的情況：無關語言，只要句子的語義接近，彼此的距離就相近的語義空間。 而 Google 在 2016 年的研究結果 發現，在此空間裡頭語言相異但擁有同語義的句子之間的距離 d1 ，要比同語言但不同語義的句子之間的距離 d2 要小得多（即 d1 << d2 ）。 換句話說，在此空間中同語義的句子會靠得比較近，我們實際得到的空間比較像右邊。 而如果我們將這些句子做 t-SNE ，甚至可以得到這樣的結果： 您的瀏覽器不支援影片標籤，請留言通知我：S 在 Seq2Seq 模型創造出來的「語義」空間裡頭，不同語言但同語義的句子彼此相當接近 （ 圖片來源 ） 此研究告訴我們，只要對自然語言做正確的轉換，就能將語言相異但同語義的句子都轉換成彼此距離相近的語義向量，並以此做出好的翻譯。 以下是我隨意挑選出來的一組句子，它們在該空間裡的距離相近： 英文： From low-cost pharmacy brand moisturizers to high-priced cosmetics brand moisturizers, competition is fierce. 日文： 低価格の薬品ブランドの保湿剤から高価な百貨店の化粧品ブランドのためには, 競争が激しい 韓文： 싸구려백화점화장품브랜드 moisturizers 에 저렴한약국브랜드 moisturizers 에서 , 경쟁이큰있습니다 這些句子都代表著類似的意思：「從低價的保濕劑到高價的化妝品牌，競爭都十分激烈」。 如果你想進一步了解這個視覺化結果，可以閱讀 Google Brain 的詳細解說 或是上 Embedding Projector 自己試看看。 另外值得注意的是，機器翻譯本身是一種 有條件的序列生成任務（Conditional Sequence Generation） ：給定一個特定的輸入句子（文字序列），依此條件輸出另外一個句子（文字序列）。這跟在 讓 AI 寫點金庸 一文中會隨機生成天龍八部文章的 語言模型（Language Model） 是有所差異的： 您的瀏覽器不支援影片標籤，請留言通知我：S 隨機序列生成的例子：一個以 LSTM 實作的簡單語言模型 （ 圖片來源 ） 一般來說，語言模型可以在不給定任何輸入的情況下生成非常隨機的文字序列；但針對機器翻譯這種有條件的序列生成任務，我們通常希望給定相同輸入，輸出的結果越穩定越好（或是每次都一模一樣）。 我們在 實作的時候 會看到怎麼達成這件事情。 Encoder-Decoder 模型 + 注意力機制 好啦，你現在應該已經了解如何使用 Seq2Seq 模型來做 NMT 了，不過現在讓我們再次複習其運作方式。這次我們把用 RNN 實作的 Encoder / Decoder 在每個時間點做的事情從左到右一字排開： 您的瀏覽器不支援影片標籤，請留言通知我：S 以 RNN 為基礎的 Seq2Seq 模型做 NMT 的流程 （ 圖片來源 ） 基本款的 Seq2Seq 模型表現得不錯，但其實有可以改善的地方。你有看出來了嗎？上圖的輸入句子只有 3 個詞彙，但如果我們想輸入一個很長的句子呢？ 我們前面曾提過 Seq2Seq 模型裡的一個重要假設是 Encoder 能把輸入句子的語義 / 文本脈絡全都壓縮成 一個 固定維度的語義向量。之後 Decoder 只要利用該向量裡頭的資訊就能重新生成具有相同意義，但不同語言的句子。 但你可以想像當我們只有一個向量的時候，是不太可能把一個很長的句子的所有資訊打包起來的。 這時候怎麼辦呢？ 與其只把 Encoder 處理完句子產生的最後「一個」向量交給 Decoder 並要求其從中萃取整句資訊，不如將 Encoder 在處理每個詞彙後所生成的「所有」輸出向量都交給 Decoder，讓 Decoder 自己決定在生成新序列的時候要把「注意」放在 Encoder 的哪些輸出向量上面。 這事實上就是 注意力機制（Attention Mechanism） 的中心思想：提供更多資訊給 Decoder，並透過類似資料庫存取的概念，令其自行學會該怎麼提取資訊。兩篇核心論文分別在 2014 年 9 月 及 2015 年 8 月 釋出，概念不難但威力十分強大。 以下就是將注意力機制加到 Seq2Seq 模型後的結果： 您的瀏覽器不支援影片標籤，請留言通知我：S 注意力機制讓 Decoder 在生成新序列時能查看 Encoder 裡所有可能有用的隱狀態向量 （ 圖片來源 ） 你可以拉回去跟沒有注意力機制的 Seq2Seq 模型比較一下差異。 現在你會看到 Encoder 把處理完每個詞彙所產生的向量都交給 Decoder 了。且透過注意力機制，Decoder 在生成新序列的每個元素時都能 動態地 考慮自己要看哪些 Encoder 的向量（還有決定從中該擷取多少資訊），因此這種運用注意力機制的 Seq2Seq 架構又被稱作 動態的條件序列生成（Dynamic Conditional Generation） 。 您的瀏覽器不支援影片標籤，請留言通知我：S 法翻英時，Decoder 在生成每個英文詞彙時都在 Encoder 的每個輸出向量上放不同的注意程度 （ 圖片來源 ） 實際構想並證明其有效的研究者們十分厲害，且其概念也挺符合人類直覺的，對吧？ 為了方便讀者理解，上面動畫實際上隱藏了一些細節： 呈現算好的注意程度而不是計算過程 Encoder / 跟 Decoder 的實際架構 既然是深度學習，Encoder / Decoder 一般來說都是由多個 LSTM / GRU 等 RNN Layers 所疊起來的。而注意力機制在這種情境下實際的運作方式如下： 英翻法情境下，Decoder 在第一個時間點進行的注意力機制 （ 圖片來源 ） 左右兩邊分別是 Encoder 與 Decoder ，縱軸則是多層的神經網路區塊 / 層。 雖然上張動畫是法翻英（這邊是英翻法），但該動畫也是以一樣的概念將圖中的注意權重（attention weights ）視覺化出來（注意權重和為 1）。 現在讓我們看一下注意力機制實際的計算步驟。在 Decoder 的每個時間點，我們都會進行注意力機制以讓 Decoder 從 Encoder 取得語境資訊： 拿 Decoder 當下的紅色隱狀態向量 ht 跟 Encoder 所有藍色隱狀態向量 hs 做比較，利用 score 函式計算出 ht 對每個 hs 的注意程度 以此注意程度為權重， 加權平均 所有 Encoder 隱狀態 hs 以取得上下文向量 context vector 將此上下文向量與 Decoder 隱狀態結合成一個注意向量 attention vector 並作為該時間的輸出 該注意向量會作為 Decoder 下個時間點的輸入 定義 score 函式的方式不少，現在就先讓我們假設有這麼一個函式。 至此為止，你應該已經能夠看懂注意力機制的計算公式： 注意力機制前 3 步驟的數學式子 （ 圖片來源 ） 而之所以稱為注意權重（attention weights），是因為注意力機制可以被視為是一個學習來源語言和目標語言 每一個單詞之間關係 的小型神經網路，而這些權重是該神經網路的參數。 我們在 後面的章節 會實際看到，在訓練還沒開始前，這些權重都是隨機且無意義的。是透過訓練，神經網路才知道該為這些權重賦予什麼值。 你也會發現我在文中提及多次的「注意程度」就是這裡的「注意權重」，而前者是一種擬人化的說法。你可以想像這些權重值讓當下的 Decoder 曉得該放多少關注在 Encoder 個別的隱狀態身上，並依此從它們身上取得上下文資訊（步驟 2）。 而事實上神經網路並沒有意識，因此也不會有感知層次上的「注意」。它學到的是讓注意力機制產生最好結果的「參數權重」，而不是我們人類想像的「注意程度」。只有人類可以賦予神經網路裡頭的計算意義。 有點扯遠了，畢竟這裡應該沒有人文學系的讀者。 讓我們拉回注意力機制。 將此機制加入 Seq2Seq 模型後，NMT 系統的翻譯水準再次起飛。Google 在 2016 年推出的 Google Neural Machine Translation system（GNMT） 是一個知名的案例。除了注意力機制以外，GNMT 在 Encoder 跟 Decoder 都採用了多達 8 層的 LSTM 神經網路 ，讓更多人見識到深度學習的威力。 跟 Google 10 年前推出的 PBMT 系統比起來，翻譯錯誤率平均下降了 60 %。 您的瀏覽器不支援影片標籤，請留言通知我：S 利用注意力機制的 GNMT 讓 Decoder 在生成「Knowledge」時能放注意力在 Encoder 處理完「知」與「識」的兩個輸出向量 e0 & e1 （ 圖片來源 ） 上圖為 GNMT 做中翻英的過程。Encoder 跟 Decoder 之間的線條代表注意力（Attention），線條越粗代表下面的 Decoder 在生成某英文字時越關注上方的某些中文字。模型自己學會在翻譯時該看來源句子中的哪些資訊，很聰明，不是嗎？ 因為其卓越的翻譯品質，在 GNMT 推出的那段時間，搭配注意力機制的 Seq2Seq 模型基本上就是拿來做 NMT 系統的不二人選。 NMT、PBMT 以及人類在中英翻譯時的結果比較 （ 圖片來源 ） 話說當年 Google 導入 GNMT 時釋出了 8 個語言之間的對應翻譯， 涵蓋了約 1/3 的世界人口以及超過 35 % 的 Google 翻譯查詢 ，是機器翻譯發展的一個重要里程碑。 Transformer：Seq2Seq 模型 + 自注意力機制 好酒沉甕底，萬眾矚目的時刻來了。 標題已經破梗。你已經知道我們將探討本文主角 Transformer，且理論上越後面出來的 BOSS 越強。 但你現在可能在想： Seq2Seq 模型搭配注意力機制感覺已經很猛了，難道還有什麼可以改善的嗎？ 答案是肯定的 Yes。 不過這次問題不是出在 Encoder 跟 Decoder 中間交換的資訊不夠，也不是 Seq2Seq 架構本身有什麼問題，問題是出在我們是用 RNN 來實作 Encoder 以及 Decoder。 循環神經網路 RNN 時常被拿來處理序列數據，但其運作方式存在著一個困擾研究者已久的問題：無法有效地平行運算。以一個有 4 個元素的輸入序列為例： [a1, a2, a3, a4] 要獲得最後一個時間點的輸出向量 b4 得把整個輸入序列跑過一遍才行： 自注意層可以做到跟雙向 RNN 一樣的事情，還可以平行運算 （ 圖片來源 ） Google 在 2017 年 6 月的一篇論文：Attention Is All You Need 裡參考了注意力機制，提出了 自 注意力機制（Self-Attention mechanism）。這個機制不只跟 RNN 一樣可以處理序列數據，還可以平行運算。 以剛剛的輸入序列 a[] 為例： [a1, a2, a3, a4] 一個自注意層（Self-Attention Layer）可以利用矩陣運算在等同於 RNN 的一個時間點內就回傳所有 bi ，且每個 bi 都包含了整個輸入序列的資訊。相比之下，RNN 得經過 4 個時間點依序看過 [a1, a2, a3, a4] 以後才能取得序列中最後一個元素的輸出 b4 。 雖然我們還沒講到實際的運作過程，但在給定一個輸入序列的情境下，自注意力機制的基本精神就是： 在建立序列中每個元素的 repr. 時，同時去「注意」並擷取同個序列中其他元素的語義資訊。接著將這些語義資訊合併成上下文資訊並當作自己的 repr. 回傳。 repr. 為 representation 縮寫，在本文的機器翻譯情境裡頭，其意味著可以用來描述某個詞彙、句子意涵的多維實數張量。 雖然我們一直強調自注意力機制的平行能力，如果你還記得我們在 上一節 講述的注意力機制，就會發現在 Seq2Seq 架構裡頭自注意力機制跟注意力機制講的根本是同樣一件事情： 注意力機制讓 Decoder 在生成輸出元素的 repr. 時關注 Encoder 的輸出序列，從中獲得上下文資訊 自注意力機制讓 Encoder 在生成輸入元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊 自注意力機制讓 Decoder 在生成輸出元素的 repr. 時關注自己序列中的其他元素，從中獲得上下文資訊 我們發現一個非常重要的模式： 注意力機制跟自注意力機制都是讓序列 q 關注序列 k 來將上下文資訊 v 匯總到序列 q 的 repr. 裡頭，只是使用的序列不同。 這也是為何 在後面實作時我們只需要一個注意函式 就好了。總之透過新設計的自注意力機制以及原有的注意力機制， Attention Is All You Need 論文 作者們打造了一個完全不需使用 RNN 的 Seq2Seq 模型：Transformer。以下是 Transformer 中非常簡化的 Encoder-Decoder 版本，讓我們找找哪邊用到了（自）注意力機制： 在 Transformer 裡頭共有 3 個地方用到（自）注意力機制 （ 圖片來源 ） 在 Transformer 裡頭，Decoder 利用注意力機制關注 Encoder 的輸出序列（Encoder-Decoder Attention），而 Encoder 跟 Decoder 各自利用自注意力機制關注自己處理的序列（Self-Attention）。無法平行運算的 RNN 完全消失，名符其實的 Attention is all you need. 以下則是 Transformer 實際上將英文句子翻譯到法文的過程： 您的瀏覽器不支援影片標籤，請留言通知我：S 用 Transformer 將英文句子翻譯到法文的例子 （ 圖片來源 ） 以 Transformer 實作的 NMT 系統基本上可以分為 6 個步驟： Encoder 為輸入序列裡的每個詞彙產生初始的 repr. （即詞向量），以空圈表示 利用自注意力機制將序列中所有詞彙的語義資訊各自匯總成每個詞彙的 repr.，以實圈表示 Encoder 重複 N 次自注意力機制，讓每個詞彙的 repr. 彼此持續修正以完整納入上下文語義 Decoder 在生成每個法文字時也運用了自注意力機制，關注自己之前已生成的元素，將其語義也納入之後生成的元素 在自注意力機制後，Decoder 接著利用注意力機制關注 Encoder 的所有輸出並將其資訊納入當前生成元素的 repr. Decoder 重複步驟 4, 5 以讓當前元素完整包含整體語義 上面動畫的 N 為 3，代表著 Encoder 與 Decoder 的層數。這是一個可以依照任務調整的超參數。 如果你看懂這張圖的資訊流動，就等於瞭解 Transformer 的核心精神了，恭喜！如果仍然有不明瞭的地方，可以搭配我上面的說明多看幾遍動畫或是直接閱讀 Google AI 部落格的原文介紹 。 Transformer 釋出時與其他模型在英德翻譯資料集上的比較 （ 圖片來源 ） 自注意力機制解開了 RNN 加在 GPU 上的拘束器。作者們用了 8 個 NVIDIA P100 GPU ，花了 3 天半訓練了一個 Transformer，而該模型在 WMT 2014 英法 / 英德翻譯都取得了最高水準的成績。 跟其他模型相比，這訓練時間跟其創造的優異成績在當時可以說是逆天的存在。自此「大注意時代」展開，該論文至今超過 1800 次引用，所有研究領域都被自注意力機制相關的論文洗了一波。 沒能趕上開心洗論文的最佳時機也別傷心難過，對我們來說仍然有個十分重要的訊息： 多數以 RNN 做過的研究，都可以用自注意力機制來取代；多數用 Seq2Seq 架構實現過的應用，也都可以用 Transformer 來替換。模型訓練速度更快，結果可能更好。 這也是我決定寫這篇文章的理由之一。雖然本文是以機器翻譯的角度來介紹 Transformer，但事實上只要是能用 RNN 或 Seq2Seq 模型進行的研究領域，你都會看到已經有大量跟（自）注意力機制或是 Transformer 有關的論文了： 文本摘要（Text Summarization） 圖像描述（Image Captioning） 閱讀理解（Reading Comprehension） 語音辨識（Voice Recognition） 語言模型（Language Model） 聊天機器人（Chat Bot） 其他任何可以用 RNN 的潛在應用 知名的 BERT 與 GPT-2 都是 Transformer 的延伸 當然不是每個人都喜歡或需要看論文。如果你只是想要應用 Transformer 也沒問題。我在 進擊的 BERT：NLP 界的巨人之力與遷移學習 一文詳細說明你可以如何用 Transformer-based 的語言代表模型進行遷移學習（transfer learning），輕鬆利用前人智慧來完成手上的 NLP 任務； OpenAI 的 GPT 則是非常厲害的語言模型，能產生非常順暢的文章。你可以參考我的 GPT-2 文章： 直觀理解 GPT-2 語言模型並生成金庸武俠小說 。 這些都是 Transformer 的應用。想了解更多，我推薦李宏毅教授最近 講解 ELMO、BERT 以及 GPT 的 YouTube 影片 ，十分通俗易懂 ： 李宏毅教授講解目前 NLP 領域的最新研究是如何讓機器讀懂文字的 如果你接下來想往深度學習領域發展（尤其是自然語言處理這塊），了解（自）注意力機制以及 Transformer 的運作方式幾乎可以說是必經之路。就算沒打算自己手刻 Transformer，你現在應該也稍微能夠體會現代的神經網路到底在在對自然語言做些什麼了。 至此本文的上半部分結束。在下半段我們將實作一個能進行英翻中的 Transformer。等等會說明一項要你完成的事情，不過現在先離開位置喝點東西、讓眼睛跟腦袋休息一下吧！ 師傅引進門，修行在個人 你回來了嗎？還是等不及待地想繼續往下閱讀？ 接下來我們會進入實際的程式實作。但跟前半段相比難度呈指數型上升，因此我只推薦符合以下條件的讀者閱讀： 想透過實作 Transformer 來徹底了解其內部運作原理的人 願意先花 1 小時了解 Transformer 的細節概念與理論的人 你馬上就會知道 1 個小時代表什麼意思。如果你覺得這聽起來很 ok，那可以繼續閱讀。 在 機器翻譯近代史 一章我們已經花了不少篇幅講解了許多在實作 Transformer 時會有幫助的重要概念，其中包含： Seq2Seq 模型的運作原理 注意力機制的概念與計算過程 自注意力機制與 Transformer 的精神 壞消息是，深度學習裡頭理論跟實作的差異常常是很大的。儘管這些背景知識對理解 Transformer 的精神非常有幫助，對從來沒有用過 RNN 實現文本生成 或是以 Seq2Seq 模型 + 注意力機制實現過 NMT 的人來說，要在第一次就正確實現 Transformer 仍是一個巨大的挑戰。 就算不說理論跟實作的差異，讓我們看看 TensorFlow 官方釋出的最新 Transformer 教學 裡頭有多少內容： 您的瀏覽器不支援影片標籤，請留言通知我：S TensorFlow 官方的 Transformer 教學 上面是我用這輩子最快的速度捲動該頁面再加速後的結果，可以看出內容還真不少。儘管中文化很重要，我在這篇文章裡不會幫你把其中的敘述翻成中文（畢竟你的英文可能比我好） 反之，我將利用 TensorFlow 官方的程式碼，以最適合「初心者」理解的實作順序來講述 Transformer 的重要技術細節及概念。在閱讀本文之後，你將有能力自行理解 TensorFlow 官方教學以及其他網路上的實作（比方說 HarvardNLP 以 Pytorch 實現的 The Annotated Transformer ）。 但在實作前有件事情要請你完成：觀看個 YouTube 影片。 教授講解 self-attention 計算方式及 Transformer 的運作原理，強力推薦 現在閱讀此文的讀者真的很幸福。 李宏毅教授前陣子才在 他 2019 年的台大機器學習課程 發佈了 Transformer 的教學影片 ，而這可以說是世界上最好的中文教學影片。如果你真的想要深入理解 Transformer，在實作前至少把上面的影片看完吧！你可以少走很多彎路。 實作時我會盡量重述關鍵概念，但如果有先看影片你會比較容易理解我在碎碎念什麼。如果看完影片你的小宇宙開始發光發熱，也可以先讀讀 Transformer 的原始論文 ，跟很多學術論文比起來相當好讀，真心不騙。 重申一次，除非你已經了解基本注意力機制的運算以及 Transformer 的整體架構，否則我不建議繼續閱讀。 11 個重要 Transformer 概念回顧 怎麼樣？你應該已經從教授的課程中學到不少重要概念了吧？我不知道你還記得多少，但讓我非常簡單地幫你複習一下。 自注意層（Self-Attention Layer）跟 RNN 一樣，輸入是一個序列，輸出一個序列。但是該層可以平行計算，且輸出序列中的每個向量都已經看了整個序列的資訊。 自注意層將輸入序列 I 裡頭的每個位置的向量 i 透過 3 個線性轉換分別變成 3 個向量： q 、 k 和 v ，並將每個位置的 q 拿去跟序列中其他位置的 k 做匹配，算出匹配程度後利用 softmax 層取得介於 0 到 1 之間的權重值，並以此權重跟每個位置的 v 作加權平均，最後取得該位置的輸出向量 o 。全部位置的輸出向量可以同時平行計算，最後輸出序列 O 。 計算匹配程度（注意）的方法不只一種，只要能吃進 2 個向量並吐出一個數值即可。但在 Transformer 論文原文是將 2 向量做 dot product 算匹配程度。 我們可以透過大量矩陣運算以及 GPU 將概念 2 提到的注意力機制的計算全部平行化，加快訓練效率（也是本文實作的重點）。 多頭注意力機制（Multi-head Attention）是將輸入序列中的每個位置的 q 、 k 和 v 切割成多個 qi 、 ki 和 vi 再分別各自進行注意力機制。各自處理完以後把所有結果串接並視情況降維。這樣的好處是能讓各個 head 各司其職，學會關注序列中不同位置在不同 representaton spaces 的資訊。 自注意力機制這樣的計算的好處是「天涯若比鄰」：序列中每個位置都可以在 O(1) 的距離內關注任一其他位置的資訊，運算效率較雙向 RNN 優秀。 自注意層可以取代 Seq2Seq 模型裡頭以 RNN 為基礎的 Encoder / Decoder，而實際上全部替換掉後就（大致上）是 Transformer。 自注意力機制預設沒有「先後順序」的概念，而這也是為何其可以快速平行運算的原因。在進行如機器翻譯等序列生成任務時，我們需要額外加入位置編碼（Positioning Encoding）來加入順序資訊。而在 Transformer 原論文中此值為手設而非訓練出來的模型權重。 Transformer 是一個 Seq2Seq 模型，自然包含了 Encoder / Decoder，而 Encoder 及 Decoder 可以包含多層結構相同的 blocks，裡頭每層都會有 multi-head attention 以及 Feed Forward Network。 在每個 Encoder / Decoder block 裡頭，我們還會使用殘差連結（Residual Connection）以及 Layer Normalization。這些能幫助模型穩定訓練。 Decoder 在關注 Encoder 輸出時會需要遮罩（mask）來避免看到未來資訊。我們後面會看到，事實上還會需要其他遮罩。 這些應該是你在看完影片後學到的東西。如果你想要快速複習，這裡則是 教授課程的 PDF 檔 。 另外你之後也可以隨時透過左側導覽的圖片 icon 來快速回顧 Transformer 的整體架構以及教授添加的註解。我相信在實作的時候它可以幫得上點忙： 您的瀏覽器不支援影片標籤，請留言通知我：S 有了這些背景知識以後，在理解程式碼時會輕鬆許多。你也可以一邊執行 TensorFlow 官方的 Colab 筆記本 一邊參考底下實作。 好戲登場！ 安裝函式庫並設置環境 在這邊我們引進一些常用的 Python 函式庫，這應該不需要特別說明。 import os import time import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt from pprint import pprint from IPython.display import clear_output 比較值得注意的是我們將以 最新的 TensorFlow 2 Beta 版本 來實作本文的 Transformer。另外也會透過 TensorFlow Datasets 來使用前人幫我們準備好的英中翻譯資料集： ! pip install tensorflow-gpu == 2 .0.0-beta0 clear_output () import tensorflow as tf import tensorflow_datasets as tfds print ( tf . __version__ ) 2.0.0-beta0 另外為了避免 TensorFlow 吐給我們太多不必要的資訊，在此文中我也將改變 logging 等級。 在 TensorFlow 2 裡頭因為 tf.logging 被 deprecated ，我們可以直接用 logging 模組來做到這件事情： import logging logging . basicConfig ( level = \"error\" ) np . set_printoptions ( suppress = True ) 我們同時也讓 numpy 不要顯示科學記號。這樣可以讓我們之後在做一些 Tensor 運算的時候版面能乾淨一點。 接著定義一些之後在儲存檔案時會用到的路徑變數： output_dir = \"nmt\" en_vocab_file = os . path . join ( output_dir , \"en_vocab\" ) zh_vocab_file = os . path . join ( output_dir , \"zh_vocab\" ) checkpoint_path = os . path . join ( output_dir , \"checkpoints\" ) log_dir = os . path . join ( output_dir , 'logs' ) download_dir = \"tensorflow-datasets/downloads\" if not os . path . exists ( output_dir ): os . makedirs ( output_dir ) 建立輸入管道 現行的 GPU 以及 TPU 能透過平行運算幫我們顯著地縮短訓練一個 step 所需的時間。而為了讓平行計算能發揮最佳性能，我們需要最佳化 輸入管道（Input pipeline） ，以在當前訓練步驟完成之前就準備好下一個時間點 GPU 要用的數據。 而我們將透過 tf.data API 以及前面導入的 TensorFlow Datasets 來建置高效的輸入管道，並將 機器翻譯競賽 WMT 2019 的中英資料集準備好。 下載並準備資料集 首先看看 tfds 裡頭 WMT 2019 的中英翻譯有哪些資料來源： tmp_builder = tfds . builder ( \"wmt19_translate/zh-en\" ) pprint ( tmp_builder . subsets ) {NamedSplit('train'): ['newscommentary_v14', 'wikititles_v1', 'uncorpus_v1', 'casia2015', 'casict2011', 'casict2015', 'datum2015', 'datum2017', 'neu2017'], NamedSplit('validation'): ['newstest2018']} 可以看到在 WMT 2019 裡中英對照的數據來源還算不少。其中幾個很好猜到其性質： 聯合國數據： uncorpus_v1 維基百科標題： wikititles_v1 新聞評論： newscommentary_v14 雖然大量數據對訓練神經網路很有幫助，本文為了節省訓練 Transformer 所需的時間，在這裡我們就只選擇一個資料來源當作資料集。至於要選哪個資料來源呢？ 聯合國的數據非常龐大，而維基百科標題通常內容很短， 新聞評論 感覺是一個相對適合的選擇。我們可以在設定檔 config 裡頭指定新聞評論這個資料來源並請 TensorFlow Datasets 下載： config = tfds . translate . wmt . WmtConfig ( version = tfds . core . Version ( '0.0.3' , experiments = { tfds . core . Experiment . S3 : False }), language_pair = ( \"zh\" , \"en\" ), subsets = { tfds . Split . TRAIN : [ \"newscommentary_v14\" ] } ) builder = tfds . builder ( \"wmt_translate\" , config = config ) builder . download_and_prepare ( download_dir = download_dir ) clear_output () 您的瀏覽器不支援影片標籤，請留言通知我：S 上面的指令約需 2 分鐘完成，而在過程中 tfds 幫我們完成不少工作： 下載包含原始數據的壓縮檔 解壓縮得到 CSV 檔案 逐行讀取該 CSV 裡頭所有中英句子 將不符合格式的 row 自動過濾 Shuffle 數據 將原數據轉換成 TFRecord 數據 以加速讀取 多花點時間把相關 API 文件 看熟，你就能把清理、準備數據的時間花在建構模型以及跑實驗上面。 切割資料集 雖然我們只下載了一個新聞評論的數據集，裡頭還是有超過 30 萬筆的中英平行句子。為了減少訓練所需的時間，讓我們使用 tfds.Split 定義一個將此數據集切成多個部分的 split ： train_perc = 20 val_prec = 1 drop_prec = 100 - train_perc - val_prec split = tfds . Split . TRAIN . subsplit ([ train_perc , val_prec , drop_prec ]) split (NamedSplit('train')(tfds.percent[0:20]), NamedSplit('train')(tfds.percent[20:21]), NamedSplit('train')(tfds.percent[21:100])) 這個 split 請 tfds 將剛剛處理好的新聞評論資料集再進一步切成 3 個部分，數據量分佈如下： Split 1：20% 數據 Split 2：1% 數據 Split 3：79% 數據 我們將前兩個 splits 拿來當作訓練以及驗證集，剩餘的部分（第 3 個 split）捨棄不用： examples = builder . as_dataset ( split = split , as_supervised = True ) train_examples , val_examples , _ = examples print ( train_examples ) print ( val_examples ) <_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)> <_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)> 你可以在 這邊 找到更多跟 split 相關的用法。 這時候 train_examples 跟 val_examples 都已經是 tf.data.Dataset 。我們在 前處理數據 一節會看到這些數據在被丟入神經網路前需要經過什麼轉換，不過現在先讓我們簡單讀幾筆數據出來看看： for en , zh in train_examples . take ( 3 ): print ( en ) print ( zh ) print ( '-' * 10 ) tf.Tensor(b'Making Do With More', shape=(), dtype=string) tf.Tensor(b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97', shape=(), dtype=string) ---------- tf.Tensor(b'If the Putins, Erdo\\xc4\\x9fans, and Orb\\xc3\\xa1ns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.', shape=(), dtype=string) tf.Tensor(b'\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe3\\x80\\x81\\xe5\\x9f\\x83\\xe5\\xb0\\x94\\xe5\\xa4\\x9a\\xe5\\xae\\x89\\xe5\\x92\\x8c\\xe6\\xac\\xa7\\xe5\\xb0\\x94\\xe7\\x8f\\xad\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b\\xe7\\xbb\\xa7\\xe7\\xbb\\xad\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe5\\xbc\\x80\\xe6\\x94\\xbe\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe4\\xbd\\x93\\xe7\\xb3\\xbb\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe7\\x9a\\x84\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e\\xe5\\x88\\xa9\\xe7\\x9b\\x8a\\xef\\xbc\\x8c\\xe5\\xb0\\xb1\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\xae\\x80\\xe5\\x8d\\x95\\xe5\\x9c\\xb0\\xe5\\x88\\xb6\\xe5\\xae\\x9a\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xa7\\x84\\xe5\\x88\\x99\\xe3\\x80\\x82', shape=(), dtype=string) ---------- tf.Tensor(b'This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.', shape=(), dtype=string) tf.Tensor(b'\\xe5\\x8f\\xaa\\xe6\\x9c\\x89\\xe5\\x9c\\xa8\\xe5\\x8f\\x91\\xe7\\x94\\x9f\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe8\\x90\\xa7\\xe6\\x9d\\xa1\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\x8f\\x8d\\xe5\\xb8\\xb8\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe6\\x97\\xb6\\xef\\xbc\\x8c\\xe8\\xbf\\x99\\xe4\\xb8\\x80\\xe4\\xb8\\x8a\\xe9\\x99\\x90\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\x81\\x9a\\xe5\\x87\\xba\\xe8\\xb0\\x83\\xe6\\x95\\xb4\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe4\\xbe\\xbf\\xe8\\xae\\xa9\\xe5\\x8f\\x8d\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x94\\xbf\\xe7\\xad\\x96\\xe5\\xae\\x9e\\xe6\\x96\\xbd\\xe8\\xb6\\xb3\\xe5\\xa4\\x9f\\xe7\\x9a\\x84\\xe9\\x95\\xbf\\xe5\\xba\\xa6\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe4\\xba\\xba\\xe4\\xbb\\xac\\xe4\\xb8\\x80\\xe8\\x87\\xb4\\xe8\\xae\\xa4\\xe4\\xb8\\xba\\xe5\\xa2\\x9e\\xe5\\x8a\\xa0\\xe7\\x9a\\x84\\xe8\\xb5\\xa4\\xe5\\xad\\x97\\xe6\\x98\\xaf\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe3\\x80\\x82', shape=(), dtype=string) ---------- 跟預期一樣，每一個例子（每一次的 take ）都包含了 2 個以 unicode 呈現的 tf.Tensor 。它們有一樣的語義，只是一個是英文，一個是中文。 讓我們將這些 Tensors 實際儲存的字串利用 numpy() 取出並解碼看看： sample_examples = [] num_samples = 10 for en_t , zh_t in train_examples . take ( num_samples ): en = en_t . numpy () . decode ( \"utf-8\" ) zh = zh_t . numpy () . decode ( \"utf-8\" ) print ( en ) print ( zh ) print ( '-' * 10 ) # 之後用來簡單評估模型的訓練情況 sample_examples . append (( en , zh )) Making Do With More 多劳应多得 ---------- If the Putins, Erdoğans, and Orbáns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules. 如果普京、埃尔多安和欧尔班希望继续享有开放国际体系提供的经济利益，就不能简单地制定自己的规则。 ---------- This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural. 只有在发生深度萧条或其他反常事件时，这一上限才能做出调整，以便让反周期政策实施足够的长度，使人们一致认为增加的赤字是周期性的，而不是结构性的。 ---------- Fascist and communist regimes of the past, which followed a similar instrumentalist approach to democracy, come to mind here. 在此我们想起了过去的法西斯主义和共产主义。 它们都相似地将民主作为实现其目的的工具。 ---------- This phase culminated with the collapse of communism in 1989, but the chance to overcome the Continent's historical divisions now required a redefinition of the European project. 这种状态随着1989年共产主义崩溃而达至巅峰，但是克服欧洲大陆历史性分裂的机遇现在需要重新定义欧洲计划。 ---------- The eurozone's collapse (and, for all practical purposes, that of the EU itself) forces a major realignment of European politics. 欧元区的瓦解强迫欧洲政治进行一次重大改组。 ---------- With energy and enthusiasm, Burden turned that operation into a thriving health (not health-care) agency that covers three cities and about 300,000 people on the western edge of Los Angeles. 在能量与激情的推动下，波顿将BCHD打造成了欣欣向荣的健康（而非医疗）机构，其服务范围覆盖了洛杉矶西侧三座城市的30万人。 ---------- The result could be a world of fragmented blocs – an outcome that would undermine not only global prosperity, but also cooperation on shared challenges. 其结果可能是一个四分五裂的世界 — — 这一结果不但会破坏全球繁荣，也会破坏面对共同挑战的合作。 ---------- Among the questions being asked by NGOs, the UN, and national donors is how to prevent the recurrence of past mistakes. 现在NGO们、联合国和捐助国们问得最多的一个问题就是如何避免再犯过去的错误。 ---------- Managing the rise of NCDs will require long-term thinking, and government leaders will have to make investments that might pay off only after they are no longer in office. 管理NCD的增加需要长期思维，政府领导人必须进行要在他们离任多年后才能收回成本的投资。 ---------- 想像一下沒有對應的中文，要閱讀這些英文得花多少時間。你可以試著消化其中幾句中文與其對應的英文句子，並比較一下所需要的時間差異。 雖然只是隨意列出的 10 個中英句子，你應該跟我一樣也能感受到機器翻譯研究的重要以及其能帶給我們的價值。 建立中文與英文字典 就跟大多數 NLP 專案相同，有了原始的中英句子以後我們得分別為其建立字典來將每個詞彙轉成索引（Index）。 tfds.features.text 底下的 SubwordTextEncoder 提供非常方便的 API 讓我們掃過整個訓練資料集並建立字典。 首先為英文語料建立字典： %%time try : subword_encoder_en = tfds . features . text . SubwordTextEncoder . load_from_file ( en_vocab_file ) print ( f \"載入已建立的字典： {en_vocab_file} \" ) except : print ( \"沒有已建立的字典，從頭建立。\" ) subword_encoder_en = tfds . features . text . SubwordTextEncoder . build_from_corpus ( ( en . numpy () for en , _ in train_examples ), target_vocab_size = 2 ** 13 ) # 有需要可以調整字典大小 # 將字典檔案存下以方便下次 warmstart subword_encoder_en . save_to_file ( en_vocab_file ) print ( f \"字典大小： {subword_encoder_en.vocab_size} \" ) print ( f \"前 10 個 subwords： {subword_encoder_en.subwords[:10]} \" ) print () 載入已建立的字典： /content/gdrive/My Drive/nmt/en_vocab 字典大小：8135 前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'that_', 'is_'] CPU times: user 41 ms, sys: 7.43 ms, total: 48.4 ms Wall time: 391 ms 如果你的語料庫（corpus） 不小，要掃過整個資料集並建立一個字典得花不少時間。因此實務上我們會先使用 load_from_file 函式嘗試讀取之前已經建好的字典檔案，失敗才 build_from_corpus 。 這招很基本，但在你需要重複處理巨大語料庫時非常重要。 subword_encoder_en 則是利用 GNMT 當初推出的 wordpieces 來進行斷詞，而簡單來說其產生的子詞（subword）介於這兩者之間： 用英文字母分隔的斷詞（character-delimited） 用空白分隔的斷詞（word-delimited） 在掃過所有英文句子以後， subword_encoder_en 建立一個有 8135 個子詞的字典。我們可以用該字典來幫我們將一個英文句子轉成對應的索引序列（index sequence）： sample_string = 'Taiwan is beautiful.' indices = subword_encoder_en . encode ( sample_string ) indices [2700, 7911, 10, 2942, 7457, 1163, 7925] 這樣的索引序列你應該已經見怪不怪了。我們在 以前的 NLP 入門文章 也使用 tf.keras 裡頭的 Tokenizer 做過類似的事情。 接著讓我們將這些索引還原，看看它們的長相： print ( \" {0:10}{1:6} \" . format ( \"Index\" , \"Subword\" )) print ( \"-\" * 15 ) for idx in indices : subword = subword_encoder_en . decode ([ idx ]) print ( ' {0:5}{1:6} ' . format ( idx , ' ' * 5 + subword )) Index Subword --------------- 2700 Taiwan 7911 10 is 2942 bea 7457 uti 1163 ful 7925 . 當 subword tokenizer 遇到從沒出現過在字典裡的詞彙，會將該詞拆成多個子詞（subwords）。比方說上面句中的 beautiful 就被拆成 bea uti ful 。這也是為何這種斷詞方法比較不怕沒有出現過在字典裡的字（out-of-vocabulary words）。 另外別在意我為了對齊寫的 print 語法。重點是我們可以用 subword_encoder_en 的 decode 函式再度將索引數字轉回其對應的子詞。編碼與解碼是 2 個完全可逆（invertable）的操作： sample_string = 'Taiwan is beautiful.' indices = subword_encoder_en . encode ( sample_string ) decoded_string = subword_encoder_en . decode ( indices ) assert decoded_string == sample_string pprint (( sample_string , decoded_string )) ('Taiwan is beautiful.', 'Taiwan is beautiful.') 酷！接著讓我們如法炮製，為中文也建立一個字典： %%time try : subword_encoder_zh = tfds . features . text . SubwordTextEncoder . load_from_file ( zh_vocab_file ) print ( f \"載入已建立的字典： {zh_vocab_file} \" ) except : print ( \"沒有已建立的字典，從頭建立。\" ) subword_encoder_zh = tfds . features . text . SubwordTextEncoder . build_from_corpus ( ( zh . numpy () for _ , zh in train_examples ), target_vocab_size = 2 ** 13 , # 有需要可以調整字典大小 max_subword_length = 1 ) # 每一個中文字就是字典裡的一個單位 # 將字典檔案存下以方便下次 warmstart subword_encoder_zh . save_to_file ( zh_vocab_file ) print ( f \"字典大小： {subword_encoder_zh.vocab_size} \" ) print ( f \"前 10 個 subwords： {subword_encoder_zh.subwords[:10]} \" ) print () 載入已建立的字典： /content/gdrive/My Drive/nmt/zh_vocab 字典大小：4201 前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这'] CPU times: user 27.6 ms, sys: 121 µs, total: 27.7 ms Wall time: 337 ms 在使用 build_from_corpus 函式掃過整個中文資料集時，我們將 max_subword_length 參數設置為 1。這樣可以讓每個漢字都會被視為字典裡頭的一個單位。畢竟跟英文的 abc 字母不同，一個漢字代表的意思可多得多了。而且如果使用 n-gram 的話可能的詞彙組合太多，在小數據集的情況非常容易遇到不存在字典裡頭的字。 另外所有漢字也就大約 4000 ~ 5000 個可能，作為一個分類問題（classification problem）還是可以接受的。 讓我們挑個中文句子來測試看看： sample_string = sample_examples [ 0 ][ 1 ] indices = subword_encoder_zh . encode ( sample_string ) print ( sample_string ) print ( indices ) 多劳应多得 [48, 557, 116, 48, 81] 好的，我們把中英文斷詞及字典的部分都搞定了。現在給定一個例子（example，在這邊以及後文指的都是一組包含同語義的中英平行句子），我們都能將其轉換成對應的索引序列了： en = \"The eurozone's collapse forces a major realignment of European politics.\" zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\" # 將文字轉成為 subword indices en_indices = subword_encoder_en . encode ( en ) zh_indices = subword_encoder_zh . encode ( zh ) print ( \"[英中原文]（轉換前）\" ) print ( en ) print ( zh ) print () print ( '-' * 20 ) print () print ( \"[英中序列]（轉換後）\" ) print ( en_indices ) print ( zh_indices ) [英中原文]（轉換前） The eurozone's collapse forces a major realignment of European politics. 欧元区的瓦解强迫欧洲政治进行一次重大改组。 -------------------- [英中序列]（轉換後） [17, 965, 11, 6, 1707, 676, 8, 211, 2712, 6683, 249, 3, 85, 1447, 7925] [45, 206, 171, 1, 847, 197, 236, 604, 45, 90, 17, 130, 102, 36, 7, 284, 80, 18, 212, 265, 3] 接著讓我們針對這些索引序列（index sequence）做一些前處理。 前處理數據 在處理序列數據時我們時常會在一個序列的前後各加入一個特殊的 token，以標記該序列的開始與完結，而它們常有許多不同的稱呼： 開始 token、 B egin o f S entence、BOS、 <start> 結束 token、 E nd o f S entence、EOS、 <end> 這邊我們定義了一個將被 tf.data.Dataset 使用的 encode 函式，它的輸入是一筆包含 2 個 string Tensors 的例子，輸出則是 2 個包含 BOS / EOS 的索引序列： def encode ( en_t , zh_t ): # 因為字典的索引從 0 開始， # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值 # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值 en_indices = [ subword_encoder_en . vocab_size ] + subword_encoder_en . encode ( en_t . numpy ()) + [ subword_encoder_en . vocab_size + 1 ] # 同理，不過是使用中文字典的最後一個索引 + 1 zh_indices = [ subword_encoder_zh . vocab_size ] + subword_encoder_zh . encode ( zh_t . numpy ()) + [ subword_encoder_zh . vocab_size + 1 ] return en_indices , zh_indices 因為 tf.data.Dataset 裡頭都是在操作 Tensors（而非 Python 字串），所以這個 encode 函式預期的輸入也是 TensorFlow 裡的 Eager Tensors 。但只要我們使用 numpy() 將 Tensor 裡的實際字串取出以後，做的事情就跟上一節完全相同。 讓我們從訓練集裡隨意取一組中英的 Tensors 來看看這個函式的實際輸出： en_t , zh_t = next ( iter ( train_examples )) en_indices , zh_indices = encode ( en_t , zh_t ) print ( '英文 BOS 的 index：' , subword_encoder_en . vocab_size ) print ( '英文 EOS 的 index：' , subword_encoder_en . vocab_size + 1 ) print ( '中文 BOS 的 index：' , subword_encoder_zh . vocab_size ) print ( '中文 EOS 的 index：' , subword_encoder_zh . vocab_size + 1 ) print ( ' \\n 輸入為 2 個 Tensors：' ) pprint (( en_t , zh_t )) print ( '-' * 15 ) print ( '輸出為 2 個索引序列：' ) pprint (( en_indices , zh_indices )) 英文 BOS 的 index： 8135 英文 EOS 的 index： 8136 中文 BOS 的 index： 4201 中文 EOS 的 index： 4202 輸入為 2 個 Tensors： (<tf.Tensor: id=306, shape=(), dtype=string, numpy=b'Making Do With More'>, <tf.Tensor: id=307, shape=(), dtype=string, numpy=b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97'>) --------------- 輸出為 2 個索引序列： ([8135, 4682, 19, 717, 7911, 298, 2701, 7980, 8136], [4201, 48, 557, 116, 48, 81, 4202]) 你可以看到不管是英文還是中文的索引序列，前面都加了一個代表 BOS 的索引（分別為 8135 與 4201），最後一個索引則代表 EOS（分別為 8136 與 4202） 但如果我們將 encode 函式直接套用到整個訓練資料集時會產生以下的錯誤訊息： train_dataset = train_examples . map ( encode ) 這是因為目前 tf.data.Dataset.map 函式裡頭的計算是在 計算圖模式（Graph mode） 下執行，所以裡頭的 Tensors 並不會有 Eager Execution 下才有的 numpy 屬性。 解法是使用 tf.py_function 將我們剛剛定義的 encode 函式包成一個以 eager 模式執行的 TensorFlow Op： def tf_encode ( en_t , zh_t ): # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors # 要到 `tf.py_funtion` 裡頭才是 # 另外因為索引都是整數，所以使用 `tf.int64` return tf . py_function ( encode , [ en_t , zh_t ], [ tf . int64 , tf . int64 ]) # `tmp_dataset` 為說明用資料集，說明完所有重要的 func， # 我們會從頭建立一個正式的 `train_dataset` tmp_dataset = train_examples . map ( tf_encode ) en_indices , zh_indices = next ( iter ( tmp_dataset )) print ( en_indices ) print ( zh_indices ) W0616 23:46:10.571188 140648854296320 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string W0616 23:46:10.573221 140648854296320 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string tf.Tensor([8135 4682 19 717 7911 298 2701 7980 8136], shape=(9,), dtype=int64) tf.Tensor([4201 48 557 116 48 81 4202], shape=(7,), dtype=int64) 有點 tricky 但任務完成！注意在套用 map 函式以後， tmp_dataset 的輸出已經是兩個索引序列，而非原文字串。 為了讓 Transformer 快點完成訓練，讓我們將長度超過 40 個 tokens 的序列都去掉吧！我們在底下定義了一個布林（boolean）函式，其輸入為一個包含兩個英中序列 en, zh 的例子，並在只有這 2 個序列的長度都小於 40 的時候回傳真值（True）： MAX_LENGTH = 40 def filter_max_length ( en , zh , max_length = MAX_LENGTH ): # en, zh 分別代表英文與中文的索引序列 return tf . logical_and ( tf . size ( en ) <= max_length , tf . size ( zh ) <= max_length ) # tf.data.Dataset.filter(func) 只會回傳 func 為真的例子 tmp_dataset = tmp_dataset . filter ( filter_max_length ) 簡單檢查是否有序列超過我們指定的長度，順便計算過濾掉過長序列後剩餘的訓練集筆數： # 因為我們數據量小可以這樣 count num_examples = 0 for en_indices , zh_indices in train_dataset : cond1 = len ( en_indices ) <= MAX_LENGTH cond2 = len ( zh_indices ) <= MAX_LENGTH assert cond1 and cond2 num_examples += 1 print ( f \"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\" ) print ( f \"訓練資料集裡總共有 {num_examples} 筆數據\" ) 訓練資料集裡總共有 29914 筆數據 過濾掉較長句子後還有接近 3 萬筆的訓練例子，看來不用擔心數據太少。 最後值得注意的是每個例子裡的索引序列長度不一，這在建立 batch 時可能會發生問題。不過別擔心，輪到 padded_batch 函式出場了： BATCH_SIZE = 64 # 將 batch 裡的所有序列都 pad 到同樣長度 tmp_dataset = tmp_dataset . padded_batch ( BATCH_SIZE , padded_shapes = ([ - 1 ], [ - 1 ])) en_batch , zh_batch = next ( iter ( tmp_dataset )) print ( \"英文索引序列的 batch\" ) print ( en_batch ) print ( '-' * 20 ) print ( \"中文索引序列的 batch\" ) print ( zh_batch ) W0616 23:46:10.753194 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string W0616 23:46:10.760091 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string W0616 23:46:10.768630 140648845903616 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string 英文索引序列的 batch tf.Tensor( [[8135 4682 19 ... 0 0 0] [8135 17 965 ... 8136 0 0] [8135 6602 2 ... 0 0 0] ... [8135 1097 270 ... 0 0 0] [8135 1713 70 ... 0 0 0] [8135 2731 4553 ... 0 0 0]], shape=(64, 34), dtype=int64) -------------------- 中文索引序列的 batch tf.Tensor( [[4201 48 557 ... 0 0 0] [4201 45 206 ... 0 0 0] [4201 58 5 ... 683 3 4202] ... [4201 29 120 ... 0 0 0] [4201 297 161 ... 0 0 0] [4201 279 149 ... 4202 0 0]], shape=(64, 40), dtype=int64) padded_batch 函式能幫我們將每個 batch 裡頭的序列都補 0 到跟當下 batch 裡頭最長的序列一樣長。 比方說英文 batch 裡最長的序列為 34；而中文 batch 裡最長的序列則長達 40 個 tokens，剛好是我們前面設定過的序列長度上限。 好啦，現在讓我們從頭建立訓練集與驗證集，順便看看這些中英句子是如何被轉換成它們的最終形態的： MAX_LENGTH = 40 BATCH_SIZE = 128 BUFFER_SIZE = 15000 # 訓練集 train_dataset = ( train_examples # 輸出：(英文句子, 中文句子) . map ( tf_encode ) # 輸出：(英文索引序列, 中文索引序列) . filter ( filter_max_length ) # 同上，且序列長度都不超過 40 . cache () # 加快讀取數據 . shuffle ( BUFFER_SIZE ) # 將例子洗牌確保隨機性 . padded_batch ( BATCH_SIZE , # 將 batch 裡的序列都 pad 到一樣長度 padded_shapes = ([ - 1 ], [ - 1 ])) . prefetch ( tf . data . experimental . AUTOTUNE )) # 加速 # 驗證集 val_dataset = ( val_examples . map ( tf_encode ) . filter ( filter_max_length ) . padded_batch ( BATCH_SIZE , padded_shapes = ([ - 1 ], [ - 1 ]))) 建構訓練資料集時我們還添加了些沒提過的函式。它們的用途大都是用來提高輸入效率，並不會影響到輸出格式。如果你想深入了解這些函式的運作方式，可以參考 tf.data 的官方教學 。 現在讓我們看看最後建立出來的資料集長什麼樣子： en_batch , zh_batch = next ( iter ( train_dataset )) print ( \"英文索引序列的 batch\" ) print ( en_batch ) print ( '-' * 20 ) print ( \"中文索引序列的 batch\" ) print ( zh_batch ) 英文索引序列的 batch tf.Tensor( [[8135 222 1 ... 0 0 0] [8135 3812 162 ... 0 0 0] [8135 6267 838 ... 0 0 0] ... [8135 17 1042 ... 0 0 0] [8135 7877 1165 ... 0 0 0] [8135 6414 7911 ... 0 0 0]], shape=(128, 40), dtype=int64) -------------------- 中文索引序列的 batch tf.Tensor( [[4201 109 54 ... 3 4202 0] [4201 30 4 ... 0 0 0] [4201 402 4 ... 0 0 0] ... [4201 626 515 ... 0 0 0] [4201 49 249 ... 0 0 0] [4201 905 209 ... 0 0 0]], shape=(128, 40), dtype=int64) 嘿！我們建立了一個可供訓練的輸入管道（Input pipeline）！ 你會發現訓練集： 一次回傳大小為 128 的 2 個 batch，分別包含 128 個英文、中文的索引序列 序列開頭皆為 BOS，英文的 BOS 索引是 8135；中文的 BOS 索引則為 4201 兩語言 batch 裡的序列都被「拉長」到我們先前定義的最長序列長度：40 驗證集也是相同的輸出形式。 現在你應該可以想像我們在每個訓練步驟會拿出來的數據長什麼樣子了：2 個 shape 為 (batch_size, seq_len) 的 Tensors，而裡頭的每一個索引數字都代表著一個中 / 英文子詞（或是 BOS / EOS）。 在這一節我們建立了一個通用資料集。「通用」代表不限於 Transformer，你也能用 一般搭配注意力機制的 Seq2Seq 模型 來處理這個資料集並做中英翻譯。 但從下節開始讓我們把這個數據集先擺一邊，將注意力全部放到 Transformer 身上並逐一實作其架構裡頭的各個元件。 理解 Transformer 之旅：跟著多維向量去冒險 在實作 Transformer 及注意力機制這種高度平行運算的模型時，你將需要一點「空間想像力」，能夠想像最高高達 4 維的向量是怎麼在 Transformer 的各個元件被處理與轉換的。 如果你跟我一樣腦袋並不是那麼靈光的話，這可不是一件簡單的事情。不過別擔心，從這節開始我會把 Transfomer （主要針對注意力機制）裡頭的矩陣運算過程視覺化（visualize）出來，讓你在這個多維空間裡頭也能悠遊自在。 Welcome to matrix, 準備進入多維空間 就好像一般你在寫程式時會追蹤某些變數在函式裡頭的變化，一個直觀理解 Transformer 的方法是將幾個句子丟入其中，並觀察 Transformer 對它們做了些什麼轉換。 首先讓我們建立兩個要拿來持續追蹤的中英平行句子： demo_examples = [ ( \"It is important.\" , \"这很重要。\" ), ( \"The numbers speak for themselves.\" , \"数字证明了一切。\" ), ] pprint ( demo_examples ) [('It is important.', '这很重要。'), ('The numbers speak for themselves.', '数字证明了一切。')] 接著利用 之前建立資料集的方法 將這 2 組中英句子做些前處理並以 Tensor 的方式讀出： batch_size = 2 demo_examples = tf . data . Dataset . from_tensor_slices (( [ en for en , _ in demo_examples ], [ zh for _ , zh in demo_examples ] )) # 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords） # 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度 demo_dataset = demo_examples . map ( tf_encode ) \\ . padded_batch ( batch_size , padded_shapes = ([ - 1 ], [ - 1 ])) # 取出這個 demo dataset 裡唯一一個 batch inp , tar = next ( iter ( demo_dataset )) print ( 'inp:' , inp ) print ( '' * 10 ) print ( 'tar:' , tar ) inp: tf.Tensor( [[8135 105 10 1304 7925 8136 0 0] [8135 17 3905 6013 12 2572 7925 8136]], shape=(2, 8), dtype=int64) tar: tf.Tensor( [[4201 10 241 80 27 3 4202 0 0 0] [4201 162 467 421 189 14 7 553 3 4202]], shape=(2, 10), dtype=int64) 上節建立的數據集屍骨未寒，你應該還記得 inp shape 裡頭第一個維度的 2 代表著這個 batch 有 2 個句子，而第二維度的 8 則代表著句子的長度（單位：子詞）； tar 則為中文子詞序列（subword sequence），不過因為中文我們以漢字為單位作斷詞，長度一般會比對應的英文句子來的長（shape 中的 10 ）。 2 維矩陣還很容易想像，但我擔心等到你進入 3 維空間後就會想放棄人生了。所以還是先讓我們用人類比較容易理解的方式來呈現這些數據。 視覺化原始句子 如果我們把這 2 個 batch 用你比較熟悉的方式呈現的話會長這樣： 您的瀏覽器不支援影片標籤，請留言通知我：S 這樣清楚多了不是嗎？現在點擊播放鍵，將索引序列還原成原始的子詞序列。 你可以清楚地看到每個 原始 句子前後都有 <start> 與 <end> 。而為了讓同個 batch 裡頭的序列長度相同，我們在較短的序列後面也補上足夠的 0，代表著 <pad> 。 這個視覺化非常簡單，但十分強大。我現在要你記住一些本文會使用的慣例： 不管 張量（Tensor） 變幾維，其第一個維度 shape[0] 永遠代表 batch_size ，也就代表著句子的數目 不同句子我們用不同顏色表示，方便你之後對照這些句子在轉換前後的差異 x 軸（橫軸）代表張量的最後一個維度 shape[-1] ，以上例來說分別為 8 和 10 x, y 軸上的標籤分別代表倒數兩個維度 shape[-2] 及 shape[-1] 其所代表的物理含義，如圖中的 句子 與 子詞 圖中張量的 name 會對應到程式碼裡頭定義的變數名稱，方便你對照並理解實作邏輯。我也會秀出張量的 shape 幫助你想像該向量在多維空間的長相。一個簡單的例子是： (batch_size, tar_seq_len) 這些準則與資訊現在看似多餘，但我保證你很快就會需要它們。 視覺化 3 維詞嵌入張量 在將索引序列丟入神經網路之前，我們一般會做 詞嵌入（word embedding） ，將一個維度為字典大小的高維離散空間「嵌入」到低維的連續空間裡頭。 讓我們為英文與中文分別建立一個詞嵌入層並實際對 inp 及 tar 做轉換： # + 2 是因為我們額外加了 <start> 以及 <end> tokens vocab_size_en = subword_encoder_en . vocab_size + 2 vocab_size_zh = subword_encoder_zh . vocab_size + 2 # 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間 d_model = 4 embedding_layer_en = tf . keras . layers . Embedding ( vocab_size_en , d_model ) embedding_layer_zh = tf . keras . layers . Embedding ( vocab_size_zh , d_model ) emb_inp = embedding_layer_en ( inp ) emb_tar = embedding_layer_zh ( tar ) emb_inp , emb_tar emb_inp: tf.Tensor( [[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.02707888 -0.03917687 -0.01213828 0.00909697] [ 0.0355427 0.04111305 0.00751223 -0.01974255] [ 0.02443342 -0.03273199 0.01267544 0.03127003] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ] [-0.00680635 0.02136201 -0.02036932 -0.04211974] [-0.00680635 0.02136201 -0.02036932 -0.04211974]] [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.0325227 -0.03433502 -0.01849879 0.01439226] [ 0.00144588 -0.00377025 -0.00798036 -0.04099905] [ 0.04524285 0.02524642 -0.00924555 -0.01368124] [-0.0159062 0.01108797 -0.0177028 -0.0435766 ] [ 0.00240784 -0.04652226 0.01821991 -0.04349295] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ]]], shape=(2, 8, 4), dtype=float32) -------------------- emb_tar: tf.Tensor( [[[-0.0441955 -0.01026772 0.03740635 0.02017349] [ 0.02129837 -0.00746276 0.03881821 -0.01586295] [-0.01179456 0.02825376 0.00738146 0.02963744] [ 0.01171205 0.04350302 -0.01190796 0.02526634] [ 0.03814722 -0.03364048 -0.03744673 0.04369817] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349]] [[-0.0441955 -0.01026772 0.03740635 0.02017349] [-0.00359621 -0.01380367 -0.02875998 -0.03855735] [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ] [ 0.04131394 -0.04065727 -0.04330624 -0.03341667] [ 0.03572228 -0.04500845 0.0470326 0.03095007] [-0.03566641 -0.03730996 -0.00597564 -0.03933349] [ 0.01850356 0.03993076 0.02729526 -0.04848848] [-0.02294568 -0.02494572 -0.0136737 -0.04278342] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902]]], shape=(2, 10, 4), dtype=float32) 注意你的詞嵌入層的隨機初始值會跟我不同，結果可能會有一點差異。 但重點是你能在腦海中理解這兩個 3 維張量嗎？花了幾秒鐘？我相信在座不乏各路高手，而且事實上在這一行混久了，你也必須能直覺地理解這個表示方式。 但如果有更好的呈現方式幫助我們理解數據，何樂而不為呢？讓我們再次視覺化這兩個 3 維詞嵌入張量： 您的瀏覽器不支援影片標籤，請留言通知我：S 依照前面提過的準則，張量中第一個維度的 2 代表著句子數 batch_size 。在 3 維空間裡頭，我會將不同句子畫在 z 軸上，也就是你現在把臉貼近 / 遠離螢幕這個維度。你同時也能用不同顏色來區分句子。 緊跟著句子的下一個維度則一樣是本來的子詞（subword）。只是現在每個子詞都已從一個索引數字被轉換成一個 4 維的詞嵌入向量，因此每個子詞都以 y 軸來表示。最後一維則代表著詞嵌入空間的維度，一樣以 x 軸來表示。 現在再次點擊播放鍵。 在學會怎麼解讀這個 3 維詞嵌入張量以後，你就能明白為何 emb_tar 第一個中文句子裡頭的倒數 3 行（row) 都長得一樣了： print ( \"tar[0]:\" , tar [ 0 ][ - 3 :]) print ( \"-\" * 20 ) print ( \"emb_tar[0]:\" , emb_tar [ 0 ][ - 3 :]) tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64) -------------------- emb_tar[0]: tf.Tensor( [[-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349]], shape=(3, 4), dtype=float32) 它們都是 <pad> token（以 0 表示），理當有一樣的詞嵌入向量。 不同顏色也讓我們可以很直觀地理解一個句子是怎麼從一個 1 維向量被轉換到 2 維的。你後面就會發現，你將需要能夠非常直覺地理解像是 emb_tar 這種 3 維張量裡頭每個維度所代表的意義。 遮罩：Transformer 的祕密配方 我們在前面並沒有仔細談過遮罩（masking）的概念，但事實上它可以說是在實作 Transformer 時最重要卻也最容易被搞砸的一環。它讓 Transformer 在進行自注意力機制（Self-Attention Mechanism）時不至於偷看到不該看的。 在 Transformer 裡頭有兩種 masks： padding mask look ahead mask padding mask 是讓 Transformer 用來識別序列實際的內容到哪裡。此遮罩負責的就是將序列中被補 0 的地方（也就是 <pad> ）的位置蓋住，讓 Transformer 可以避免「關注」到這些位置。 look ahead mask 人如其名，是用來確保 Decoder 在進行自注意力機制時每個子詞只會「往前看」：關注（包含）自己之前的字詞，不會不小心關注「未來」Decoder 產生的子詞。我們後面還會看到 look ahead mask 的詳細介紹，但不管是哪一種遮罩向量，那些值為 1 的位置就是遮罩存在的地方。 因為 padding mask 的概念相對簡單，讓我們先看這種遮罩： def create_padding_mask ( seq ): # padding mask 的工作就是把索引序列中為 0 的位置設為 1 mask = tf . cast ( tf . equal ( seq , 0 ), tf . float32 ) return mask [:, tf . newaxis , tf . newaxis , :] # broadcasting inp_mask = create_padding_mask ( inp ) inp_mask <tf.Tensor: id=193029, shape=(2, 1, 1, 8), dtype=float32, numpy= array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]], [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)> 登登！我們的第一個 4 維張量！不過別緊張，我們在中間加了 2 個新維度是為了之後可以做 broadcasting ，現在可以忽視。喔！不過如果這是你第一次聽到 broadcasting，我強烈建議你現在就閱讀 numpy 官方的簡短教學 了解其概念。我們後面也會看到實際的 broadcasting 例子。 回到我們的 inp_mask 遮罩。現在我們可以先將額外的維度去掉以方便跟 inp 作比較： print ( \"inp:\" , inp ) print ( \"-\" * 20 ) print ( \"tf.squeeze(inp_mask):\" , tf . squeeze ( inp_mask )) inp: tf.Tensor( [[8135 105 10 1304 7925 8136 0 0] [8135 17 3905 6013 12 2572 7925 8136]], shape=(2, 8), dtype=int64) -------------------- tf.squeeze(inp_mask): tf.Tensor( [[0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32) 你可以看到 inp_mask 將 inp 裡頭為 0 的對應位置設為 1 凸顯出來，這樣之後其他函式就知道要把哪邊「遮住」。 讓我們看看被降到 2 維的 inp_mask 是怎麼被套用在 inp 身上的： 您的瀏覽器不支援影片標籤，請留言通知我：S 很好懂，不是嗎？但這只是小暖身，等到之後要將遮罩 broadcast 到 3、4 維張量的時候你可能會黑人問號，所以最好做點心理準備（笑 至於另外一種遮罩 look ahead mask，等我們說明完下節的注意函式以後你會比較容易理解它的作用，所以先賣個關子。現在讓我們進入 Tranformer 最核心的部分：注意力機制。 Scaled dot product attention：一種注意函式 我們在文中以及教授的影片已經多次看到，所謂的注意力機制（或稱注意函式，attention function）概念上就是拿一個查詢（query）去跟一組 key-values 做運算，最後產生一個輸出。只是我們會利用矩陣運算同時讓多個查詢跟一組 key-values 做運算，最大化計算效率。 而不管是查詢（query）、鍵值（keys）還是值（values）或是輸出，全部都是向量（vectors）。該輸出是 values 的加權平均，而每個 value 獲得的權重則是由當初 value 對應的 key 跟 query 計算匹配程度所得來的。（ 論文原文 稱此計算匹配程度的函式為 compatibility function） 將此運算以圖表示的話則會長得像這樣： 左右兩邊大致上講的是一樣的事情，不過右側省略 Scale 以及 Mask 步驟，而左側則假設我們已經拿到經過線性轉換的 Q, K, V 我們是第一次秀出論文裡頭的圖片（左），但右邊你應該不陌生才對。 Scaled dot product attention 跟以往 multiplicative attention 一樣是先將維度相同的 Q 跟 K 做 點積 ：將對應維度的值兩兩相乘後相加得到單一數值，接著把這些數值除以一個 scaling factor sqrt(dk) ，然後再丟入 softmax 函式 得到相加為 1 的注意權重（attention weights）。 最後以此權重對 V 作加權平均得到輸出結果。 除以 scaling factor 的目的是為了讓點積出來的值不會因為 Q 以及 K 的維度 dk 太大而跟著太大（舌頭打結）。因為太大的點積值丟入 softmax 函式有可能使得其梯度變得極小，導致訓練結果不理想。 Softmax 函式讓某個 Q 與多個 K 之間的匹配值和為 1 說完概念，讓我們看看 Transformer 論文中的這個注意函式怎麼運作吧！首先我們得先準備這個函式的輸入 Q, K, V 才行。我們在 Multi-head attention 一節會看到，在進行 scaled dot product attention 時會需要先將 Q、K 以及 V 分別做一次線性轉換，但現在讓我們先忽略這部分。 這邊我們可以拿已經被轉換成詞嵌入空間的英文張量 emb_inp 來充當左圖中的 Q 以及 K，讓它自己跟自己做匹配。V 則讓我隨機產生一個 binary 張量（裡頭只有 1 或 0）來當作每個 K 所對應的值，方便我們直觀解讀 scaled dot product attention 的輸出結果： # 設定一個 seed 確保我們每次都拿到一樣的隨機結果 tf . random . set_seed ( 9527 ) # 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp` q = emb_inp k = emb_inp # 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector v = tf . cast ( tf . math . greater ( tf . random . uniform ( shape = emb_inp . shape ), 0.5 ), tf . float32 ) v <tf.Tensor: id=193043, shape=(2, 8, 4), dtype=float32, numpy= array([[[1., 0., 0., 0.], [0., 1., 0., 1.], [0., 0., 0., 1.], [1., 0., 1., 0.], [1., 0., 1., 0.], [0., 1., 0., 1.], [0., 0., 1., 0.], [0., 1., 0., 1.]], [[1., 0., 1., 1.], [1., 0., 1., 0.], [1., 0., 0., 0.], [1., 0., 1., 0.], [0., 1., 0., 1.], [1., 1., 1., 1.], [0., 0., 0., 0.], [0., 0., 1., 0.]]], dtype=float32)> 好啦，我想你現在應該能快速地解讀 3 維張量了，但還是讓我雞婆點，將現在的 Q, K, V 都畫出來讓你參考： 注意顏色。雖然我們將拿 Q 跟 K 來做匹配，這個匹配只會發生在同個句子（同個顏色）底下（即 shape[1:] ）。在深度學習世界，我們會為了最大化 GPU 的運算效率而一次將 64 個、128 個或是更多個 batch_size 的句子丟入模型。習慣 batch 維度的存在是非常實際的。 接著讓我們看看 scaled dot product attention 在 TensorFlow 裡是 怎麼被實作 的： def scaled_dot_product_attention ( q , k , v , mask ): \"\"\"Calculate the attention weights. q, k, v must have matching leading dimensions. k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v. The mask has different shapes depending on its type(padding or look ahead) but it must be broadcastable for addition. Args: q: query shape == (..., seq_len_q, depth) k: key shape == (..., seq_len_k, depth) v: value shape == (..., seq_len_v, depth_v) mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None. Returns: output, attention_weights \"\"\" # 將 `q`、 `k` 做點積再 scale matmul_qk = tf . matmul ( q , k , transpose_b = True ) # (..., seq_len_q, seq_len_k) dk = tf . cast ( tf . shape ( k )[ - 1 ], tf . float32 ) # 取得 seq_k 的序列長度 scaled_attention_logits = matmul_qk / tf . math . sqrt ( dk ) # scale by sqrt(dk) # 將遮罩「加」到被丟入 softmax 前的 logits if mask is not None : scaled_attention_logits += ( mask * - 1e9 ) # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均 attention_weights = tf . nn . softmax ( scaled_attention_logits , axis =- 1 ) # (..., seq_len_q, seq_len_k) # 以注意權重對 v 做加權平均（weighted average） output = tf . matmul ( attention_weights , v ) # (..., seq_len_q, depth_v) return output , attention_weights 別被嚇到了。除了遮罩的運算部分我們還沒解釋，這 Python 函式事實上就是用 TensorFlow API 來實現剛剛才說的注意力機制邏輯罷了： 將 q 和 k 做點積得到 matmul_qk 將 matmul_qk 除以 scaling factor sqrt(dk) 有遮罩的話在丟入 softmax 前 套用 通過 softmax 取得加總為 1 的注意權重 以該權重加權平均 v 作為輸出結果 回傳輸出結果以及注意權重 扣掉註解事實上也就只有 8 行代碼（當然有很多實作細節）。現在先讓我們實際將 q , k , v 輸入此函式看看得到的結果。假設沒有遮罩的存在： mask = None output , attention_weights = scaled_dot_product_attention ( q , k , v , mask ) print ( \"output:\" , output ) print ( \"-\" * 20 ) print ( \"attention_weights:\" , attention_weights ) output: tf.Tensor( [[[0.37502408 0.37503672 0.37488326 0.49993956] [0.37513658 0.37514552 0.37500778 0.49994028] [0.37483314 0.37482613 0.3749625 0.50006175] [0.37516367 0.37501514 0.3750258 0.49997073] [0.37503195 0.3751256 0.3750621 0.49998796] [0.37512696 0.37512186 0.37502852 0.49996266] [0.3748441 0.3749599 0.37492597 0.50001484] [0.3748441 0.3749599 0.37492597 0.50001484]] [[0.62516296 0.2500847 0.6250717 0.37522966] [0.62490153 0.24994145 0.62504375 0.37497035] [0.62509674 0.2501282 0.6249581 0.37518966] [0.62518024 0.25003165 0.6250133 0.37507355] [0.6250232 0.25011832 0.62486345 0.37516582] [0.6251376 0.25018096 0.625095 0.37525034] [0.62478966 0.24995528 0.6248975 0.37490302] [0.62492853 0.24997747 0.62507135 0.37497336]]], shape=(2, 8, 4), dtype=float32) -------------------- attention_weights: tf.Tensor( [[[0.12517719 0.12502946 0.12490283 0.12493535 0.12491155 0.12497091 0.12503636 0.12503636] [0.12505189 0.12512855 0.12479477 0.1250193 0.12506542 0.12509388 0.12492308 0.12492308] [0.12497574 0.12484524 0.1252356 0.12496044 0.12489695 0.1248758 0.12510511 0.12510511] [0.12500346 0.12506503 0.1249556 0.12519364 0.12496658 0.12508455 0.12486558 0.12486558] [0.12494988 0.12508136 0.12486238 0.12493681 0.12514524 0.12506418 0.12498005 0.12498005] [0.12500885 0.12510943 0.12484082 0.12505434 0.12506378 0.12510203 0.12491038 0.12491038] [0.1250592 0.12492351 0.12505497 0.12482036 0.12496454 0.12489527 0.12514108 0.12514108] [0.1250592 0.12492351 0.12505497 0.12482036 0.12496454 0.12489527 0.12514108 0.12514108]] [[0.12514497 0.1249882 0.12503006 0.12493392 0.1250188 0.12506588 0.1248794 0.12493874] [0.1250289 0.12513264 0.12493595 0.12481083 0.12494826 0.12499319 0.12507208 0.12507817] [0.12506142 0.12492662 0.12505917 0.12498691 0.12506557 0.12506266 0.12491715 0.12492047] [0.12504192 0.12487808 0.1250636 0.12521076 0.12504579 0.12498584 0.12487733 0.12489669] [0.12504749 0.12493626 0.12506288 0.12496644 0.12510824 0.12501009 0.12496544 0.12490314] [0.12506938 0.12495602 0.1250348 0.12488137 0.12498492 0.12519602 0.12488527 0.12499221] [0.12494776 0.12509981 0.1249542 0.12483776 0.12500516 0.12495013 0.12514311 0.12506206] [0.12499588 0.12509465 0.12494626 0.12484587 0.1249316 0.12504588 0.12505081 0.12508905]]], shape=(2, 8, 8), dtype=float32) scaled_dot_product_attention 函式輸出兩個張量： output 代表注意力機制的結果 attention_weights 代表句子 q 裡頭每個子詞對句子 k 裡頭的每個子詞的注意權重 而因為你知道目前的 q 跟 k 都代表同個張量 emb_inp ，因此 attention_weights 事實上就代表了 emb_inp 裡頭每個英文序列中的子詞對其他位置的子詞的注意權重。你可以再次參考之前 Transformer 是如何做 encoding 的動畫。 output 則是句子裡頭每個位置的子詞將 attention_weights 當作權重，從其他位置的子詞對應的資訊 v 裡頭抽取有用訊息後匯總出來的結果。你可以想像 ouput 裡頭的每個子詞都獲得了一個包含自己以及周遭子詞語義資訊的新 representation。而因為現在每個字詞的注意權重都相同，最後得到的每個 repr. 都長得一樣。 下面則是我們實作的注意函式的所有輸入與輸出張量。透過多次的矩陣運算，注意力機制能讓查詢 Q 跟鍵值 K 做匹配，再依據此匹配程度將值 V 做加權平均獲得新的 representation。 您的瀏覽器不支援影片標籤，請留言通知我：S Scaled dot product attention 的實際運算過程 別只聽我碎碎念，自己點擊播放鍵來了解背後到底發生什麼事情吧！ 動畫裡包含許多細節，但只要有矩陣運算的基本概念，你應該已經能夠直觀且正確地理解注意函式是怎麼運作的了。在真實世界裡我們當然會用更長的序列、更大的 batch_size 來處理數據，但上面呈現的是程式碼的實際結果，而非示意圖而已。這是注意力機制真正的「所見即所得」。 一般來說注意函式的輸出 output 張量維度會跟 q 張量相同（假設圖上的 depth_v 等於 depth ）。此張量也被稱作「注意張量」，你可以將其解讀為 q 在關注 k 並從 v 得到上下文訊息後的所獲得的新 representation。而注意權重 attention_weights 則是 q 裡頭每個句子的每個子詞對其他位置的子詞的關注程度。 P.S. 一般注意函式只需輸出注意張量。而我們在這邊將注意權重 attention_weights 也輸出是為了方便之後觀察 Transformer 在訓練的時候將「注意力」放在哪裡。 直觀理解遮罩在注意函式中的效果 剛剛為了讓你消化注意函式裡頭最重要的核心邏輯，我刻意忽略了遮罩（masking）的存在。現在讓我們重新把 scaled_dot_product_attention 裡頭跟遮罩相關的程式碼拿出來瞧瞧： ... # 將 `q`、 `k` 做點積再 scale scaled_attention_logits = matmul_qk / tf . math . sqrt ( dk ) # 將遮罩「加」到被丟入 softmax 前的 logits if mask is not None : scaled_attention_logits += ( mask * - 1e9 ) # 取 softmax 是為了得到總和為 1 的比例做加權平均 attention_weights = tf . nn . softmax ( scaled_attention_logits , axis =- 1 ) ... 如果你剛剛有仔細看上面的動畫的話（17 秒之後），應該能想像 scaled_attention_logits 的 shape 為 （batch_size, seq_len_q, seq_len_k）。其最後一個維度代表某個序列 q 裡的某個子詞與序列 k 的 每個 子詞的匹配程度，但加總不為 1。而為了之後跟與 k 對應的 v 做加權平均，我們針對最後一個維度做 softmax 運算使其和為 1，也就是上圖 axis=-1 的部分： 您的瀏覽器不支援影片標籤，請留言通知我：S 對最後一維做 softmax。模型還沒經過訓練所以「注意力」非常平均 如果序列 k 裡頭的每個子詞 sub_k 都是實際存在的中文字或英文詞彙，這運算當然沒問題。我們會希望序列 q 裡頭的每個子詞 sub_q 都能從每個 sub_k 獲得它所需要的語義資訊。 但李組長眉頭一皺，發現案情並不單純。 回想一下，我們的 q 跟 k 都是從 emb_inp 來的。 emb_inp 代表著英文句子的詞嵌入張量，而裡頭的第一個句子應該是有 <pad> token 的。啊哈！誰會想要放注意力在沒有實際語義的傢伙上呢？ ... if mask is not None : scaled_attention_logits += ( mask * - 1e9 ) # 是 -1e9 不是 1e-9 attention_weights = tf . nn . softmax ( scaled_attention_logits , axis =- 1 ) ... 因此在注意函式裡頭，我們將遮罩乘上一個接近 負 無窮大的 -1e9 ，並把它加到進入 softmax 前 的 logits 上面。這樣可以讓這些被加上 極大負值 的位置變得無關緊要，在經過 softmax 以後的值趨近於 0。這效果等同於序列 q 中的某個子詞 sub_q 完全沒放注意力在這些被遮罩蓋住的子詞 sub_k 之上（此例中 sub_k 指是的 <pad> ）。 （動腦時間：為何遮罩要放在 softmax 之前而不能放之後？） 聽我說那麼多不如看實際的運算結果。讓我們再次為英文句子 inp 產生對應的 padding mask： def create_padding_mask ( seq ): # padding mask 的工作就是把索引序列中為 0 的位置設為 1 mask = tf . cast ( tf . equal ( seq , 0 ), tf . float32 ) return mask [:, tf . newaxis , tf . newaxis , :] # broadcasting print ( \"inp:\" , inp ) inp_mask = create_padding_mask ( inp ) print ( \"-\" * 20 ) print ( \"inp_mask:\" , inp_mask ) inp: tf.Tensor( [[8135 105 10 1304 7925 8136 0 0] [8135 17 3905 6013 12 2572 7925 8136]], shape=(2, 8), dtype=int64) -------------------- inp_mask: tf.Tensor( [[[[0. 0. 0. 0. 0. 0. 1. 1.]]] [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32) 很明顯地， 第一個 英文序列的最後 2 個位置是不具任何語義的 <pad> （圖中為 0 的部分）。而這也是為何我們需要將遮罩 inp_mask 輸入到注意函式，避免序列中的子詞關注到這 2 個傢伙的原因。 我們這次把 inp_mask 降到 3 維，並且將其跟剛剛的 q 、 k 和 v 一起丟進注意函式裡頭，看看注意權重有什麼變化： # 這次讓我們將 padding mask 放入注意函式並觀察 # 注意權重的變化 mask = tf . squeeze ( inp_mask , axis = 1 ) # (batch_size, 1, seq_len_q) _ , attention_weights = scaled_dot_product_attention ( q , k , v , mask ) print ( \"attention_weights:\" , attention_weights ) attention_weights: tf.Tensor( [[[0.16691911 0.1667221 0.16655324 0.16659662 0.1665649 0.16664404 0. 0. ] [0.16670164 0.16680385 0.1663589 0.16665822 0.16671969 0.16675764 0. 0. ] [0.16668104 0.16650699 0.16702762 0.16666064 0.16657597 0.16654775 0. 0. ] [0.16661155 0.16669361 0.16654776 0.16686502 0.16656238 0.16671962 0. 0. ] [0.16659099 0.16676629 0.16647433 0.16657357 0.16685146 0.16674338 0. 0. ] [0.16663864 0.16677272 0.16641466 0.16669929 0.16671185 0.16676286 0. 0. ] [0.16680835 0.16662736 0.1668027 0.16648975 0.16668208 0.1665897 0. 0. ] [0.16680835 0.16662736 0.1668027 0.16648975 0.16668208 0.1665897 0. 0. ]] [[0.12514497 0.1249882 0.12503006 0.12493392 0.1250188 0.12506588 0.1248794 0.12493874] [0.1250289 0.12513264 0.12493595 0.12481083 0.12494826 0.12499319 0.12507208 0.12507817] [0.12506142 0.12492662 0.12505917 0.12498691 0.12506557 0.12506266 0.12491715 0.12492047] [0.12504192 0.12487808 0.1250636 0.12521076 0.12504579 0.12498584 0.12487733 0.12489669] [0.12504749 0.12493626 0.12506288 0.12496644 0.12510824 0.12501009 0.12496544 0.12490314] [0.12506938 0.12495602 0.1250348 0.12488137 0.12498492 0.12519602 0.12488527 0.12499221] [0.12494776 0.12509981 0.1249542 0.12483776 0.12500516 0.12495013 0.12514311 0.12506206] [0.12499588 0.12509465 0.12494626 0.12484587 0.1249316 0.12504588 0.12505081 0.12508905]]], shape=(2, 8, 8), dtype=float32) 加了 padding mask 後，第一個句子裡頭的每個子詞針對倒數兩個字詞的「注意權重」的值都變成 0 了。上句話非常饒舌，但我相信已經是非常精準的說法了。讓我把這句話翻譯成 numpy 的 slice 語法： # 事實上也不完全是上句話的翻譯， # 因為我們在第一個維度還是把兩個句子都拿出來方便你比較 attention_weights [:, :, - 2 :] <tf.Tensor: id=193086, shape=(2, 8, 2), dtype=float32, numpy= array([[[0. , 0. ], [0. , 0. ], [0. , 0. ], [0. , 0. ], [0. , 0. ], [0. , 0. ], [0. , 0. ], [0. , 0. ]], [[0.1248794 , 0.12493874], [0.12507208, 0.12507817], [0.12491715, 0.12492047], [0.12487733, 0.12489669], [0.12496544, 0.12490314], [0.12488527, 0.12499221], [0.12514311, 0.12506206], [0.12505081, 0.12508905]]], dtype=float32)> 第一個英文句子的最後 2 個位置因為是 <pad> 所以被遮罩「蓋住」而沒有權重值（上方 2 維陣列）；第二個句子的序列（下方 2 維陣列）則因為最後 2 個位置仍是正常的英文子詞，因此都有被其他子詞關注。 如果聽完我的碎碎念你還是無法理解以上結果，或是不確定有遮罩的注意函式到底怎麼運作，就實際看看其中的計算過程吧！ 您的瀏覽器不支援影片標籤，請留言通知我：S 將 padding mask 應用到自注意力機制運算（q = k） 一張圖勝過千言萬語。在 padding mask 的幫助下，注意函式輸出的新序列 output 裡頭的每個子詞都只從序列 k （也就是序列 q 自己）的前 6 個實際子詞而非 <pad> 來獲得語義資訊（最後一張圖的黑框部分）。 再次提醒，因為我們輸入注意函式的 q 跟 k 都是同樣的英文詞嵌入張量 emb_inp ，事實上這邊做的就是讓英文句子裡頭的每個子詞都去關注同句子中其他位置的子詞的資訊，並從中獲得上下文語義，而這就是所謂的自注意力機制（self-attention）：序列關注自己。 當序列 q 換成 Decoder 的輸出序列而序列 k 變成 Encoder 的輸出序列時，我們就變成在計算一般 Seq2Seq 模型中的注意力機制。這點觀察非常重要，且 我們在前面就已經提過了 。 打鐵趁熱，讓我們看看前面提過的另一種遮罩 look ahead mask： # 建立一個 2 維矩陣，維度為 (size, size)， # 其遮罩為一個右上角的三角形 def create_look_ahead_mask ( size ): mask = 1 - tf . linalg . band_part ( tf . ones (( size , size )), - 1 , 0 ) return mask # (seq_len, seq_len) seq_len = emb_tar . shape [ 1 ] # 注意這次我們用中文的詞嵌入張量 `emb_tar` look_ahead_mask = create_look_ahead_mask ( seq_len ) print ( \"emb_tar:\" , emb_tar ) print ( \"-\" * 20 ) print ( \"look_ahead_mask\" , look_ahead_mask ) emb_tar: tf.Tensor( [[[-0.0441955 -0.01026772 0.03740635 0.02017349] [ 0.02129837 -0.00746276 0.03881821 -0.01586295] [-0.01179456 0.02825376 0.00738146 0.02963744] [ 0.01171205 0.04350302 -0.01190796 0.02526634] [ 0.03814722 -0.03364048 -0.03744673 0.04369817] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349]] [[-0.0441955 -0.01026772 0.03740635 0.02017349] [-0.00359621 -0.01380367 -0.02875998 -0.03855735] [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ] [ 0.04131394 -0.04065727 -0.04330624 -0.03341667] [ 0.03572228 -0.04500845 0.0470326 0.03095007] [-0.03566641 -0.03730996 -0.00597564 -0.03933349] [ 0.01850356 0.03993076 0.02729526 -0.04848848] [-0.02294568 -0.02494572 -0.0136737 -0.04278342] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902]]], shape=(2, 10, 4), dtype=float32) -------------------- look_ahead_mask tf.Tensor( [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32) 我們已經知道 demo 用的中文（目標語言）的序列長度為 10 ，而 look ahead 遮罩就是產生一個 2 維矩陣，其兩個維度都跟中文的詞嵌入張量 emb_tar 的倒數第 2 個維度（序列長度）一樣，且裡頭是一個倒三角形（1 的部分）。 我們 前面曾經說過 look_ahead_mask 是用來確保 Decoder 在進行自注意力機制時輸出序列裡頭的每個子詞只會關注到自己之前（左邊）的字詞，不會不小心關注到未來（右邊）理論上還沒被 Decoder 生成的子詞。 運用從 padding mask 學到的概念，想像一下如果把這個倒三角的遮罩跟之前一樣套用到進入 softmax 之前 的 scaled_attention_logits ，輸出序列 output 裡頭的每個子詞的 repr. 會有什麼性質？ 溫馨小提醒： scaled_attention_logits 裡頭的每一 row 紀錄了某個特定子詞對其他子詞的注意權重。 # 讓我們用目標語言（中文）的 batch # 來模擬 Decoder 處理的情況 temp_q = temp_k = emb_tar temp_v = tf . cast ( tf . math . greater ( tf . random . uniform ( shape = emb_tar . shape ), 0.5 ), tf . float32 ) # 將 look_ahead_mask 放入注意函式 _ , attention_weights = scaled_dot_product_attention ( temp_q , temp_k , temp_v , look_ahead_mask ) print ( \"attention_weights:\" , attention_weights ) attention_weights: tf.Tensor( [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0.49974996 0.50025004 0. 0. 0. 0. 0. 0. 0. 0. ] [0.33338806 0.33309633 0.3335156 0. 0. 0. 0. 0. 0. 0. ] [0.24980238 0.2497976 0.25013384 0.25026616 0. 0. 0. 0. 0. 0. ] [0.19975185 0.19982941 0.19989952 0.199991 0.20052823 0. 0. 0. 0. 0. ] [0.16658378 0.16686733 0.16656147 0.16657883 0.1664059 0.16700274 0. 0. 0. 0. ] [0.14259693 0.1427213 0.14279391 0.1429158 0.14314583 0.14267854 0.14314772 0. 0. 0. ] [0.12491751 0.12487698 0.12503591 0.12508857 0.12503389 0.12487747 0.12507991 0.12508978 0. 0. ] [0.11102892 0.1109929 0.11113416 0.11118097 0.11113235 0.11099333 0.11117328 0.11118205 0.11118205 0. ] [0.09991965 0.09988723 0.10001437 0.10005648 0.10001273 0.09988762 0.10004956 0.10005745 0.10005745 0.10005745]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0.4994912 0.5005088 0. 0. 0. 0. 0. 0. 0. 0. ] [0.33261845 0.33340293 0.3339786 0. 0. 0. 0. 0. 0. 0. ] [0.24919374 0.25002357 0.25033304 0.25044966 0. 0. 0. 0. 0. 0. ] [0.19997214 0.19964042 0.20002526 0.19986893 0.20049322 0. 0. 0. 0. 0. ] [0.16662474 0.16674054 0.16659829 0.16668092 0.16645522 0.16690029 0. 0. 0. 0. ] [0.14276491 0.14288287 0.14274995 0.14281946 0.1427529 0.14282054 0.14320944 0. 0. 0. ] [0.12491003 0.1250709 0.12497466 0.12504703 0.12481265 0.1251362 0.12493407 0.12511446 0. 0. ] [0.11102156 0.11105824 0.11103692 0.11106326 0.11112017 0.11104742 0.11128615 0.11106552 0.11130078 0. ] [0.09983386 0.10001399 0.10016464 0.10015456 0.09999382 0.09989963 0.0998925 0.09993652 0.099891 0.10021948]]], shape=(2, 10, 10), dtype=float32) 答案呼之欲出，套用 look ahead mask 的結果就是讓序列 q 裡的每個字詞只關注包含自己左側的子詞，在自己之後的位置的字詞都不看。比方說兩個中文句子的第一個字詞都只關注自己： attention_weights [:, 0 , :] <tf.Tensor: id=193126, shape=(2, 10), dtype=float32, numpy= array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)> 注意到了嗎？兩個句子的第一個子詞因為自己前面已經沒有其他子詞，所以將全部的注意力 1 都放在自己身上。讓我們看看第二個子詞： attention_weights [:, 1 , :] <tf.Tensor: id=193131, shape=(2, 10), dtype=float32, numpy= array([[0.49974996, 0.50025004, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0.4994912 , 0.5005088 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32)> 兩個句子的第 2 個子詞因為只能看到序列中的第一個子詞以及自己，因此前兩個位置的注意權重加總即為 1，後面位置的權重皆為 0。而現在 2 個值都接近 0.5 是因為我們還沒開始訓練，Transformer 還不知道該把注意力放在哪裡。 就跟一般的 Seq2Seq 模型 相同，Transformer 裡頭的 Decoder 在生成輸出序列時也是一次產生一個子詞。因此跟輸入的英文句子不同，中文句子裡頭的每個子詞都是在不同時間點產生的。所以理論上 Decoder 在時間點 t - 1 （或者說是位置 t - 1 ）已經生成的子詞 subword_t_minus_1 在生成的時候是不可能能夠關注到下個時間點 t （位置 t ）所生成的子詞 subword_t 的，儘管它們在 Transformer 裡頭同時被做矩陣運算。 一個位置的子詞不能去關注未來會在自己之後生成的子詞，而這就像是 祖父悖論 一樣有趣。 實際上 look ahead mask 讓 Decoder 在生成第 1 個子詞時只看自己；在生成第 2 個子詞時關注前 1 個子詞與自己； 在生成第 3 個子詞時關注前兩個已經生成的子詞以及自己，以此類推。透過 look ahead mask，你可以想像 Transformer 既可以平行運算，又可以像是 RNN 一樣，在生成子詞時從前面已生成的子詞獲得必要的語義資訊。 挺酷的，不是嗎？ 您的瀏覽器不支援影片標籤，請留言通知我：S look ahead mask 讓每個子詞都只關注序列中自己與之前的位置 在實際做矩陣運算的時候我們當然還是會讓注意權重為 0 的位置跟對應的 v 相乘，但是上圖的黑框才是實際會對最後的 output 值造成影響的權重與 v 。 我們在這節了解 Transformer 架構裡頭的兩種遮罩以及它們的作用： padding mask：遮住 <pad> token 不讓所有子詞關注 look ahead mask：遮住 Decoder 未來生成的子詞不讓之前的子詞關注 你現在應該能夠想像遮罩在注意力機制裡頭顯得有多麽重要了：它讓注意函式進行高效率的矩陣平行運算的時候不需擔心會關注到不該關注的位置，一次獲得序列中所有位置的子詞各自應有的注意權重以及新的 reprsentation。 如果 Transformer 是 變形金剛 的話，注意力機制跟遮罩就是 火種源 了。 Multi-head attention：你看你的，我看我的 有好好聽教授講解 Transformer 的話，你應該還記得所謂的多頭注意（multi-head attention）概念。如果你到現在還沒看課程影片或者想複習一下，我把 multi-head attention 的開始跟結束時間都設置好了，你只需觀看大約 1 分半左右的影片： 李宏毅教授講解 multi-head attention 的概念 複習完了嗎？mutli-head attention 的概念本身並不難，用比較正式的說法就是將 Q、K 以及 V 這三個張量先 個別 轉換到 d_model 維空間，再將其拆成多個比較低維的 depth 維度 N 次以後，將這些產生的小 q、小 k 以及小 v 分別丟入前面的注意函式得到 N 個結果。接著將這 N 個 heads 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出 而為何要那麼「搞剛」把本來 d_model 維的空間投影到多個維度較小的子空間（subspace）以後才各自進行注意力機制呢？這是因為這給予模型更大的彈性，讓它可以同時關注不同位置的子詞在不同子空間下的 representation，而不只是本來 d_model 維度下的一個 representation。 我們在文章最開頭看過的英翻中就是一個活生生的 mutli-head attention 例子： 在經過 前面 2 節注意函式 的洗禮之後，你應該已經能夠看出這裏每張小圖都是一個注意權重（為了方便渲染我做了 transpose）。而事實上每張小圖都是 multi-head attention 裡頭某一個 head 的結果，總共是 8 個 heads。 你會發現每個 head 在 Transformer 生成同樣的中文字時關注的英文子詞有所差異： Head 4 在生成「們」與「再」時特別關注「renewed」 Head 5 在生成「必」與「須」時特別關注「must」 Head 6 & 8 在生成「希」與「望」時特別關注「hope」 透過這樣同時關注多個不同子空間裡頭的子詞的 representation，Transformer 最終可以生成更好的結果。 話是這麼說，但程式碼該怎麼寫呢？ 為了要實現 multi-head attention，得先能把一個 head 變成多個 heads。而這實際上就是把一個 d_model 維度的向量「折」成 num_heads 個 depth 維向量，使得： num_heads * depth = d_model 讓我們實作一個可以做到這件事情的函式，並將英文詞嵌入張量 emb_inp 實際丟進去看看： def split_heads ( x , d_model , num_heads ): # x.shape: (batch_size, seq_len, d_model) batch_size = tf . shape ( x )[ 0 ] # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度 assert d_model % num_heads == 0 depth = d_model // num_heads # 這是分成多頭以後每個向量的維度 # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。 # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維 # (batch_size, seq_len, num_heads, depth) reshaped_x = tf . reshape ( x , shape = ( batch_size , - 1 , num_heads , depth )) # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量 # (batch_size, num_heads, seq_len, depth) output = tf . transpose ( reshaped_x , perm = [ 0 , 2 , 1 , 3 ]) return output # 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量 d_model = 4 # 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣 num_heads = 2 x = emb_inp output = split_heads ( x , d_model , num_heads ) print ( \"x:\" , x ) print ( \"output:\" , output ) x: tf.Tensor( [[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.02707888 -0.03917687 -0.01213828 0.00909697] [ 0.0355427 0.04111305 0.00751223 -0.01974255] [ 0.02443342 -0.03273199 0.01267544 0.03127003] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ] [-0.00680635 0.02136201 -0.02036932 -0.04211974] [-0.00680635 0.02136201 -0.02036932 -0.04211974]] [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.0325227 -0.03433502 -0.01849879 0.01439226] [ 0.00144588 -0.00377025 -0.00798036 -0.04099905] [ 0.04524285 0.02524642 -0.00924555 -0.01368124] [-0.0159062 0.01108797 -0.0177028 -0.0435766 ] [ 0.00240784 -0.04652226 0.01821991 -0.04349295] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ]]], shape=(2, 8, 4), dtype=float32) output: tf.Tensor( [[[[ 0.00695511 -0.03370368] [-0.02707888 -0.03917687] [ 0.0355427 0.04111305] [ 0.02443342 -0.03273199] [-0.04879753 -0.00119017] [-0.02148524 -0.03413673] [-0.00680635 0.02136201] [-0.00680635 0.02136201]] [[-0.03656032 -0.03336458] [-0.01213828 0.00909697] [ 0.00751223 -0.01974255] [ 0.01267544 0.03127003] [-0.00157104 0.01117355] [ 0.00708324 0.0121879 ] [-0.02036932 -0.04211974] [-0.02036932 -0.04211974]]] [[[ 0.00695511 -0.03370368] [-0.0325227 -0.03433502] [ 0.00144588 -0.00377025] [ 0.04524285 0.02524642] [-0.0159062 0.01108797] [ 0.00240784 -0.04652226] [-0.04879753 -0.00119017] [-0.02148524 -0.03413673]] [[-0.03656032 -0.03336458] [-0.01849879 0.01439226] [-0.00798036 -0.04099905] [-0.00924555 -0.01368124] [-0.0177028 -0.0435766 ] [ 0.01821991 -0.04349295] [-0.00157104 0.01117355] [ 0.00708324 0.0121879 ]]]], shape=(2, 2, 8, 2), dtype=float32) 觀察 output 與 emb_inp 之間的關係，你會發現 3 維詞嵌入張量 emb_inp 已經被轉換成一個 4 維張量了，且最後一個維度 shape[-1] = 4 被拆成兩半。 不過如果你不熟 TensorFlow API 或是矩陣運算，或許無法馬上理解 head 的維度在哪裡、還有不同 heads 之間有什麼差異。為了幫助你直觀理解 split_heads 函式，我將運算過程中產生的張量都視覺化出來給你瞧瞧： 您的瀏覽器不支援影片標籤，請留言通知我：S split_heads 函式將 3 維張量轉換為 multi-head 的 4 維張量過程 觀察 split_heads 的輸入輸出，你會發現序列裡每個子詞原來為 d_model 維的 reprsentation 被拆成多個相同但較短的 depth 維度。而每個 head 的 2 維矩陣事實上仍然代表原來的序列，只是裡頭子詞的 repr. 維度降低了。 透過動畫，你現在應該已經能夠了解要產生 multi-head 就是將輸入張量中本來是 d_model 的最後一個維度平均地「折」成想要的 head 數，進而產生一個新的 head 維度。一個句子裡頭的子詞現在不只會有一個 d_model 的 repr.，而是會有 num_heads 個 depth 維度的 representation。 接下來只要把 3 維的 Q、K 以及 V 用 split_heads 拆成多個 heads 的 4 維張量，利用 broadcasting 就能以之前定義的 Scaled dot product attention 來為每個句子裡頭的每個 head 平行計算注意結果了，超有效率！ 在明白如何產生 multi-head 的 4 維張量以後，multi-head attention 的實現就比較容易理解了： # 實作一個執行多頭注意力機制的 keras layer # 在初始的時候指定輸出維度 `d_model` & `num_heads， # 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask` # 輸出跟 scaled_dot_product_attention 函式一樣有兩個： # output.shape == (batch_size, seq_len_q, d_model) # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) class MultiHeadAttention ( tf . keras . layers . Layer ): # 在初始的時候建立一些必要參數 def __init__ ( self , d_model , num_heads ): super ( MultiHeadAttention , self ) . __init__ () self . num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads self . d_model = d_model # 在 split_heads 之前的基底維度 assert d_model % self . num_heads == 0 # 前面看過，要確保可以平分 self . depth = d_model // self . num_heads # 每個 head 裡子詞的新的 repr. 維度 self . wq = tf . keras . layers . Dense ( d_model ) # 分別給 q, k, v 的 3 個線性轉換 self . wk = tf . keras . layers . Dense ( d_model ) # 注意我們並沒有指定 activation func self . wv = tf . keras . layers . Dense ( d_model ) self . dense = tf . keras . layers . Dense ( d_model ) # 多 heads 串接後通過的線性轉換 # 這跟我們前面看過的函式有 87% 相似 def split_heads ( self , x , batch_size ): \"\"\"Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) \"\"\" x = tf . reshape ( x , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( x , perm = [ 0 , 2 , 1 , 3 ]) # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致） def call ( self , v , k , q , mask ): batch_size = tf . shape ( q )[ 0 ] # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間 q = self . wq ( q ) # (batch_size, seq_len, d_model) k = self . wk ( k ) # (batch_size, seq_len, d_model) v = self . wv ( v ) # (batch_size, seq_len, d_model) # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度 q = self . split_heads ( q , batch_size ) # (batch_size, num_heads, seq_len_q, depth) k = self . split_heads ( k , batch_size ) # (batch_size, num_heads, seq_len_k, depth) v = self . split_heads ( v , batch_size ) # (batch_size, num_heads, seq_len_v, depth) # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制 # 輸出會多一個 head 維度 scaled_attention , attention_weights = scaled_dot_product_attention ( q , k , v , mask ) # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth) # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度 scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ]) # (batch_size, seq_len_q, num_heads, depth) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) # (batch_size, seq_len_q, d_model) # 通過最後一個線性轉換 output = self . dense ( concat_attention ) # (batch_size, seq_len_q, d_model) return output , attention_weights 是的，就算你有自己實作過 keras layer，multi-head attention layer 也不算短的實作。如果這是你第一次碰到客製化的 keras layer，別打退堂鼓，你可以多看幾次我寫給你的註解，或是參考等等下方的動畫來加深對 multi-head attention 的理解。 split_heads 函式我們在前面就已經看過了，你應該還有印象。 call 函式則定義了這個 multi-head attention layer 實際的計算流程，而這流程跟我在本節開頭講的可以說是有 87% 相似： 將 Q、K 以及 V 這三個張量先個別轉換到 d_model 維空間，再將其拆成多個比較低維的 depth 維度 N 次以後，將這些產生的小 q、小 k 以及小 v 分別丟入前面的注意函式得到 N 個結果。接著將這 N 個 heads 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出 差別只在於實際上我們是利用矩陣運算以及 broadcasting 讓 GPU 一次計算整個 batch 裡所有句子的所有 head 的注意結果。 定義了一個新 layer 當然要實際試試。現在讓我們初始一個 multi-head attention layer 並將英文詞嵌入向量 emb_inp 輸入進去看看： # emb_inp.shape == (batch_size, seq_len, d_model) # == (2, 8, 4) assert d_model == emb_inp . shape [ - 1 ] == 4 num_heads = 2 print ( f \"d_model: {d_model} \" ) print ( f \"num_heads: {num_heads} \\n \" ) # 初始化一個 multi-head attention layer mha = MultiHeadAttention ( d_model , num_heads ) # 簡單將 v, k, q 都設置為 `emb_inp` # 順便看看 padding mask 的作用。 # 別忘記，第一個英文序列的最後兩個 tokens 是 <pad> v = k = q = emb_inp padding_mask = create_padding_mask ( inp ) print ( \"q.shape:\" , q . shape ) print ( \"k.shape:\" , k . shape ) print ( \"v.shape:\" , v . shape ) print ( \"padding_mask.shape:\" , padding_mask . shape ) output , attention_weights = mha ( v , k , q , mask ) print ( \"output.shape:\" , output . shape ) print ( \"attention_weights.shape:\" , attention_weights . shape ) print ( \" \\n output:\" , output ) d_model: 4 num_heads: 2 q.shape: (2, 8, 4) k.shape: (2, 8, 4) v.shape: (2, 8, 4) padding_mask.shape: (2, 1, 1, 8) output.shape: (2, 8, 4) attention_weights.shape: (2, 2, 8, 8) output: tf.Tensor( [[[ 0.00862424 0.00463534 0.00123856 0.01982255] [ 0.00860434 0.00464583 0.00125165 0.01984711] [ 0.00863869 0.00461318 0.00122942 0.01981261] [ 0.00858585 0.00465442 0.00125683 0.0198578 ] [ 0.0086211 0.00462923 0.0012448 0.01983759] [ 0.00860078 0.00464716 0.00125472 0.01985404] [ 0.00865074 0.00461071 0.00122681 0.01980557] [ 0.00865074 0.00461071 0.00122681 0.01980557]] [[-0.00233657 0.02963993 0.01171194 0.03959805] [-0.00234752 0.02964369 0.01171828 0.03960991] [-0.00232748 0.02962957 0.01170804 0.03959192] [-0.00233163 0.02963142 0.0117076 0.03959151] [-0.00231678 0.02962143 0.01170276 0.03957902] [-0.00234718 0.02964409 0.01171941 0.03961902] [-0.00233476 0.029631 0.01171241 0.03959794] [-0.00235306 0.02964601 0.01172148 0.03961948]]], shape=(2, 8, 4), dtype=float32) 你現在應該明白為何 我們當初要在 padding mask 加入兩個新維度了 ：一個是用來遮住同個句子但是不同 head 的注意權重，一個則是用來 broadcast 到 2 維注意權重的（詳見 直觀理解遮罩 一節）。 沒意外的話你也已經能夠解讀 mutli-head attention 的輸出了： output ：序列中每個子詞的新 repr. 都包含同序列其他位置的資訊 attention_weights ：包含每個 head 的每個序列 q 中的字詞對序列 k 的注意權重 如果你還無法想像每個計算步驟，讓我們看看 multi-head attention 是怎麼將輸入的 q 、 k 以及 v 轉換成最後的 output 的： 您的瀏覽器不支援影片標籤，請留言通知我：S Multi-head attention 完整計算過程 這應該是你這輩子第一次也可能是唯一一次有機會看到 multi-head 注意力機制是怎麼處理 4 維張量的。 細節不少，我建議將動畫跟程式碼比較一下，確保你能想像每一個步驟產生的張量以及其物理意義。到此為止，我們已經把 Transformer 裡最困難的 multi-head attention 的概念以及運算都看過一遍了。 如果你腦袋還是一團亂，只要記得最後一個畫面：在 q 、 k 以及 v 的最後一維已經是 d_model 的情況下，multi-head attention 跟 scaled dot product attention 一樣，就是吐出一個完全一樣維度的張量 output 。 multi-head attention 的輸出張量 output 裡頭每個句子的每個字詞的 repr. 維度 d_model 雖然跟函式的輸入張量相同，但實際上已經是從同個序列中 不同位置且不同空間 中的 repr. 取得語義資訊的結果。 要確保自己真的掌握了 multi-head attention 的精神，你可以試著向旁邊的朋友（如果他 / 她願意聽的話）解釋一下整個流程。 喔對了，不用擔心我們做 multi-head 以後計算量會大增。因為 head 的數目雖然變多了，每個子空間的維度也下降了。跟 single-head attention 使用的計算量是差不多的。 打造 Transformer：疊疊樂時間 以前我們曾提到 深度學習模型就是一層層的幾何運算過程。Transformer 也不例外，剛才實作的 mutli-head attention layer 就是一個最明顯的例子。而它正好是 Transformer 裡頭最重要的一層運算。 在這節我們會把 Transformer 裡頭除了注意力機制的其他運算通通實作成一個個的 layers，並將它們全部「疊」起來。 你可以點擊下方的影片來了解接下來的實作順序： 您的瀏覽器不支援影片標籤，請留言通知我：S 一步步打造 Transformer 如果這是你第一次看到 Transformer 的架構圖 ... 代表你沒認真上教授的課，等等別忘記 去前面領補課號碼牌 。 影片中左側就是我們接下來會依序實作的 layers。 Transformer 是一種使用自注意力機制的 Seq2Seq 模型 ，裡頭包含了兩個重要角色，分別為 Encoder 與 Decoder： 最初輸入的英文序列會通過 Encoder 中 N 個 Encoder layers 並被轉換成一個 相同長度 的序列。每個 layer 都會為自己的輸入序列裡頭的子詞產生 新的 repr.，然後交給下一個 layer。 Decoder 在生成（預測）下一個中文子詞時會一邊觀察 Encoder 輸出序列裡 所有 英文子詞的 repr.，一邊觀察自己前面已經生成的中文子詞。 值得一提的是，N = 1 （Encoder / Decoder layer 數目 = 1）時就是最陽春版的 Transformer。但在深度學習領域裡頭我們常常想對原始數據做多層的轉換，因此會將 N 設為影片最後出現的 2 層或是 Transformer 論文中的 6 層 Encoder / Decoder layers。 Encoder 裡頭的 Encoder layer 裡又分兩個 sub-layers，而 Decoder 底下的 Decoder layer 則包含 3 個 sub-layers。真的是 layer layer 相扣。將這些 layers 的階層關係簡單列出來大概就長這樣（位置 Encoding 等實作時解釋）： Transformer Encoder 輸入 Embedding 位置 Encoding N 個 Encoder layers sub-layer 1: Encoder 自注意力機制 sub-layer 2: Feed Forward Decoder 輸出 Embedding 位置 Encoding N 個 Decoder layers sub-layer 1: Decoder 自注意力機制 sub-layer 2: Decoder-Encoder 注意力機制 sub-layer 3: Feed Forward Final Dense Layer 不過就像影片中顯示的一樣，實作的時候我們傾向從下往上疊上去。畢竟地基打得好，樓才蓋得高，對吧？ Position-wise Feed-Forward Networks 如同影片中所看到的， Encoder layer 跟 Decoder layer 裡頭都各自有一個 Feed Forward 的元件。此元件構造簡單，不用像前面的 multi-head attention 建立 客製化的 keras layer ，只需要寫一個 Python 函式讓它在被呼叫的時候回傳一個 新的 tf.keras.Sequential 模型 給我們即可： # 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件 def point_wise_feed_forward_network ( d_model , dff ): # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func return tf . keras . Sequential ([ tf . keras . layers . Dense ( dff , activation = 'relu' ), # (batch_size, seq_len, dff) tf . keras . layers . Dense ( d_model ) # (batch_size, seq_len, d_model) ]) 此函式在每次被呼叫的時候都會回傳一組新的全連接前饋神經網路（Fully-connected F eed F orward N etwork，FFN），其輸入張量與輸出張量的最後一個維度皆為 d_model ，而在 FFN 中間層的維度則為 dff 。一般會讓 dff 大於 d_model ，讓 FFN 從輸入的 d_model 維度裡頭擷取些有用的資訊。在論文中 d_model 為 512， dff 則為 4 倍的 2048。兩個都是可以調整的超參數。 讓我們建立一個 FFN 試試： batch_size = 64 seq_len = 10 d_model = 512 dff = 2048 x = tf . random . uniform (( batch_size , seq_len , d_model )) ffn = point_wise_feed_forward_network ( d_model , dff ) out = ffn ( x ) print ( \"x.shape:\" , x . shape ) print ( \"out.shape:\" , out . shape ) x.shape: (64, 10, 512) out.shape: (64, 10, 512) 在輸入張量的最後一維已經是 d_model 的情況，FFN 的輸出張量基本上會跟輸入一模一樣： 輸入：（batch_size, seq_len, d_model） 輸出：（batch_size, seq_len, d_model） FFN 輸出 / 輸入張量的 shape 相同很容易理解。比較沒那麼明顯的是這個 FFN 事實上對序列中的所有位置做的線性轉換都是一樣的。我們可以假想一個 2 維的 duumy_sentence ，裡頭有 5 個以 4 維向量表示的子詞： d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model` dff = 6 # 建立一個小 FFN small_ffn = point_wise_feed_forward_network ( d_model , dff ) # 懂子詞梗的站出來 dummy_sentence = tf . constant ([[ 5 , 5 , 6 , 6 ], [ 5 , 5 , 6 , 6 ], [ 9 , 5 , 2 , 7 ], [ 9 , 5 , 2 , 7 ], [ 9 , 5 , 2 , 7 ]], dtype = tf . float32 ) small_ffn ( dummy_sentence ) <tf.Tensor: id=193585, shape=(5, 4), dtype=float32, numpy= array([[ 2.8674245, -2.174698 , -1.3073452, -6.4233937], [ 2.8674245, -2.174698 , -1.3073452, -6.4233937], [ 3.650207 , -0.973258 , -2.4126565, -6.5094995], [ 3.650207 , -0.973258 , -2.4126565, -6.5094995], [ 3.650207 , -0.973258 , -2.4126565, -6.5094995]], dtype=float32)> 你會發現同一個子詞不會因為 位置的改變 而造成 FFN 的輸出結果產生差異。但因為我們實際上會有多個 Encoder / Decoder layers，而每個 layers 都會有不同參數的 FFN，因此每個 layer 裡頭的 FFN 做的轉換都會有所不同。 值得一提的是，儘管對所有位置的子詞都做一樣的轉換，這個轉換是獨立進行的，因此被稱作 Position-wise Feed-Forward Networks。 Encoder layer：Encoder 小弟 有了 M ulti- H ead A ttention（MHA）以及 F eed- F orward N etwork（FFN），我們事實上已經可以實作第一個 Encoder layer 了。讓我們複習一下這 layer 裡頭有什麼重要元件： 您的瀏覽器不支援影片標籤，請留言通知我：S Encoder layer 裡的重要元件 我想上面的動畫已經很清楚了。一個 Encoder layer 裡頭會有兩個 sub-layers，分別為 MHA 以及 FFN。在 Add & Norm 步驟裡頭，每個 sub-layer 會有一個 殘差連結（residual connection） 來幫助減緩梯度消失（Gradient Vanishing）的問題。接著兩個 sub-layers 都會針對最後一維 d_model 做 layer normalization ，將 batch 裡頭每個子詞的輸出獨立做轉換，使其平均與標準差分別靠近 0 和 1 之後輸出。 另外在將 sub-layer 的輸出與其輸入相加之前，我們還會做點 regularization，對該 sub-layer 的輸出使用 dropout 。 總結一下。如果輸入是 x ，最後輸出寫作 out 的話，則每個 sub-layer 的處理邏輯如下： sub_layer_out = Sublayer(x) sub_layer_out = Dropout(sub_layer_out) out = LayerNorm(x + sub_layer_out) Sublayer 則可以是 MHA 或是 FFN。現在讓我們看看 Encoder layer 的實作： # Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN class EncoderLayer ( tf . keras . layers . Layer ): # Transformer 論文內預設 dropout rate 為 0.1 def __init__ ( self , d_model , num_heads , dff , rate = 0.1 ): super ( EncoderLayer , self ) . __init__ () self . mha = MultiHeadAttention ( d_model , num_heads ) self . ffn = point_wise_feed_forward_network ( d_model , dff ) # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-6 ) # 一樣，一個 sub-layer 一個 dropout layer self . dropout1 = tf . keras . layers . Dropout ( rate ) self . dropout2 = tf . keras . layers . Dropout ( rate ) # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同 def call ( self , x , training , mask ): # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model) # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len) # sub-layer 1: MHA # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己 # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token attn_output , attn = self . mha ( x , x , x , mask ) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( x + attn_output ) # sub-layer 2: FFN ffn_output = self . ffn ( out1 ) ffn_output = self . dropout2 ( ffn_output , training = training ) # 記得 training out2 = self . layernorm2 ( out1 + ffn_output ) return out2 跟當初 MHA layer 的實作比起來輕鬆多了，對吧？ 基本上 Encoder layer 裡頭就是兩個架構一模一樣的 sub-layer，只差在一個是 MHA，一個是 FFN。另外為了方便 residual connection 的計算，所有 sub-layers 的 輸出 維度都是 d_model 。而 sub-layer 內部產生的維度當然就隨我們開心啦！我們可以為 FFN 設置不同的 dff 值，也能設定不同的 num_heads 來改變 MHA 內部每個 head 裡頭的維度。 論文裡頭的 d_model 為 512，而我們 demo 用的英文詞嵌入張量的 d_model 維度則為 4： # 之後可以調的超參數。這邊為了 demo 設小一點 d_model = 4 num_heads = 2 dff = 8 # 新建一個使用上述參數的 Encoder Layer enc_layer = EncoderLayer ( d_model , num_heads , dff ) padding_mask = create_padding_mask ( inp ) # 建立一個當前輸入 batch 使用的 padding mask enc_out = enc_layer ( emb_inp , training = False , mask = padding_mask ) # (batch_size, seq_len, d_model) print ( \"inp:\" , inp ) print ( \"-\" * 20 ) print ( \"padding_mask:\" , padding_mask ) print ( \"-\" * 20 ) print ( \"emb_inp:\" , emb_inp ) print ( \"-\" * 20 ) print ( \"enc_out:\" , enc_out ) assert emb_inp . shape == enc_out . shape inp: tf.Tensor( [[8135 105 10 1304 7925 8136 0 0] [8135 17 3905 6013 12 2572 7925 8136]], shape=(2, 8), dtype=int64) -------------------- padding_mask: tf.Tensor( [[[[0. 0. 0. 0. 0. 0. 1. 1.]]] [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32) -------------------- emb_inp: tf.Tensor( [[[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.02707888 -0.03917687 -0.01213828 0.00909697] [ 0.0355427 0.04111305 0.00751223 -0.01974255] [ 0.02443342 -0.03273199 0.01267544 0.03127003] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ] [-0.00680635 0.02136201 -0.02036932 -0.04211974] [-0.00680635 0.02136201 -0.02036932 -0.04211974]] [[ 0.00695511 -0.03370368 -0.03656032 -0.03336458] [-0.0325227 -0.03433502 -0.01849879 0.01439226] [ 0.00144588 -0.00377025 -0.00798036 -0.04099905] [ 0.04524285 0.02524642 -0.00924555 -0.01368124] [-0.0159062 0.01108797 -0.0177028 -0.0435766 ] [ 0.00240784 -0.04652226 0.01821991 -0.04349295] [-0.04879753 -0.00119017 -0.00157104 0.01117355] [-0.02148524 -0.03413673 0.00708324 0.0121879 ]]], shape=(2, 8, 4), dtype=float32) -------------------- enc_out: tf.Tensor( [[[ 1.2521563 0.3273945 -1.5237452 -0.0558054 ] [-1.0591918 -0.42765176 -0.14816867 1.6350121 ] [ 0.299005 1.3632457 -1.4101827 -0.252068 ] [ 0.7023785 -1.479373 -0.32433346 1.1013279 ] [-1.6220697 1.0153029 0.02592759 0.5808392 ] [-1.0757908 -0.7200314 0.30136684 1.4944555 ] [-0.22072682 1.5675467 -1.215218 -0.1316019 ] [-0.22072682 1.5675467 -1.215218 -0.1316019 ]] [[ 1.475371 0.30539253 -1.1591307 -0.6216327 ] [-1.4569639 0.00421676 0.08528362 1.3674635 ] [ 0.61611307 1.3085197 -0.79488575 -1.1297472 ] [ 0.80156547 0.9995991 -1.5072922 -0.29387245] [-0.11611538 1.6353902 -1.0406278 -0.47864679] [ 0.9602699 -0.3459822 0.8696089 -1.4838965 ] [-1.6676238 0.9936579 0.2892594 0.38470644] [-1.2698565 -0.67637944 1.1073651 0.8388707 ]]], shape=(2, 8, 4), dtype=float32) 在本來的輸入維度即為 d_model 的情況下，Encoder layer 就是給我們一個一模一樣 shape 的張量。當然，實際上內部透過 MHA 以及 FFN sub-layer 的轉換，每個子詞的 repr. 都大幅改變了。 有了 Encoder layer，接著讓我們看看 Decoder layer 的實作。 Decoder layer：Decoder 小弟 一個 Decoder layer 裡頭有 3 個 sub-layers： Decoder layer 自身的 Masked MHA 1 Decoder layer 關注 Encoder 輸出序列的 MHA 2 FFN 你也可以看一下影片來回顧它們所在的位置： 您的瀏覽器不支援影片標籤，請留言通知我：S Decoder layer 中的 sub-layers 跟實作 Encoder layer 時一樣，每個 sub-layer 的邏輯同下： sub_layer_out = Sublayer(x) sub_layer_out = Dropout(sub_layer_out) out = LayerNorm(x + sub_layer_out) Decoder layer 用 MHA 1 來關注輸出序列，查詢 Q、鍵值 K 以及值 V 都是自己。而之所以有個 masked 是因為（中文）輸出序列除了跟（英文）輸入序列一樣需要 padding mask 以外，還需要 look ahead mask 來避免 Decoder layer 關注到未來的子詞。look ahead mask 在 前面章節 已經有詳細說明了。 MHA1 處理完的輸出序列會成為 MHA 2 的 Q，而 K 與 V 則使用 Encoder 的輸出序列。這個運算的概念是讓一個 Decoder layer 在生成新的中文子詞時先參考先前已經產生的中文字，並為當下要生成的子詞產生一個包含前文語義的 repr. 。接著將此 repr. 拿去跟 Encoder 那邊的英文序列做匹配，看當下字詞的 repr. 有多好並予以修正。 用簡單點的說法就是 Decoder 在生成中文字詞時除了參考已經生成的中文字以外，也會去關注 Encoder 輸出的英文子詞（的 repr.）。 # Decoder 裡頭會有 N 個 DecoderLayer， # 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN class DecoderLayer ( tf . keras . layers . Layer ): def __init__ ( self , d_model , num_heads , dff , rate = 0.1 ): super ( DecoderLayer , self ) . __init__ () # 3 個 sub-layers 的主角們 self . mha1 = MultiHeadAttention ( d_model , num_heads ) self . mha2 = MultiHeadAttention ( d_model , num_heads ) self . ffn = point_wise_feed_forward_network ( d_model , dff ) # 定義每個 sub-layer 用的 LayerNorm self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-6 ) self . layernorm3 = tf . keras . layers . LayerNormalization ( epsilon = 1e-6 ) # 定義每個 sub-layer 用的 Dropout self . dropout1 = tf . keras . layers . Dropout ( rate ) self . dropout2 = tf . keras . layers . Dropout ( rate ) self . dropout3 = tf . keras . layers . Dropout ( rate ) def call ( self , x , enc_output , training , combined_mask , inp_padding_mask ): # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model) # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model) # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len) # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len) # sub-layer 1: Decoder layer 自己對輸出序列做注意力。 # 我們同時需要 look ahead mask 以及輸出序列的 padding mask # 來避免前面已生成的子詞關注到未來的子詞以及 <pad> attn1 , attn_weights_block1 = self . mha1 ( x , x , x , combined_mask ) attn1 = self . dropout1 ( attn1 , training = training ) out1 = self . layernorm1 ( attn1 + x ) # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出 # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad> attn2 , attn_weights_block2 = self . mha2 ( enc_output , enc_output , out1 , inp_padding_mask ) # (batch_size, target_seq_len, d_model) attn2 = self . dropout2 ( attn2 , training = training ) out2 = self . layernorm2 ( attn2 + out1 ) # (batch_size, target_seq_len, d_model) # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣 ffn_output = self . ffn ( out2 ) # (batch_size, target_seq_len, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + out2 ) # (batch_size, target_seq_len, d_model) # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況 return out3 , attn_weights_block1 , attn_weights_block2 Decoder layer 的實作跟 Encoder layer 大同小異，不過還是有幾點細節特別需要注意： 在做 Masked MHA（MHA 1）的時候我們需要同時套用兩種遮罩： 輸出 序列的 padding mask 以及 look ahead mask。因此 Decoder layer 預期的遮罩是兩者結合的 combined_mask MHA 1 因為是 Decoder layer 關注自己，multi-head attention 的參數 v 、 k 以及 q 都是 x MHA 2 是 Decoder layer 關注 Encoder 輸出序列，因此，multi-head attention 的參數 v 、 k 為 enc_output ， q 則為 MHA 1 sub-layer 的結果 out1 產生 comined_mask 也很簡單，我們只要把兩個遮罩取大的即可： tar_padding_mask = create_padding_mask ( tar ) look_ahead_mask = create_look_ahead_mask ( tar . shape [ - 1 ]) combined_mask = tf . maximum ( tar_padding_mask , look_ahead_mask ) print ( \"tar:\" , tar ) print ( \"-\" * 20 ) print ( \"tar_padding_mask:\" , tar_padding_mask ) print ( \"-\" * 20 ) print ( \"look_ahead_mask:\" , look_ahead_mask ) print ( \"-\" * 20 ) print ( \"combined_mask:\" , combined_mask ) tar: tf.Tensor( [[4201 10 241 80 27 3 4202 0 0 0] [4201 162 467 421 189 14 7 553 3 4202]], shape=(2, 10), dtype=int64) -------------------- tar_padding_mask: tf.Tensor( [[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]] [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32) -------------------- look_ahead_mask: tf.Tensor( [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32) -------------------- combined_mask: tf.Tensor( [[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]] [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32) 注意 combined_mask 的 shape 以及裡頭遮罩所在的位置。利用 broadcasting 我們將 combined_mask 的 shape 也擴充到 4 維： (batch_size, num_heads, seq_len_tar, seq_len_tar) = (2, 1, 10, 10) 這方便之後 multi-head attention 的計算。另外因為我們 demo 的中文 batch 裡頭的第一個句子有 <pad> ， combined_mask 除了 look ahead 的效果以外還加了 padding mask。 因為剛剛實作的是 Decoder layer，這次讓我們把中文（目標語言）的詞嵌入張量以及相關的遮罩丟進去看看： # 超參數 d_model = 4 num_heads = 2 dff = 8 dec_layer = DecoderLayer ( d_model , num_heads , dff ) # 來源、目標語言的序列都需要 padding mask inp_padding_mask = create_padding_mask ( inp ) tar_padding_mask = create_padding_mask ( tar ) # masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住 look_ahead_mask = create_look_ahead_mask ( tar . shape [ - 1 ]) combined_mask = tf . maximum ( tar_padding_mask , look_ahead_mask ) # 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算 dec_out , dec_self_attn_weights , dec_enc_attn_weights = dec_layer ( emb_tar , enc_out , False , combined_mask , inp_padding_mask ) print ( \"emb_tar:\" , emb_tar ) print ( \"-\" * 20 ) print ( \"enc_out:\" , enc_out ) print ( \"-\" * 20 ) print ( \"dec_out:\" , dec_out ) assert emb_tar . shape == dec_out . shape print ( \"-\" * 20 ) print ( \"dec_self_attn_weights.shape:\" , dec_self_attn_weights . shape ) print ( \"dec_enc_attn_weights:\" , dec_enc_attn_weights . shape ) emb_tar: tf.Tensor( [[[-0.0441955 -0.01026772 0.03740635 0.02017349] [ 0.02129837 -0.00746276 0.03881821 -0.01586295] [-0.01179456 0.02825376 0.00738146 0.02963744] [ 0.01171205 0.04350302 -0.01190796 0.02526634] [ 0.03814722 -0.03364048 -0.03744673 0.04369817] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349] [-0.00285894 0.02392108 -0.03126474 0.01345349]] [[-0.0441955 -0.01026772 0.03740635 0.02017349] [-0.00359621 -0.01380367 -0.02875998 -0.03855735] [ 0.04516688 -0.04480755 -0.03278694 -0.0093614 ] [ 0.04131394 -0.04065727 -0.04330624 -0.03341667] [ 0.03572228 -0.04500845 0.0470326 0.03095007] [-0.03566641 -0.03730996 -0.00597564 -0.03933349] [ 0.01850356 0.03993076 0.02729526 -0.04848848] [-0.02294568 -0.02494572 -0.0136737 -0.04278342] [ 0.0280853 0.01269842 0.04268574 -0.04069148] [ 0.04029209 -0.00619308 -0.04934603 0.02242902]]], shape=(2, 10, 4), dtype=float32) -------------------- enc_out: tf.Tensor( [[[ 1.2521563 0.3273945 -1.5237452 -0.0558054 ] [-1.0591918 -0.42765176 -0.14816867 1.6350121 ] [ 0.299005 1.3632457 -1.4101827 -0.252068 ] [ 0.7023785 -1.479373 -0.32433346 1.1013279 ] [-1.6220697 1.0153029 0.02592759 0.5808392 ] [-1.0757908 -0.7200314 0.30136684 1.4944555 ] [-0.22072682 1.5675467 -1.215218 -0.1316019 ] [-0.22072682 1.5675467 -1.215218 -0.1316019 ]] [[ 1.475371 0.30539253 -1.1591307 -0.6216327 ] [-1.4569639 0.00421676 0.08528362 1.3674635 ] [ 0.61611307 1.3085197 -0.79488575 -1.1297472 ] [ 0.80156547 0.9995991 -1.5072922 -0.29387245] [-0.11611538 1.6353902 -1.0406278 -0.47864679] [ 0.9602699 -0.3459822 0.8696089 -1.4838965 ] [-1.6676238 0.9936579 0.2892594 0.38470644] [-1.2698565 -0.67637944 1.1073651 0.8388707 ]]], shape=(2, 8, 4), dtype=float32) -------------------- dec_out: tf.Tensor( [[[-0.4073423 -1.3681166 0.4482983 1.3271605 ] [ 0.9023904 -1.6660724 0.1386456 0.6250363 ] [-0.68705463 0.04485544 -0.9672582 1.6094574 ] [ 0.40446007 0.7378753 -1.7199682 0.5776328 ] [ 0.66626793 -0.7429294 -1.1866593 1.2633208 ] [ 1.3847514 0.0595071 -0.00241444 -1.441844 ] [ 0.77179515 -0.15832207 -1.5698854 0.9564123 ] [ 0.19740774 0.9835156 -1.6620107 0.4810872 ] [ 0.19740774 0.9835156 -1.6620107 0.4810872 ] [ 0.19740774 0.9835156 -1.6620107 0.4810872 ]] [[-0.35176337 -1.3861214 0.39734656 1.3405383 ] [ 1.0155624 0.28156188 -1.6605129 0.36338854] [ 0.9295503 -0.96635836 -1.0307404 1.0675484 ] [ 1.2389433 -0.7855455 -1.1608163 0.70741844] [ 0.11645091 -1.565496 0.23167732 1.2173678 ] [-0.44791234 -1.3678643 1.2819183 0.53385824] [-0.05676413 0.90384555 0.7641177 -1.611199 ] [-0.4362856 -1.3157362 1.397403 0.35461882] [ 0.21431251 -0.8140781 1.5471766 -0.94741106] [ 0.4220932 -0.4875322 -1.3055642 1.3710032 ]]], shape=(2, 10, 4), dtype=float32) -------------------- dec_self_attn_weights.shape: (2, 2, 10, 10) dec_enc_attn_weights: (2, 2, 10, 8) 跟 Encoder layer 相同，Decoder layer 輸出張量的最後一維也是 d_model 。而 dec_self_attn_weights 則代表著 Decoder layer 的自注意權重，因此最後兩個維度皆為中文序列的長度 10 ；而 dec_enc_attn_weights 因為 Encoder 輸出序列的長度為 8 ，最後一維即爲 8 。 都讀到這裡了，判斷每一維的物理意義對你來說應該是小菜一碟了。 Positional encoding：神奇數字 透過多層的自注意力層，Transformer 在處理序列時裡頭所有子詞都是「天涯若比鄰」：想要關注序列中 任何 位置的資訊只要 O(1) 就能辦到。這讓 Transformer 能很好地 model 序列中長距離的依賴關係（long-range dependencise）。但反過來說 Transformer 則無法 model 序列中字詞的順序關係，所以我們得額外加入一些「位置資訊」給 Transformer。 這個資訊被稱作位置編碼（Positional Encoding），實作上是直接加到最一開始的英文 / 中文詞嵌入向量（word embedding）裡頭。其直觀的想法是想辦法讓被加入位置編碼的 word embedding 在 d_model 維度的空間裡頭不只會因為 語義相近 而靠近，也會因為 位置靠近 而在該空間裡頭靠近。 論文裡頭使用的位置編碼的公式如下： 嗯 ... 第一次看到這函式的人會黑人問號是很正常。 論文裡頭提到 他們之所以這樣設計位置編碼（ P ositional E ncoding, PE）是因為這個函數有個很好的特性：給定任一位置 pos 的位置編碼 PE(pos) ，跟它距離 k 個單位的位置 pos + k 的位置編碼 PE(pos + k) 可以表示為 PE(pos) 的一個線性函數（linear function）。 因此透過在 word embedding 裡加入這樣的資訊，作者們認為可以幫助 Transformer 學會 model 序列中的子詞的相對位置關係。 子曰：「由！誨女知之乎？知之為知之，不知為不知，是知也。」 ─ 《論語 為政篇》 就算我們無法自己想出論文裡頭的位置編碼公式，還是可以直接把 TensorFlow 官方 的實作搬過來使用： # 以下直接參考 TensorFlow 官方 tutorial def get_angles ( pos , i , d_model ): angle_rates = 1 / np . power ( 10000 , ( 2 * ( i // 2 )) / np . float32 ( d_model )) return pos * angle_rates def positional_encoding ( position , d_model ): angle_rads = get_angles ( np . arange ( position )[:, np . newaxis ], np . arange ( d_model )[ np . newaxis , :], d_model ) # apply sin to even indices in the array; 2i sines = np . sin ( angle_rads [:, 0 :: 2 ]) # apply cos to odd indices in the array; 2i+1 cosines = np . cos ( angle_rads [:, 1 :: 2 ]) pos_encoding = np . concatenate ([ sines , cosines ], axis =- 1 ) pos_encoding = pos_encoding [ np . newaxis , ... ] return tf . cast ( pos_encoding , dtype = tf . float32 ) seq_len = 50 d_model = 512 pos_encoding = positional_encoding ( seq_len , d_model ) pos_encoding <tf.Tensor: id=194541, shape=(1, 50, 512), dtype=float32, numpy= array([[[ 0. , 0. , 0. , ..., 1. , 1. , 1. ], [ 0.84147096, 0.8218562 , 0.8019618 , ..., 1. , 1. , 1. ], [ 0.9092974 , 0.9364147 , 0.95814437, ..., 1. , 1. , 1. ], ..., [ 0.12357312, 0.97718984, -0.24295525, ..., 0.9999863 , 0.99998724, 0.99998814], [-0.76825464, 0.7312359 , 0.63279754, ..., 0.9999857 , 0.9999867 , 0.9999876 ], [-0.95375264, -0.14402692, 0.99899054, ..., 0.9999851 , 0.9999861 , 0.9999871 ]]], dtype=float32)> 一路看下來你應該也可以猜到位置編碼的每一維意義了： 第 1 維代表 batch_size，之後可以 broadcasting 第 2 維是序列長度，我們會為每個在輸入 / 輸出序列裡頭的子詞都加入位置編碼 第 3 維跟詞嵌入向量同維度 因為是要跟詞嵌入向量相加，位置編碼的維度也得是 d_model 。我們也可以把位置編碼畫出感受一下： plt . pcolormesh ( pos_encoding [ 0 ], cmap = 'RdBu' ) plt . xlabel ( 'd_model' ) plt . xlim (( 0 , 512 )) plt . ylabel ( 'Position' ) plt . colorbar () plt . show () 這圖你應該在很多教學文章以及教授的影片裡都看過了。就跟我們前面看過的各種 2 維矩陣相同，x 軸代表著跟詞嵌入向量相同的維度 d_model ，y 軸則代表序列中的每個位置。之後我們會看輸入 / 輸出序列有多少個子詞，就加入幾個位置編碼。 關於位置編碼我們現在只需要知道這些就夠了，但如果你想知道更多相關的數學計算，可以參考 這個筆記本 。 Encoder Encoder 裡頭主要包含了 3 個元件： 輸入的詞嵌入層 位置編碼 N 個 Encoder layers 大部分的工作都交給 Encoder layer 小弟做了，因此 Encoder 的實作很單純： class Encoder ( tf . keras . layers . Layer ): # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了： # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N` # - input_vocab_size: 用來把索引轉成詞嵌入向量 def __init__ ( self , num_layers , d_model , num_heads , dff , input_vocab_size , rate = 0.1 ): super ( Encoder , self ) . __init__ () self . d_model = d_model self . embedding = tf . keras . layers . Embedding ( input_vocab_size , d_model ) self . pos_encoding = positional_encoding ( input_vocab_size , self . d_model ) # 建立 `num_layers` 個 EncoderLayers self . enc_layers = [ EncoderLayer ( d_model , num_heads , dff , rate ) for _ in range ( num_layers )] self . dropout = tf . keras . layers . Dropout ( rate ) def call ( self , x , training , mask ): # 輸入的 x.shape == (batch_size, input_seq_len) # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model) input_seq_len = tf . shape ( x )[ 1 ] # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model) # 再加上對應長度的位置編碼 x = self . embedding ( x ) x *= tf . math . sqrt ( tf . cast ( self . d_model , tf . float32 )) x += self . pos_encoding [:, : input_seq_len , :] # 對 embedding 跟位置編碼的總合做 regularization # 這在 Decoder 也會做 x = self . dropout ( x , training = training ) # 通過 N 個 EncoderLayer 做編碼 for i , enc_layer in enumerate ( self . enc_layers ): x = enc_layer ( x , training , mask ) # 以下只是用來 demo EncoderLayer outputs #print('-' * 20) #print(f\"EncoderLayer {i + 1}'s output:\", x) return x 比較值得注意的是我們依照論文將 word embedding 乘上 sqrt(d_model) ，並在 embedding 跟位置編碼相加以後通過 dropout 層來達到 regularization 的效果。 現在我們可以直接將索引序列 inp 丟入 Encoder： # 超參數 num_layers = 2 # 2 層的 Encoder d_model = 4 num_heads = 2 dff = 8 input_vocab_size = subword_encoder_en . vocab_size + 2 # 記得加上 <start>, <end> # 初始化一個 Encoder encoder = Encoder ( num_layers , d_model , num_heads , dff , input_vocab_size ) # 將 2 維的索引序列丟入 Encoder 做編碼 enc_out = encoder ( inp , training = False , mask = None ) print ( \"inp:\" , inp ) print ( \"-\" * 20 ) print ( \"enc_out:\" , enc_out ) inp: tf.Tensor( [[8135 105 10 1304 7925 8136 0 0] [8135 17 3905 6013 12 2572 7925 8136]], shape=(2, 8), dtype=int64) -------------------- enc_out: tf.Tensor( [[[-0.80654097 -0.5846039 -0.31439844 1.7055433 ] [-0.46891153 -0.57408124 -0.6840381 1.727031 ] [-0.319709 -0.17782518 -1.1191479 1.616682 ] [-0.49274105 0.26990706 -1.2412689 1.4641027 ] [-0.88477194 0.16279429 -0.8493918 1.5713693 ] [-0.96625364 -0.25279218 -0.4533522 1.6723981 ] [-0.8476429 -0.5615218 -0.28872433 1.6978891 ] [-0.61957765 -0.5919263 -0.51938564 1.7308894 ]] [[-0.8083886 -0.56457365 -0.33460823 1.7075704 ] [-0.50152016 -0.5214133 -0.7037289 1.7266623 ] [-0.34244898 -0.11313835 -1.1444559 1.6000432 ] [-0.5072439 0.21401608 -1.2050328 1.4982607 ] [-0.88611245 0.26368466 -0.9036027 1.5260304 ] [-0.96629447 -0.21083635 -0.49055386 1.6676848 ] [-0.86832803 -0.5383212 -0.28836083 1.6950101 ] [-0.6246328 -0.57586765 -0.5305909 1.7310913 ]]], shape=(2, 8, 4), dtype=float32) 注意因為 Encoder 已經包含了詞嵌入層，因此我們不用再像呼叫 Encoder layer 時一樣還得自己先做 word embedding。現在的輸入及輸出張量為： 輸入：（batch_size, seq_len） 輸出：（batch_size, seq_len, d_model） 有了 Encoder，我們之後就可以直接把 2 維的索引序列 inp 丟入 Encoder，讓它幫我們把裡頭所有的英文序列做一連串的轉換。 Decoder Decoder layer 本來就只跟 Encoder layer 差在一個 MHA，而這邏輯被包起來以後呼叫它的 Decoder 做的事情就跟 Encoder 基本上沒有兩樣了。 在 Decoder 裡頭我們只需要建立一個專門給中文用的詞嵌入層以及位置編碼即可。我們在呼叫每個 Decoder layer 的時候也順便把其注意權重存下來，方便我們了解模型訓練完後是怎麼做翻譯的。 以下則是實作： class Decoder ( tf . keras . layers . Layer ): # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size` def __init__ ( self , num_layers , d_model , num_heads , dff , target_vocab_size , rate = 0.1 ): super ( Decoder , self ) . __init__ () self . d_model = d_model # 為中文（目標語言）建立詞嵌入層 self . embedding = tf . keras . layers . Embedding ( target_vocab_size , d_model ) self . pos_encoding = positional_encoding ( target_vocab_size , self . d_model ) self . dec_layers = [ DecoderLayer ( d_model , num_heads , dff , rate ) for _ in range ( num_layers )] self . dropout = tf . keras . layers . Dropout ( rate ) # 呼叫時的參數跟 DecoderLayer 一模一樣 def call ( self , x , enc_output , training , combined_mask , inp_padding_mask ): tar_seq_len = tf . shape ( x )[ 1 ] attention_weights = {} # 用來存放每個 Decoder layer 的注意權重 # 這邊跟 Encoder 做的事情完全一樣 x = self . embedding ( x ) # (batch_size, tar_seq_len, d_model) x *= tf . math . sqrt ( tf . cast ( self . d_model , tf . float32 )) x += self . pos_encoding [:, : tar_seq_len , :] x = self . dropout ( x , training = training ) for i , dec_layer in enumerate ( self . dec_layers ): x , block1 , block2 = dec_layer ( x , enc_output , training , combined_mask , inp_padding_mask ) # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察 attention_weights [ 'decoder_layer {} _block1' . format ( i + 1 )] = block1 attention_weights [ 'decoder_layer {} _block2' . format ( i + 1 )] = block2 # x.shape == (batch_size, tar_seq_len, d_model) return x , attention_weights 接著讓我們初始並呼叫一個 Decoder 看看： # 超參數 num_layers = 2 # 2 層的 Decoder d_model = 4 num_heads = 2 dff = 8 target_vocab_size = subword_encoder_zh . vocab_size + 2 # 記得加上 <start>, <end> # 遮罩 inp_padding_mask = create_padding_mask ( inp ) tar_padding_mask = create_padding_mask ( tar ) look_ahead_mask = create_look_ahead_mask ( tar . shape [ 1 ]) combined_mask = tf . math . maximum ( tar_padding_mask , look_ahead_mask ) # 初始化一個 Decoder decoder = Decoder ( num_layers , d_model , num_heads , dff , target_vocab_size ) # 將 2 維的索引序列以及遮罩丟入 Decoder print ( \"tar:\" , tar ) print ( \"-\" * 20 ) print ( \"combined_mask:\" , combined_mask ) print ( \"-\" * 20 ) print ( \"enc_out:\" , enc_out ) print ( \"-\" * 20 ) print ( \"inp_padding_mask:\" , inp_padding_mask ) print ( \"-\" * 20 ) dec_out , attn = decoder ( tar , enc_out , training = False , combined_mask = combined_mask , inp_padding_mask = inp_padding_mask ) print ( \"dec_out:\" , dec_out ) print ( \"-\" * 20 ) for block_name , attn_weights in attn . items (): print ( f \" {block_name} .shape: {attn_weights.shape} \" ) tar: tf.Tensor( [[4201 10 241 80 27 3 4202 0 0 0] [4201 162 467 421 189 14 7 553 3 4202]], shape=(2, 10), dtype=int64) -------------------- combined_mask: tf.Tensor( [[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]] [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32) -------------------- enc_out: tf.Tensor( [[[-0.80654097 -0.5846039 -0.31439844 1.7055433 ] [-0.46891153 -0.57408124 -0.6840381 1.727031 ] [-0.319709 -0.17782518 -1.1191479 1.616682 ] [-0.49274105 0.26990706 -1.2412689 1.4641027 ] [-0.88477194 0.16279429 -0.8493918 1.5713693 ] [-0.96625364 -0.25279218 -0.4533522 1.6723981 ] [-0.8476429 -0.5615218 -0.28872433 1.6978891 ] [-0.61957765 -0.5919263 -0.51938564 1.7308894 ]] [[-0.8083886 -0.56457365 -0.33460823 1.7075704 ] [-0.50152016 -0.5214133 -0.7037289 1.7266623 ] [-0.34244898 -0.11313835 -1.1444559 1.6000432 ] [-0.5072439 0.21401608 -1.2050328 1.4982607 ] [-0.88611245 0.26368466 -0.9036027 1.5260304 ] [-0.96629447 -0.21083635 -0.49055386 1.6676848 ] [-0.86832803 -0.5383212 -0.28836083 1.6950101 ] [-0.6246328 -0.57586765 -0.5305909 1.7310913 ]]], shape=(2, 8, 4), dtype=float32) -------------------- inp_padding_mask: tf.Tensor( [[[[0. 0. 0. 0. 0. 0. 1. 1.]]] [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32) -------------------- dec_out: tf.Tensor( [[[-0.5437632 -1.055963 1.6090912 -0.0093651 ] [-0.35729456 -1.2363737 1.5295789 0.06408926] [ 0.35950443 -1.4217519 1.3327445 -0.27049693] [ 0.00910451 -1.3681054 1.4556323 -0.09663116] [-0.39842203 -1.0891637 1.6237149 -0.13612938] [-0.41910946 -1.0254465 1.6521797 -0.20762381] [-0.36797434 -1.036104 1.6521349 -0.2480565 ] [-0.19375193 -1.1218892 1.6165614 -0.30092025] [ 0.40127647 -1.3597702 1.3540744 -0.39558053] [ 0.17590097 -1.419068 1.3905344 -0.14736754]] [[-0.54991776 -1.0509207 1.6102997 -0.00946123] [-0.3790077 -1.2450974 1.514628 0.10947719] [ 0.1746773 -1.3877552 1.415193 -0.20211506] [-0.03870562 -1.3375971 1.4825788 -0.10627584] [-0.43508232 -1.067575 1.6293938 -0.12673649] [-0.41048303 -1.0317237 1.6503688 -0.20816201] [-0.3626595 -1.0360833 1.652463 -0.25372016] [-0.24817836 -1.1092765 1.6238651 -0.26641032] [ 0.1850568 -1.3670969 1.4271388 -0.2450987 ] [ 0.09142628 -1.3988855 1.4218552 -0.11439597]]], shape=(2, 10, 4), dtype=float32) -------------------- decoder_layer1_block1.shape: (2, 2, 10, 10) decoder_layer1_block2.shape: (2, 2, 10, 8) decoder_layer2_block1.shape: (2, 2, 10, 10) decoder_layer2_block2.shape: (2, 2, 10, 8) 麻雀雖小，五臟俱全。雖然我們是使用 demo 數據，但基本上這就是你在呼叫 Decoder 時需要做的所有事情： 初始時給它中文（目標語言）的字典大小、其他超參數 輸入中文 batch 的索引序列 也要輸入兩個遮罩以及 Encoder 輸出 enc_out Decoder 的輸出你現在應該都可以很輕鬆地解讀才是。基本上跟 Decoder layer 一模一樣，只差在我們額外輸出一個 Python dict，裡頭存放所有 Decoder layers 的注意權重。 第一個 Transformer 沒錯，終於到了這個時刻。在實作 Transformer 之前先點擊影片來簡單回顧一下我們在這一章實作了什麼些玩意兒： 您的瀏覽器不支援影片標籤，請留言通知我：S Transformer 本身只有 3 個 layers 在我們前面已經將大大小小的 layers 一一實作並組裝起來以後，真正的 Transformer 模型只需要 3 個元件： Encoder Decoder Final linear layer 馬上讓我們看看 Transformer 的實作： # Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型 class Transformer ( tf . keras . Model ): # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目 def __init__ ( self , num_layers , d_model , num_heads , dff , input_vocab_size , target_vocab_size , rate = 0.1 ): super ( Transformer , self ) . __init__ () self . encoder = Encoder ( num_layers , d_model , num_heads , dff , input_vocab_size , rate ) self . decoder = Decoder ( num_layers , d_model , num_heads , dff , target_vocab_size , rate ) # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率 self . final_layer = tf . keras . layers . Dense ( target_vocab_size ) # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask， # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用 def call ( self , inp , tar , training , enc_padding_mask , combined_mask , dec_padding_mask ): enc_output = self . encoder ( inp , training , enc_padding_mask ) # (batch_size, inp_seq_len, d_model) # dec_output.shape == (batch_size, tar_seq_len, d_model) dec_output , attention_weights = self . decoder ( tar , enc_output , training , combined_mask , dec_padding_mask ) # 將 Decoder 輸出通過最後一個 linear layer final_output = self . final_layer ( dec_output ) # (batch_size, tar_seq_len, target_vocab_size) return final_output , attention_weights 扣掉註解，Transformer 的實作本身非常簡短。 被輸入 Transformer 的多個 2 維英文張量 inp 會一路通過 Encoder 裡頭的詞嵌入層，位置編碼以及 N 個 Encoder layers 後被轉換成 Encoder 輸出 enc_output ，接著對應的中文序列 tar 則會在 Decoder 裡頭走過相似的旅程並在每一層的 Decoder layer 利用 MHA 2 關注 Encoder 的輸出 enc_output ，最後被 Decoder 輸出。 而 Decoder 的輸出 dec_output 則會通過 Final linear layer ，被轉成進入 Softmax 前的 logits final_output ，其 logit 的數目則跟中文字典裡的子詞數相同。 因為 Transformer 把 Decoder 也包起來了，現在我們連 Encoder 輸出 enc_output 也不用管，只要把英文（來源）以及中文（目標）的索引序列 batch 丟入 Transformer，它就會輸出最後一維為中文字典大小的張量。第 2 維是輸出序列，裡頭每一個位置的向量就代表著該位置的中文字的機率分佈（事實上通過 softmax 才是，但這邊先這樣說方便你理解）： 輸入： 英文序列：（batch_size, inp_seq_len） 中文序列：（batch_size, tar_seq_len） 輸出： 生成序列：（batch_size, tar_seq_len, target_vocab_size） 注意權重的 dict 讓我們馬上建一個 Transformer，並假設我們已經準備好用 demo 數據來訓練它做英翻中： # 超參數 num_layers = 1 d_model = 4 num_heads = 2 dff = 8 # + 2 是為了 <start> & <end> token input_vocab_size = subword_encoder_en . vocab_size + 2 output_vocab_size = subword_encoder_zh . vocab_size + 2 # 重點中的重點。訓練時用前一個字來預測下一個中文字 tar_inp = tar [:, : - 1 ] tar_real = tar [:, 1 :] # 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一 inp_padding_mask = create_padding_mask ( inp ) tar_padding_mask = create_padding_mask ( tar_inp ) look_ahead_mask = create_look_ahead_mask ( tar_inp . shape [ 1 ]) combined_mask = tf . math . maximum ( tar_padding_mask , look_ahead_mask ) # 初始化我們的第一個 transformer transformer = Transformer ( num_layers , d_model , num_heads , dff , input_vocab_size , output_vocab_size ) # 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果 predictions , attn_weights = transformer ( inp , tar_inp , False , inp_padding_mask , combined_mask , inp_padding_mask ) print ( \"tar:\" , tar ) print ( \"-\" * 20 ) print ( \"tar_inp:\" , tar_inp ) print ( \"-\" * 20 ) print ( \"tar_real:\" , tar_real ) print ( \"-\" * 20 ) print ( \"predictions:\" , predictions ) tar: tf.Tensor( [[4201 10 241 80 27 3 4202 0 0 0] [4201 162 467 421 189 14 7 553 3 4202]], shape=(2, 10), dtype=int64) -------------------- tar_inp: tf.Tensor( [[4201 10 241 80 27 3 4202 0 0] [4201 162 467 421 189 14 7 553 3]], shape=(2, 9), dtype=int64) -------------------- tar_real: tf.Tensor( [[ 10 241 80 27 3 4202 0 0 0] [ 162 467 421 189 14 7 553 3 4202]], shape=(2, 9), dtype=int64) -------------------- predictions: tf.Tensor( [[[ 0.00929452 -0.01123782 0.05421777 ... -0.01170466 0.00628542 -0.07576236] [ 0.03640017 -0.01885041 0.05113849 ... -0.02349908 0.01716622 -0.06729948] [ 0.05617092 -0.02265774 0.04667147 ... -0.02913139 0.0241506 -0.05331099] ... [ 0.00905135 -0.01058669 0.05486142 ... -0.01039154 0.0058039 -0.07445519] [ 0.02215609 -0.01478041 0.05375389 ... -0.0170105 0.01135763 -0.07241639] [ 0.0478656 -0.02148081 0.04837158 ... -0.02759764 0.02148173 -0.06043392]] [[ 0.00996658 -0.01115559 0.05453676 ... -0.0114185 0.00637141 -0.07500792] [ 0.03897631 -0.01930442 0.0508956 ... -0.02409907 0.01803425 -0.0656432 ] [ 0.05387272 -0.02244362 0.04702405 ... -0.02893805 0.02348556 -0.05554678] ... [ 0.01048942 -0.01085559 0.05502523 ... -0.01070841 0.0062833 -0.07385261] [ 0.02370835 -0.01504852 0.05381611 ... -0.01732858 0.01186723 -0.07158875] [ 0.04920105 -0.02166032 0.0481827 ... -0.02781233 0.02190085 -0.05933255]]], shape=(2, 9, 4203), dtype=float32) 有了前面的各種 layers，建立一個 Transformer 並不難。但要輸入什麼數據就是一門大學問了： ... tar_inp = tar [:, : - 1 ] tar_real = tar [:, 1 :] predictions , attn_weights = transformer ( inp , tar_inp , False , ... ) ... 為何是丟少了尾巴一個字的 tar_inp 序列進去 Transformer，而不是直接丟 tar 呢？ 別忘記我們才剛初始一個 Transformer，裡頭所有 layers 的權重都是隨機的，你可不能指望它真的會什麼「黑魔法」來幫你翻譯。我們得先訓練才行。但訓練時如果你把整個正確的中文序列 tar 都進去給 Transformer 看，你期待它產生什麼？一首新的中文詩嗎？ 如果你曾經實作過序列生成模型或是看過 我之前的語言模型文章 ，就會知道在序列生成任務裡頭，模型獲得的正確答案是輸入序列往左位移一個位置的結果。 這樣講很抽象，讓我們看個影片了解序列生成是怎麼運作的： 您的瀏覽器不支援影片標籤，請留言通知我：S 了解序列生成以及如何訓練一個生成模型 你現在應該明白 Transformer 在訓練的時候並不是吃整個中文序列，而是吃一個去掉尾巴的序列 tar_inp ，然後試著去預測「左移」一個字以後的序列 tar_real 。同樣概念當然也可以運用到以 RNN 或是 CNN-based 的模型上面。 從影片中你也可以發現給定 tar_inp 序列中的任一位置，其對應位置的 tar_real 就是下個時間點模型應該要預測的中文字。 序列生成任務可以被視為是一個分類任務（Classification），而每一個中文字都是一個分類。而 Transformer 就是要去產生一個中文字的機率分佈，想辦法跟正解越接近越好。 跟用已訓練的 Transformer 做 預測 時不同，在 訓練 時為了穩定模型表現，我們並不會將 Transformer 的輸出再度丟回去當做其輸入（人形蜈蚣？），而是像影片中所示，給它左移一個位置後的序列 tar_real 當作正解讓它去最小化 error。 這種無視模型預測結果，而將正確解答丟入的訓練方法一般被稱作 teacher forcing 。你也可以參考教授的 Sequence-to-sequence Learning 教學 。 定義損失函數與指標 因為被視為是一個分類任務，我們可以使用 cross entropy 來計算序列生成任務中實際的中文字跟模型預測的中文字分佈（distribution）相差有多遠。 這邊簡單定義一個損失函式： loss_object = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True , reduction = 'none' ) # 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label real = tf . constant ([ 1 , 1 , 0 ], shape = ( 1 , 3 ), dtype = tf . float32 ) pred = tf . constant ([[ 0 , 1 ], [ 0 , 1 ], [ 0 , 1 ]], dtype = tf . float32 ) loss_object ( real , pred ) <tf.Tensor: id=197487, shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)> 如果你曾做過分類問題，應該能看出預測序列 pred 裡頭的第 3 個預測結果出錯因此 entropy 值上升。損失函數 loss_object 做的事情就是比較 2 個序列並計算 cross entropy： real ：一個包含 N 個正確 labels 的序列 pred ：一個包含 N 個維度為 label 數的 logit 序列 我們在這邊將 reduction 參數設為 none ，請 loss_object 不要把每個位置的 error 加總。而這是因為我們之後要自己把 <pad> token 出現的位置的損失捨棄不計。 而將 from_logits 參數設為 True 是因為從 Transformer 得到的預測還沒有經過 softmax，因此加總還不等於 1： print ( \"predictions:\" , predictions ) print ( \"-\" * 20 ) print ( tf . reduce_sum ( predictions , axis =- 1 )) predictions: tf.Tensor( [[[ 0.00929452 -0.01123782 0.05421777 ... -0.01170466 0.00628542 -0.07576236] [ 0.03640017 -0.01885041 0.05113849 ... -0.02349908 0.01716622 -0.06729948] [ 0.05617092 -0.02265774 0.04667147 ... -0.02913139 0.0241506 -0.05331099] ... [ 0.00905135 -0.01058669 0.05486142 ... -0.01039154 0.0058039 -0.07445519] [ 0.02215609 -0.01478041 0.05375389 ... -0.0170105 0.01135763 -0.07241639] [ 0.0478656 -0.02148081 0.04837158 ... -0.02759764 0.02148173 -0.06043392]] [[ 0.00996658 -0.01115559 0.05453676 ... -0.0114185 0.00637141 -0.07500792] [ 0.03897631 -0.01930442 0.0508956 ... -0.02409907 0.01803425 -0.0656432 ] [ 0.05387272 -0.02244362 0.04702405 ... -0.02893805 0.02348556 -0.05554678] ... [ 0.01048942 -0.01085559 0.05502523 ... -0.01070841 0.0062833 -0.07385261] [ 0.02370835 -0.01504852 0.05381611 ... -0.01732858 0.01186723 -0.07158875] [ 0.04920105 -0.02166032 0.0481827 ... -0.02781233 0.02190085 -0.05933255]]], shape=(2, 9, 4203), dtype=float32) -------------------- tf.Tensor( [[1.4971986 3.1899047 4.1454954 3.7353938 2.869739 1.8605256 1.3746347 2.2779167 3.8190796] [1.4881071 3.303587 4.0757227 3.7524652 2.836317 1.9132937 1.4376438 2.3432927 3.8689976]], shape=(2, 9), dtype=float32) 有了 loss_object 實際算 cross entropy 以後，我們需要另外一個函式來建立遮罩並加總序列裡頭不包含 ` token 位置的損失： def loss_function ( real , pred ): # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 mask = tf . math . logical_not ( tf . math . equal ( real , 0 )) # 照樣計算所有位置的 cross entropy 但不加總 loss_ = loss_object ( real , pred ) mask = tf . cast ( mask , dtype = loss_ . dtype ) loss_ *= mask # 只計算非 <pad> 位置的損失 return tf . reduce_mean ( loss_ ) 我另外再定義兩個 tf.keras.metrics ，方便之後使用 TensorBoard 來追蹤模型 performance： train_loss = tf . keras . metrics . Mean ( name = 'train_loss' ) train_accuracy = tf . keras . metrics . SparseCategoricalAccuracy ( name = 'train_accuracy' ) 設置超參數 前面實作了那麼多 layers，你應該還記得有哪些是你自己可以調整的超參數吧？ 讓我幫你全部列出來： num_layers 決定 Transfomer 裡頭要有幾個 Encoder / Decoder layers d_model 決定我們子詞的 representation space 維度 num_heads 要做幾頭的自注意力運算 dff 決定 FFN 的中間維度 dropout_rate 預設 0.1，一般用預設值即可 input_vocab_size ：輸入語言（英文）的字典大小 target_vocab_size ：輸出語言（中文）的字典大小 論文裡頭最基本的 Transformer 配置為： num_layers=6 d_model=512 dff=2048 有大量數據以及大的 Transformer，你可以在很多機器學習任務都達到不錯的成績。為了不要讓訓練時間太長，在這篇文章裡頭我會把 Transformer 裡頭的超參數設小一點： num_layers = 4 d_model = 128 dff = 512 num_heads = 8 input_vocab_size = subword_encoder_en . vocab_size + 2 target_vocab_size = subword_encoder_zh . vocab_size + 2 dropout_rate = 0.1 # 預設值 print ( \"input_vocab_size:\" , input_vocab_size ) print ( \"target_vocab_size:\" , target_vocab_size ) input_vocab_size: 8137 target_vocab_size: 4203 4 層 Encoder / Decoder layers 不算貪心，小巫見大巫（笑 設置 Optimizer 我們在這邊跟 論文 一致，使用 Adam optimizer 以及自定義的 learning rate scheduler： 這 schedule 讓訓練過程的前 warmup_steps 的 learning rate 線性增加，在那之後則跟步驟數 step_num 的反平方根成比例下降。不用擔心你沒有完全理解這公式，我們一樣可以直接使用 TensorFlow 官方教學的實作 ： class CustomSchedule ( tf . keras . optimizers . schedules . LearningRateSchedule ): # 論文預設 `warmup_steps` = 4000 def __init__ ( self , d_model , warmup_steps = 4000 ): super ( CustomSchedule , self ) . __init__ () self . d_model = d_model self . d_model = tf . cast ( self . d_model , tf . float32 ) self . warmup_steps = warmup_steps def __call__ ( self , step ): arg1 = tf . math . rsqrt ( step ) arg2 = step * ( self . warmup_steps ** - 1.5 ) return tf . math . rsqrt ( self . d_model ) * tf . math . minimum ( arg1 , arg2 ) # 將客製化 learning rate schdeule 丟入 Adam opt. # Adam opt. 的參數都跟論文相同 learning_rate = CustomSchedule ( d_model ) optimizer = tf . keras . optimizers . Adam ( learning_rate , beta_1 = 0.9 , beta_2 = 0.98 , epsilon = 1e-9 ) 我們可以觀察看看這個 schedule 是怎麼隨著訓練步驟而改變 learning rate 的： d_models = [ 128 , 256 , 512 ] warmup_steps = [ 1000 * i for i in range ( 1 , 4 )] schedules = [] labels = [] colors = [ \"blue\" , \"red\" , \"black\" ] for d in d_models : schedules += [ CustomSchedule ( d , s ) for s in warmup_steps ] labels += [ f \"d_model: {d} , warm: {s} \" for s in warmup_steps ] for i , ( schedule , label ) in enumerate ( zip ( schedules , labels )): plt . plot ( schedule ( tf . range ( 10000 , dtype = tf . float32 )), label = label , color = colors [ i // 3 ]) plt . legend () plt . ylabel ( \"Learning Rate\" ) plt . xlabel ( \"Train Step\" ) 不同 d_model 以及 warmup_steps 的 learning rate 變化 你可以明顯地看到所有 schedules 都先經過 warmup_steps 個步驟直線提升 learning rate，接著逐漸平滑下降。另外我們也會給比較高維的 d_model 維度比較小的 learning rate。 實際訓練以及定時存檔 好啦，什麼都準備齊全了，讓我們開始訓練 Transformer 吧！記得使用前面已經定義好的超參數來初始化一個全新的 Transformer： transformer = Transformer ( num_layers , d_model , num_heads , dff , input_vocab_size , target_vocab_size , dropout_rate ) print ( f \"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers d_model: {d_model} num_heads: {num_heads} dff: {dff} input_vocab_size: {input_vocab_size} target_vocab_size: {target_vocab_size} dropout_rate: {dropout_rate} \"\"\" ) 這個 Transformer 有 4 層 Encoder / Decoder layers d_model: 128 num_heads: 8 dff: 512 input_vocab_size: 8137 target_vocab_size: 4203 dropout_rate: 0.1 打遊戲時你會記得要定期存檔以防任何意外發生，訓練深度學習模型也是同樣道理。設置 checkpoint 來定期儲存 / 讀取模型及 optimizer 是必備的。 我們在底下會定義一個 checkpoint 路徑，此路徑包含了各種超參數的資訊，方便之後比較不同實驗的結果並載入已訓練的進度。我們也需要一個 checkpoint manager 來做所有跟存讀模型有關的雜事，並只保留最新 5 個 checkpoints 以避免佔用太多空間： # 方便比較不同實驗/ 不同超參數設定的結果 run_id = f \" {num_layers} layers_ {d_model} d_ {num_heads} heads_ {dff} dff_ {train_perc} train_perc\" checkpoint_path = os . path . join ( checkpoint_path , run_id ) log_dir = os . path . join ( log_dir , run_id ) # tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取 # 一般來說你會想存下模型以及 optimizer 的狀態 ckpt = tf . train . Checkpoint ( transformer = transformer , optimizer = optimizer ) # ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西 # 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除 ckpt_manager = tf . train . CheckpointManager ( ckpt , checkpoint_path , max_to_keep = 5 ) # 如果在 checkpoint 路徑上有發現檔案就讀進來 if ckpt_manager . latest_checkpoint : ckpt . restore ( ckpt_manager . latest_checkpoint ) # 用來確認之前訓練多少 epochs 了 last_epoch = int ( ckpt_manager . latest_checkpoint . split ( \"-\" )[ - 1 ]) print ( f '已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。' ) else : last_epoch = 0 print ( \"沒找到 checkpoint，從頭訓練。\" ) 已讀取最新的 checkpoint，模型已訓練 50 epochs。 我知道你在想什麼。 「誒！？ 你不當場訓練嗎？」「直接載入已訓練的模型太狗了吧！」 拜託，我都訓練 N 遍了，每次都重新訓練也太沒意義了。而且你能想像為了寫一個章節我就得重新訓練一個 Transformer 來 demo 嗎？這樣太沒效率了。比起每次重新訓練模型，這才是你在真實世界中應該做的事情：盡可能回復之前的訓練進度來節省時間。 不過放心，我仍會秀出完整的訓練程式碼讓你可以執行第一次的訓練。當你想要依照本文訓練自己的 Transformer 時會感謝有 checkpoint manager 的存在。現在假設我們還沒有 checkpoints。 在實際訓練 Transformer 之前還需要定義一個簡單函式來產生所有的遮罩： # 為 Transformer 的 Encoder / Decoder 準備遮罩 def create_masks ( inp , tar ): # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的 enc_padding_mask = create_padding_mask ( inp ) # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 # 關注 Encoder 輸出序列用的 dec_padding_mask = create_padding_mask ( inp ) # Decoder layer 的 MHA1 在做自注意力機制用的 # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加 look_ahead_mask = create_look_ahead_mask ( tf . shape ( tar )[ 1 ]) dec_target_padding_mask = create_padding_mask ( tar ) combined_mask = tf . maximum ( dec_target_padding_mask , look_ahead_mask ) return enc_padding_mask , combined_mask , dec_padding_mask 如果沒有本文前面針對遮罩的詳細說明，很多第一次實作的人得花不少時間來確實地掌握這些遮罩的用途。不過對現在的你來說應該也是小菜一碟。 一個數據集包含多個 batch，而每次拿一個 batch 來訓練的步驟就稱作 train_step 。為了讓程式碼更簡潔以及容易優化，我們會定義 Transformer 在一次訓練步驟（處理一個 batch）所需要做的所有事情。 不限於 Transformer，一般來說 train_step 函式裡會有幾個重要步驟： 對訓練數據做些必要的前處理 將數據丟入模型，取得預測結果 用預測結果跟正確解答計算 loss 取出梯度並利用 optimizer 做梯度下降 有了這個概念以後看看程式碼： @tf . function # 讓 TensorFlow 幫我們將 eager code 優化並加快運算 def train_step ( inp , tar ): # 前面說過的，用去尾的原始序列去預測下一個字的序列 tar_inp = tar [:, : - 1 ] tar_real = tar [:, 1 :] # 建立 3 個遮罩 enc_padding_mask , combined_mask , dec_padding_mask = create_masks ( inp , tar_inp ) # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降 with tf . GradientTape () as tape : # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True predictions , _ = transformer ( inp , tar_inp , True , enc_padding_mask , combined_mask , dec_padding_mask ) # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss loss = loss_function ( tar_real , predictions ) # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數 gradients = tape . gradient ( loss , transformer . trainable_variables ) optimizer . apply_gradients ( zip ( gradients , transformer . trainable_variables )) # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要 train_loss ( loss ) train_accuracy ( tar_real , predictions ) 如果你曾經以TensorFlow 2 實作過稍微複雜一點的模型，應該就知道 train_step 函式的寫法非常固定： 對輸入數據做些前處理（本文中的遮罩、將輸出序列左移當成正解 etc.） 利用 tf.GradientTape 輕鬆記錄數據被模型做的所有轉換並計算 loss 將梯度取出並讓 optimzier 對可被訓練的權重做梯度下降（上升） 你完全可以用一模一樣的方式將任何複雜模型的處理過程包在 train_step 函式，這樣可以讓我們之後在 iterate 數據集時非常輕鬆。而且最重要的是可以用 tf.function 來提高此函式裡頭運算的速度。你可以點擊連結來了解更多。 處理一個 batch 的 train_step 函式也有了，就只差寫個 for loop 將數據集跑個幾遍了。我之前的模型雖然訓練了 50 個 epochs，但事實上大概 30 epochs 翻譯的結果就差不多穩定了。所以讓我們將 EPOCHS 設定為 30： # 定義我們要看幾遍數據集 EPOCHS = 30 print ( f \"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\" ) print ( f \"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\" ) # 用來寫資訊到 TensorBoard，非必要但十分推薦 summary_writer = tf . summary . create_file_writer ( log_dir ) # 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs for epoch in range ( last_epoch , EPOCHS ): start = time . time () # 重置紀錄 TensorBoard 的 metrics train_loss . reset_states () train_accuracy . reset_states () # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 for ( step_idx , ( inp , tar )) in enumerate ( train_dataset ): # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss train_step ( inp , tar ) # 每個 epoch 完成就存一次檔 if ( epoch + 1 ) % 1 == 0 : ckpt_save_path = ckpt_manager . save () print ( 'Saving checkpoint for epoch {} at {} ' . format ( epoch + 1 , ckpt_save_path )) # 將 loss 以及 accuracy 寫到 TensorBoard 上 with summary_writer . as_default (): tf . summary . scalar ( \"train_loss\" , train_loss . result (), step = epoch + 1 ) tf . summary . scalar ( \"train_acc\" , train_accuracy . result (), step = epoch + 1 ) print ( 'Epoch {} Loss {:.4f} Accuracy {:.4f} ' . format ( epoch + 1 , train_loss . result (), train_accuracy . result ())) print ( 'Time taken for 1 epoch: {} secs \\n ' . format ( time . time () - start )) 此超參數組合的 Transformer 已經訓練 50 epochs。 剩餘 epochs：0 如訊息所示，當指定的 EPOCHS 「落後」於之前的訓練進度我們就不再訓練了。但如果是第一次訓練或是訓練到指定 EPOCHS 的一部分，我們都會從正確的地方開始訓練並存檔，不會浪費到訓練時間或計算資源。 這邊的邏輯也很簡單，在每個 epoch 都： （非必要）重置寫到 TensorBoard 的 metrics 的值 將整個數據集的 batch 取出，交給 train_step 函式處理 （非必要）存 checkpoints （非必要）將當前 epoch 結果寫到 TensorBoard （非必要）在標準輸出顯示當前 epoch 結果 是的，如果你真的只是想要訓練個模型，什麼其他事情都不想考慮的話那你可以： # 87 分，不能再高了。 for epoch in range ( EPOCHS ): for inp , tar in train_dataset : train_step ( inp , tar ) 嗯 ... 話是這麼說，但我仍然建議你至少要記得存檔並將訓練過程顯示出來。我知道你會好奇訓練一個這樣的 Transformer 要多久時間，讓我把之前訓練的一些 log 顯示出來給你瞧瞧： Saving checkpoint for epoch 1 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-1 Epoch 1 Loss 5.2072 Accuracy 0.0179 Time taken for 1 epoch: 206.54558181762695 secs Saving checkpoint for epoch 2 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-2 Epoch 2 Loss 4.2652 Accuracy 0.0560 Time taken for 1 epoch: 68.48831677436829 secs Saving checkpoint for epoch 3 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-3 Epoch 3 Loss 3.7987 Accuracy 0.0910 Time taken for 1 epoch: 68.41022562980652 secs ... Saving checkpoint for epoch 29 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-29 Epoch 29 Loss 1.2693 Accuracy 0.3929 Time taken for 1 epoch: 69.18679404258728 secs Saving checkpoint for epoch 30 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-30 Epoch 30 Loss 1.2426 Accuracy 0.3965 Time taken for 1 epoch: 68.7313539981842 secs 事實上我們定義的 4 層 Transformer 大約每 70 秒就可以看完一遍有 3 萬筆訓練例子的數據集，而且你從上面的 loss 以及 accuracy 可以看出來 Transformer 至少在訓練集裡頭進步地挺快的。 而就我自己的觀察大約經過 30 個 epochs 翻譯結果就很穩定了。所以你大約只需半個小時就能有一個非常簡單，有點水準的英翻中 Transformer（在至少有個一般 GPU 的情況）。 但跟看上面的 log 比起來，我個人還是比較推薦使用 TensorBoard。在 TensorFlow 2 裡頭，你甚至能直接在 Jupyter Notebook 或是 Colab 裡頭開啟它： % load_ext tensorboard % tensorboard -- logdir { your_log_dir } 使用 TensorBoard 可以讓你輕鬆比較不同超參數的訓練結果 透過 TensorBoard，你能非常清楚地比較不同實驗以及不同點子的效果，知道什麼 work 什麼不 work，進而修正之後嘗試的方向。如果只是簡單寫個 print ，那你永遠只會看到最新一次訓練過程的 log，然後忘記之前到底發生過什麼事。 實際進行英翻中 有了已經訓練一陣子的 Transformer，當然得拿它來實際做做翻譯。 跟訓練的時候不同，在做預測時我們不需做 teacher forcing 來穩定 Transformer 的訓練過程。反之，我們將 Transformer 在每個時間點生成的中文索引加到之前已經生成的序列尾巴，並以此新序列作為其下一次的輸入。這是因為 Transformer 事實上是一個 自迴歸模型（Auto-regressive model） ：依據自己生成的結果預測下次輸出。 利用 Transformer 進行翻譯（預測）的邏輯如下： 將輸入的英文句子利用 Subword Tokenizer 轉換成子詞索引序列（還記得 inp 吧？） 在該英文索引序列前後加上代表英文 BOS / EOS 的 tokens 在 Transformer 輸出序列長度達到 MAX_LENGTH 之前重複以下步驟： 為目前已經生成的中文索引序列產生新的遮罩 將剛剛的英文序列、當前的中文序列以及各種遮罩放入 Transformer 將 Transformer 輸出序列的最後一個位置的向量取出，並取 argmax 取得新的預測中文索引 將此索引加到目前的中文索引序列裡頭作為 Transformer 到此為止的輸出結果 如果新生成的中文索引為 <end> 則代表中文翻譯已全部生成完畢，直接回傳 將最後得到的中文索引序列回傳作為翻譯結果 是的，一個時間點生成一個中文字，而在第一個時間點因為 Transformer 還沒有任何輸出，我們會丟中文字的 <start> token 進去。你可能會想： 為何每次翻譯開頭都是 start token，Transformer 還能產生不一樣且正確的結果？ 答案也很簡單，因為 Decoder 可以透過「關注」 Encoder 處理完不同英文句子的輸出來獲得語義資訊，了解它在當下該生成什麼中文字作為第一個輸出。 現在讓我們定義一個 evaluate 函式實現上述邏輯。此函式的輸入是一個完全沒有經過處理的英文句子（以字串表示），輸出則是一個索引序列，裡頭的每個索引就代表著 Transformer 預測的中文字。 讓我們實際看看 evaluate 函式： # 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict def evaluate ( inp_sentence ): # 準備英文句子前後會加上的 <start>, <end> start_token = [ subword_encoder_en . vocab_size ] end_token = [ subword_encoder_en . vocab_size + 1 ] # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列 # 並在前後加上 BOS / EOS inp_sentence = start_token + subword_encoder_en . encode ( inp_sentence ) + end_token encoder_input = tf . expand_dims ( inp_sentence , 0 ) # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入 # 是一個只包含一個中文 <start> token 的序列 decoder_input = [ subword_encoder_zh . vocab_size ] output = tf . expand_dims ( decoder_input , 0 ) # 增加 batch 維度 # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer for i in range ( MAX_LENGTH ): # 每多一個生成的字就得產生新的遮罩 enc_padding_mask , combined_mask , dec_padding_mask = create_masks ( encoder_input , output ) # predictions.shape == (batch_size, seq_len, vocab_size) predictions , attention_weights = transformer ( encoder_input , output , False , enc_padding_mask , combined_mask , dec_padding_mask ) # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字 predictions = predictions [: , - 1 :, :] # (batch_size, 1, vocab_size) predicted_id = tf . cast ( tf . argmax ( predictions , axis =- 1 ), tf . int32 ) # 遇到 <end> token 就停止回傳，代表模型已經產生完結果 if tf . equal ( predicted_id , subword_encoder_zh . vocab_size + 1 ): return tf . squeeze ( output , axis = 0 ), attention_weights #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生 # 下個中文字的時候關注到最新的 `predicted_id` output = tf . concat ([ output , predicted_id ], axis =- 1 ) # 將 batch 的維度去掉後回傳預測的中文索引序列 return tf . squeeze ( output , axis = 0 ), attention_weights 我知道這章節程式碼很多很長，但搭配註解後你會發現它們實際上都不難，而且這也是你看這篇文章的主要目的：實際了解 Transformer 是怎麼做英中翻譯的。你不想只是紙上談兵，對吧？ 有了 evaluate 函式，要透過 Transformer 做翻譯非常容易： # 要被翻譯的英文句子 sentence = \"China, India, and others have enjoyed continuing economic growth.\" # 取得預測的中文索引序列 predicted_seq , _ = evaluate ( sentence ) # 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子 target_vocab_size = subword_encoder_zh . vocab_size predicted_seq_without_bos_eos = [ idx for idx in predicted_seq if idx < target_vocab_size ] predicted_sentence = subword_encoder_zh . decode ( predicted_seq_without_bos_eos ) print ( \"sentence:\" , sentence ) print ( \"-\" * 20 ) print ( \"predicted_seq:\" , predicted_seq ) print ( \"-\" * 20 ) print ( \"predicted_sentence:\" , predicted_sentence ) sentence: China, India, and others have enjoyed continuing economic growth. -------------------- predicted_seq: tf.Tensor( [4201 16 4 37 386 101 8 34 32 4 33 110 956 186 14 22 52 107 84 1 104 292 49 218 3], shape=(25,), dtype=int32) -------------------- predicted_sentence: 中国、印度和其他国家都享受了经济增长的持续发展。 考慮到這個 Transformer 不算巨大（約 400 萬個參數），且模型訓練時用的數據集不大的情況下，我們達到相當不錯的結果，你說是吧？在這個例子裡頭該翻的詞彙都翻了出來，句子本身也還算自然。 transformer . summary () Model: \"transformer_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= encoder_2 (Encoder) multiple 1834624 _________________________________________________________________ decoder_2 (Decoder) multiple 1596288 _________________________________________________________________ dense_137 (Dense) multiple 542187 ================================================================= Total params: 3,973,099 Trainable params: 3,973,099 Non-trainable params: 0 _________________________________________________________________ 視覺化注意權重 除了其運算高度平行以及表現不錯以外，Transformer 另外一個優點在於我們可以透過視覺化注意權重（attention weights）來了解模型實際在生成序列的時候放「注意力」在哪裡。別忘記我們當初在 Decoder layers 做完 multi-head attention 之後都將注意權重輸出。現在正是它們派上用場的時候了。 先讓我們看看有什麼注意權重可以拿來視覺化： predicted_seq , attention_weights = evaluate ( sentence ) # 在這邊我們自動選擇最後一個 Decoder layer 的 MHA 2，也就是 Decoder 關注 Encoder 的 MHA layer_name = f \"decoder_layer {num_layers} _block2\" print ( \"sentence:\" , sentence ) print ( \"-\" * 20 ) print ( \"predicted_seq:\" , predicted_seq ) print ( \"-\" * 20 ) print ( \"attention_weights.keys():\" ) for layer_name , attn in attention_weights . items (): print ( f \" {layer_name} .shape: {attn.shape} \" ) print ( \"-\" * 20 ) print ( \"layer_name:\" , layer_name ) sentence: China, India, and others have enjoyed continuing economic growth. -------------------- predicted_seq: tf.Tensor( [4201 16 4 37 386 101 8 34 32 4 33 110 956 186 14 22 52 107 84 1 104 292 49 218 3], shape=(25,), dtype=int32) -------------------- attention_weights.keys(): decoder_layer1_block1.shape: (1, 8, 25, 25) decoder_layer1_block2.shape: (1, 8, 25, 15) decoder_layer2_block1.shape: (1, 8, 25, 25) decoder_layer2_block2.shape: (1, 8, 25, 15) decoder_layer3_block1.shape: (1, 8, 25, 25) decoder_layer3_block2.shape: (1, 8, 25, 15) decoder_layer4_block1.shape: (1, 8, 25, 25) decoder_layer4_block2.shape: (1, 8, 25, 15) -------------------- layer_name: decoder_layer4_block2 block1 代表是 Decoder layer 自己關注自己的 MHA 1，因此倒數兩個維度都跟中文序列長度相同； block2 則是 Decoder layer 用來關注 Encoder 輸出的 MHA 2 ，在這邊我們選擇最後一個 Decoder layer 的 MHA 2 來看 Transformer 在生成中文序列時關注在英文句子的那些位置。 但首先，我們得要有一個繪圖的函式才行： import matplotlib as mpl # 你可能會需要自行下載一個中文字體檔案以讓 matplotlib 正確顯示中文 zhfont = mpl . font_manager . FontProperties ( fname = '/usr/share/fonts/SimHei/simhei.ttf' ) plt . style . use ( \"seaborn-whitegrid\" ) # 這個函式將英 -> 中翻譯的注意權重視覺化（注意：我們將注意權重 transpose 以最佳化渲染結果 def plot_attention_weights ( attention_weights , sentence , predicted_seq , layer_name , max_len_tar = None ): fig = plt . figure ( figsize = ( 17 , 7 )) sentence = subword_encoder_en . encode ( sentence ) # 只顯示中文序列前 `max_len_tar` 個字以避免畫面太過壅擠 if max_len_tar : predicted_seq = predicted_seq [: max_len_tar ] else : max_len_tar = len ( predicted_seq ) # 將某一個特定 Decoder layer 裡頭的 MHA 1 或 MHA2 的注意權重拿出來並去掉 batch 維度 attention_weights = tf . squeeze ( attention_weights [ layer_name ], axis = 0 ) # (num_heads, tar_seq_len, inp_seq_len) # 將每個 head 的注意權重畫出 for head in range ( attention_weights . shape [ 0 ]): ax = fig . add_subplot ( 2 , 4 , head + 1 ) # [注意]我為了將長度不短的英文子詞顯示在 y 軸，將注意權重做了 transpose attn_map = np . transpose ( attention_weights [ head ][: max_len_tar , :]) ax . matshow ( attn_map , cmap = 'viridis' ) # (inp_seq_len, tar_seq_len) fontdict = { \"fontproperties\" : zhfont } ax . set_xticks ( range ( max ( max_len_tar , len ( predicted_seq )))) ax . set_xlim ( - 0.5 , max_len_tar - 1.5 ) ax . set_yticks ( range ( len ( sentence ) + 2 )) ax . set_xticklabels ([ subword_encoder_zh . decode ([ i ]) for i in predicted_seq if i < subword_encoder_zh . vocab_size ], fontdict = fontdict , fontsize = 18 ) ax . set_yticklabels ( [ '<start>' ] + [ subword_encoder_en . decode ([ i ]) for i in sentence ] + [ '<end>' ], fontdict = fontdict ) ax . set_xlabel ( 'Head {} ' . format ( head + 1 )) ax . tick_params ( axis = \"x\" , labelsize = 12 ) ax . tick_params ( axis = \"y\" , labelsize = 12 ) plt . tight_layout () plt . show () plt . close ( fig ) 這個函式不難，且裡頭不少是調整圖片的細節設定因此我將它留給你自行參考。 比較值得注意的是因為我們在這篇文章是做英文（來源）到中文（目標）的翻譯，注意權重的 shape 為： (batch_size, num_heads, zh_seq_len, en_seq_len) 如果你直接把注意權重繪出的話 y 軸就會是每個中文字，而 x 軸則會是每個英文子詞。而英文子詞繪在 x 軸太佔空間，我將每個注意權重都做 transpose 並呈現結果，這點你得注意一下。 讓我們馬上畫出剛剛翻譯的注意權重看看： plot_attention_weights ( attention_weights , sentence , predicted_seq , layer_name , max_len_tar = 18 ) 很美，不是嗎？ 如果你還記得，我在本文開頭就曾經秀過這張圖甚至開玩笑地跟你說： 好黑魔法，不學嗎？ 我不知道你當初跟現在的感受，但我相信在你閱讀完本文，尤其是對自注意機制以及 Transformer 有了深刻理解之後，這之間的感受肯定是有不少差異的。 儘管其運算機制十分錯綜複雜，閱讀本文後 Transformer 對你來說不再是黑魔法，也不再是遙不可及的存在。如果你現在覺得「Transformer 也不過就這樣嘛！」那就達成我寫這篇文章的目的了。 自注意力機制以及 Transformer 在推出之後就被非常廣泛地使用並改進，但在我自己開始接觸相關知識以後一直沒有發現完整的繁中教學，因此寫了這篇當初的我殷殷期盼的文章，也希望能幫助到更多人學習。 在進入結語之前，讓我們看看文中的 Transformer 是怎麼逐漸學會做好翻譯的： 您的瀏覽器不支援影片標籤，請留言通知我：S Transformer 在訓練過程中逐漸學會關注在對的位置 在你離開之前 這篇是當初在學習 Transformer 的我希望有人分享給自己的文章。 我相信人類之所以強大是因為集體知識：我們能透過書籍、影片以及語言將一個人腦中的知識與思想共享給其他人，讓寶貴的知識能夠「scale」，在更多人的腦袋中發光發熱，創造更多價值。 我希望你有從本文中學到一點東西，並幫助我將本文的這些知識「scale」，把文章分享給更多有興趣的人，並利用所學應用在一些你一直想要完成的任務上面。 以 Transformer 為基礎的語言代表模型 BERT （ 圖片來源 ） 如果你想要了解更多 Transformer 的相關應用，我推薦接著閱讀 進擊的 BERT：NLP 界的巨人之力與遷移學習 ，了解現在 NLP 領域裡頭知名的語言代表模型 BERT。 最後一點提醒，就算 Transformer 比古早時代的方法好再多終究也只是個工具，其最大價值不會超過於被你拿來應用的問題之上。就好像現在已有不少超越基本 Transformer 的翻譯方法，但我們仍然持續在追尋更好的機器翻譯系統。 工具會被淘汰，需求一直都在。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html","loc":"https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html"},{"title":"用 CartoonGAN 及 TensorFlow 2 生成新海誠與宮崎駿動畫","text":"如果你能用 AI 為圖片添加新海誠或是宮崎駿等人的動漫風格，你會選擇什麼圖片？ 這篇文章將簡單介紹最近我與夥伴 mnicnc404 以 TensorFlow 2.0 Alpha 實作的 CartoonGAN（ Github 連結 ）。我們同時也會展示一個 TensorFlow.js 應用，讓你可以直接在瀏覽器上產生動漫。 CartoonGAN（原論文） 於 2018 CVPR 推出，是一個嘗試將真實世界圖片轉換成動漫的 對抗生成網路（Generative Adversarial Network，以下簡稱 GAN） 。 您的瀏覽器不支援影片標籤，請留言通知我：S 左上為原圖，其餘三圖則為我們使用 CartoonGAN 將不同動漫風格套用到原圖上的結果 當初看到這篇 論文 覺得很有趣，且裡頭正好展示了我喜愛的兩位日本動畫作家： 新海誠 及 宮崎駿 的風格轉換結果，因此決定寫個 TensorFlow.js 應用，讓更多人可以實際體驗這個有趣的 CartoonGAN。 使用方法很直覺，選擇風格並上傳照片，完成！ 您的瀏覽器不支援影片標籤，請留言通知我：S TensorFlow.js 在瀏覽器裡運行，其背後處理主要分為兩個步驟： 下載模型 轉換圖片 因手機的運算能力有限，強烈建議： 使用桌筆電等 計算能力 強的設備開啟本頁面 並在網速快的環境測試（減少 載入模型 時間） 動手玩 CartoonGAN 以下就是實際的應用： 選擇圖片 新海誠 風格 宮崎駿 風格 細田守 風格 盜夢偵探 風格 結果出來了嗎？如果上傳圖片後一直停在： Loading Models ：代表仍在下載模型 Cartoonizing images ：代表設備運算資源的不足 也先別走開！你可以使用下一節介紹的方法來生成動漫，保證有效。 TensorFlow 2 畫動漫 適逢 PoweredByTF 2.0 挑戰 ，我們也用 TensorFlow 2.0 Alpha 完整地實作了 CartoonGAN 的訓練以及推論邏輯。如果你想要轉換大張圖片或是動圖，可以執行 這個 Colab 筆記本 ： 您的瀏覽器不支援影片標籤，請留言通知我：S 執行我們準備的 Colab 筆記本可以讓你用 CartoonGAN 轉換任何圖片（上圖顯示該筆記本的部分內容） Google Colaboratory 是一個雲端 Jupyter 筆記本 環境，提供 GPU 讓任何人都可以立即開始一個深度學習專案。 在 這個筆記本 裡頭，以下步驟都幫你寫好了： 建置 TensorFlow 2.0 環境 下載 我們的 Github 專案及預訓練模型 下載任意網路圖片 / gif 使用 CartoonGAN 轉換圖片 你只需打開筆記本並依照指示一步步執行，即可為你的圖片添加動漫風格。 一些轉換後的動漫結果 獨樂樂不如眾樂樂。這一節和你分享一些我們用 CartoonGAN 得到的結果。 以下每張圖片都分為四個區塊，從左到右、由上而下分別為： 原始圖片 新海誠 風格 宮崎駿 風格 細田守 風格 點擊下圖左右兩側的小箭頭可以切換不同圖片： <!-- https://www.w3schools.com/w3css/w3css_slideshow.asp --> .w3-content, .w3-auto { margin-left: auto; margin-right: auto } .w3-content { max-width: 980px } .w3-display-container:hover .w3-display-hover { display: block } .w3-display-container:hover span.w3-display-hover { display: inline-block } .w3-display-container { position: relative } .w3-button:hover { color: #000!important; background-color: inherit; } .w3-button { border: none; display: inline-block; padding: 8px 16px; vertical-align: middle; overflow: hidden; text-decoration: none; color: inherit; background-color: inherit; text-align: center; cursor: pointer; white-space: nowrap } .w3-button { -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none } .w3-button:disabled { cursor: not-allowed; opacity: 0.3 } .w3-display-left { position: absolute; top: 50%; left: 0%; transform: translate(0%, -50%); -ms-transform: translate(-0%, -50%) } .w3-display-right { position: absolute; top: 50%; right: 0%; transform: translate(0%, -50%); -ms-transform: translate(0%, -50%) } .mySlides {display:none;} 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S 您的瀏覽器不支援影片標籤，請留言通知我：S ❮ ❯ var slideIndex = 1; showDivs(slideIndex); function plusDivs(n) { showDivs(slideIndex += n); } function showDivs(n) { var i; var x = document.getElementsByClassName(\"mySlides\"); if (n > x.length) {slideIndex = 1} if (n < 1) {slideIndex = x.length} for (i = 0; i < x.length; i++) { x[i].style.display = \"none\"; } x[slideIndex-1].style.display = \"block\"; } 從漫威電影到可愛貓咪，從自然風景到人氣偶像，任何你想得到的圖片都可以拿來進行轉換。一旦訓練完成，我們就能使用 CartoonGAN 在彈指之間將各式各樣的圖片跟動漫做連結。 在 我們的 Github 專案 裡頭，你甚至只需要執行一個指令就能取得上面展示的結果： python cartoonize.py \\ --styles shinkai hayao hosoda 當然，接下來數年類似應用的效果會更加卓越。未來誰都能用這些 apps 來創造自己的動漫，而動漫作家可以據此更快速地畫出草稿、測試新點子。 如 CartoonGAN 這樣的生成模型能讓我們看到未來更多的可能性。 順帶一提，我們在 讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部 也已經看過類似的概念。只是在該篇裡頭，我們是讓機器生成武俠小說而非動漫圖片。 訓練你自己的 CartoonGAN 你可能發現本文非常注重在 CartoonGAN 的實際應用而非演算法細節。這是因為： 比起 論文細節 ，多數人應該都對如何生成有趣的動漫比較感興趣 網上已有不少 CartoonGAN 的介紹文章以及 GAN 的學習資源 對 CartoonGAN 的實作細節有興趣的讀者可以參考 我們的 Github 專案 。基本上只要依照指示裝好開發環境以及準備好資料集，你甚至可以一鍵訓練 CartoonGAN： 我們的 Python 腳本提供了詳盡訊息，方便你理解訓練過程究竟發生了什麼事情 因為所有邏輯皆以 TensorFlow 2.0 Alpha 實作，非常適合想要跟上最新 TensorFlow 發展的讀者學習： 使用 tf.keras 實作 Layers 以及 GAN 使用 tf.data 讀取大量圖片並進行前處理 寫訓練邏輯並使用 tf.function 加快處理速度 使用 TensorBoard 來即時觀測模型表現 訓練 GAN 本身並不是一件非常容易的事情。你會需要時時觀察模型的表現以評估如何調整超參數。因此我們的 訓練腳本 也整合了 TensorBoard ，讓你可以在執行後即時地監控模型表現： 我們訓練 CartoonGAN 的其中一次實際結果 除了指標與損失函數以外，在訓練 GAN 的過程中觀察模型生成的圖片也是一件非常重要的事情。因此我們也將 CartoonGAN 生成的圖片寫入 TensorBoard 以方便你觀測比較： 在腳本執行的過程中可以直接在 TensorBoard 上觀察 CartoonGAN 當下生成的圖片 雖然我們只紀錄專屬於 CartoonGAN 的指標以及生成圖片，你完全可以運用類似的方法來監測任何想要訓練的模型。 最後，為了方便了解 CartoonGAN 在訓練過程的表現，我們可以事先存一組真實世界的圖片當作驗證集（Validation Set），並固定讓模型在訓練一段時間後都試著將其轉換成動漫。 我們可以用驗證集的轉換結果來評估模型在當下的表現。將這些圖片依時間排序後可以得到這樣的結果： 您的瀏覽器不支援影片標籤，請留言通知我：S CartoonGAN 學習將真實世界圖片轉換為動漫的過程 儘管尚未訓練完成，你可以看到模型逐漸學會將原始圖片轉換成簡單的動漫：線條變得清晰、顏色變得平滑。 跟訓練一個 簡單分類器 比起來，訓練一個 GAN 的難度可說是完全不同級別。但我們盡量讓 Github 專案 裡頭的程式碼邏輯簡單易懂，希望能讓更多人入門 TensorFlow 2.0 以及 GAN 的實作。 當然，具備理論基礎能讓你更容易理解 TensorFlow 2 的程式碼。下節將列出推薦的學習資源供你參考。 推薦的 GAN 學習資源 近年以深度學習為基礎的生成模型（Generative Models）領域蓬勃發展，其中最亮眼的發展之一當屬對抗生成網路了。而本文的 CartoonGAN 也是 眾多 GANs 裡頭的其中一個小夥子。 對抗生成網路 GAN 實際上由兩個獨立的神經網路相互「對抗」 （ 圖片來源 ） 對 GAN 的理論 & 基礎知識有興趣的讀者，我推薦以下的學習資源： 李宏毅教授在 Youtube 上的 GAN 教學影片 MIT 6.S191 的 Deep Generative Models GAN Lab 讓你在瀏覽器上訓練並學習 GAN Andrej Karpathy 的簡單 GAN Demo TensorFlow 官方教學帶你生成 MNIST 機器之心講解 CartoonGAN 運作原理 Synced: Reproducing Japanese Anime Styles With CartoonGAN AI Open Questions about Generative Adversarial Networks MIT 6.S191 的 Deep Generative Models 適合入門 （ 圖片來源 ） 生成模型及 GAN 的研究領域博大精深，但我相信參考過以上資源，你將具備實作一個簡單 GAN 所需的基礎知識。 更多學習資源則請參閱 由淺入深的深度學習資源整理 。另外如果你有其他推薦的學習資源，還請不吝留言與我及其他讀者分享，謝謝！ 結語 要讓深度學習、人工智慧等研究與應用的發展加快，我們應該想辦法先讓更多人實際體驗其應用，進而理解其運作原理，最後參與其中。 以 CartoonGAN 為例，本文先專注在 如何讓每個人都能實際體驗並應用此技術 ，接著才簡單介紹讀者可以如何使用 我們的 Github 專案 來訓練自己的模型。最後我也附上了一些實用的學習資源供對背後原理有興趣的讀者做些參考。 這是我第一次用 TensorFlow 2.0 以及 TensorFlow.js 實作 GAN，感謝 CartoonGAN 原作者的研究、TensorFlow / TensorFlow.js 團隊的努力、夥伴 mnicnc404 強大的實作支援以及許多寶貴見解，獲益良多。 希望透過此文能讓更多人進一步探索 GAN 以及生成模型的相關知識，嘗試更多新的可能性，做出更多有趣的 AI 應用。 也別忘了跟我分享你生成的動漫！你可以在 Twitter 或是 Facebook 上標註我：） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html","loc":"https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2.html"},{"title":"讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部","text":"木婉清轉頭向他，背脊向著南海鱷神，低聲道：「你是世上第一個見到我容貌的男子！」緩緩拉開了面幕。段譽登時全身一震，眼前所見，如新月清暉，如花樹堆雪，一張臉秀麗絕俗。 第四回：崖高人遠 《天龍八部》 一直是我最喜歡的 金庸著作 之一，最近重新翻閱，有很多新的感受。 閱讀到一半我突發奇想，決定嘗試用 深度學習 以及 TensorFlow 2.0 來訓練一個能夠生成《天龍八部》的 循環神經網路 。生成結果仍不完美，但我認為已經很有娛樂性質，且有時能夠產生令人驚嘆或是捧腹大笑的文章了。 因此我決定使用 Tensorflow.js 將訓練出來的模型弄上線，讓你也能實際看看這個 AI 嗑了什麼藥。 大理古城一隅，段譽出身之地 在 demo 之後，我將以此文的 AI 應用為例，用 TensorFlow 2.0 帶你走過深度學習專案中常見的 7 個步驟： 定義問題及要解決的任務 準備原始數據、資料清理 建立能丟入模型的資料集 定義能解決問題的函式集 定義評量函式好壞的指標 訓練並選擇出最好的函式 將函式 / 模型拿來做預測 希望閱讀本文後能讓你學到點東西，從中獲得些啟發，並運用自己的想像力創造點新的東西。 前言夠長了，讓我們馬上進入 demo 吧！ 生成新的天龍八部橋段 本篇使用一個十分簡單的 長短期記憶 RNN 來生成文章。在多次「閱讀」天龍八部之後，這個模型可以在給定一段文本的情況下，逐字產生類似天龍八部小說的文章。 比方說給定書中的一個橋段： 烏老大偏生要考一考慕容復，說道：「慕容公子，你瞧這不是大大的 你會怎麼接下去？ 本文的模型順著上面的話生成的其中一次結果： 不算？」馬夫人道：「不錯，咱們非要尋死不可。」 段譽大喜，說道：「小姑娘，你待我這麼好，鬼鬼祟祟，一切又不聽你的話，你管甚麼老兄弟不相干，我去幫過彥之。」 王夫人哼了一聲，說道：「這裏是甚麼話？」段譽道：「不行！你別過來。用真蠻子，我便將這件事了，一大惡人擠在地下，立時便會斃命，那便如何是好？」 文章內容很ㄎ一ㄤ，惹人發笑，但用詞本身很天龍八部。（至少我自己寫不出這樣的內容） 姑蘇慕容家所在的蘇州 現在馬上就讓我們產生一些新的橋段吧！首先將已經訓練好的模型載入你的瀏覽器。 （建議在網速快的地方載入模型以減少等待時間，或者點擊載入後先閱讀 模型是怎麼被訓練的 ，等等再回來查看） 成功載入模型後，你將可以用它不斷地產生新的橋段： 載入模型 另外你會發現有 2 個可供你調整的參數： 生成長度（字單位） 生成溫度（隨機度） 第一次可以直接使用預設值。現在點擊 生成文章 來產生全新的天龍八部橋段： 生成文章 重置輸入 起始句子： 蕭峯吃了一驚，心想：「哥哥大喜之餘，說話有些忘形了，眼下亂成 生成結果： 如何？希望模型產生的結果有成功令你嘴角上揚。當初它可快把我逗死了。 現在你可以嘗試幾件事情： 點 生成文章 來讓模型依據同輸入產生新橋段 點 重置輸入 來隨機取得一個新的起始句子 增加模型生成的 文章長度 調整 生成溫度 來改變文章的變化性 生成溫度是一個實數值，而當溫度越高，模型產生出來的結果越隨機、越不可預測（也就越ㄎㄧㄤ）；而溫度越低，產生的結果就會越像天龍八部原文。優點是真實，但同時字詞的重複性也會提升。 機器並沒有情感，只有人類可以賦予事物意義。我們無法讓機器自動找出最佳的生成溫度，因為人的感覺十分主觀：找出你自己覺得最適合的溫度來生成文章。 如果你沒有打算深入探討技術細節，那只需要記得在這篇文章裡頭的模型是一個以「字」為單位的語言模型（Character-based Language Model）即可：給定一連串已經出現過的字詞，模型會想辦法去預測出下一個可能出現的字。 值得注意的是，我們並不單純是拿出現機率最高的字出來當生成結果，這樣太無趣了。 每次機器做預測前都會拿著一個包含大量中文字的機率分布 p，在決定要吐出哪個字時，會對該機率分佈 p 做抽樣，從中隨機選出一個字。 因此就跟你在上面 demo 看到的一樣，就算輸入的句子相同，每次模型仍然會生成完全不同的文章。 抽樣的過程類似擲骰子，儘管有些結果較易出現，你還是有機會骰到豹子 因為隨機抽樣的關係，每次模型產生的結果基本上都是獨一無二的。 如果你在生成文章的過程中得到什麼有趣的虛擬橋段，都歡迎與我分享：） 本文接著將詳細解說此應用是怎麼被開發出來的。如果你現在沒有打算閱讀，可以直接跳到 結語 。 模型是怎麼被訓練的 在看完 demo 以後，你可能會好奇這個模型是怎麼被訓練出來的。 實際的開發流程大致可以分為兩個部分： 用 TensorFlow 2.0 訓練一個 LSTM 模型 使用 TensorFlow.js 部屬該模型 這些在 TensorFlow 以及 TensorFlow.js 的官網都有詳細的教學以及程式碼供你參考。 這篇文章參考了不少 TensorFlow 官網（左）及 TensorFlow.js 線上 demo（右）的程式碼 如果你也想開發一個類似的應用，閱讀官方教學中你所熟悉的語言版本（Python / JavaScript）是最直接的作法： TensorFlow 2.0 Alpha - Text generation with an RNN TensorFlow.js Example: Train LSTM to Generate Text 因為官方已經有提供能在 Google Colab 上使用 GPU 訓練 LSTM 的教學筆記本 ，本文便不再另行提供。 另外，具備以下背景可以讓你更輕鬆地閱讀接下來的內容： 熟悉 Python 碰過 Keras 或是 TensorFlow 具備 機器學習 & 深度學習基礎 了解何謂 循環神經網路 以及 長短期記憶 如果你是喜歡先把基礎打好的人，可以先查閱我上面附的這些資源連結。 TensorFlow 2.0 開發 平常有在接觸深度學習的讀者或許都已經知道，最近 TensorFlow 隆重推出 2.0 Alpha 預覽版 ，希望透過全新的 API 讓更多人可以輕鬆地開發機器學習以及深度學習應用。 當初撰寫本文的其中一個目的，也是想趁著這次大改版來讓自己熟悉一下 TensorFlow 2.0 的開發方式。 TensorFlow 2.0 值得關注的 更新 不少，但以下幾點跟一般的 ML 開發者最為相關： tf.keras 被視為官方高級 API，強調其地位 方便除錯的 Eager Execution 成為預設值 負責讀取、處理大量數據的 tf.data API 自動幫你建構計算圖的 tf.function 在這篇文章裡頭會看到前 3 者。下節列出的程式碼皆在 Google Colab 上用最新版本的 TensorFlow 2.0 Nightly 執行。 pip install tf-nightly-gpu-2.0-preview 如果有 GPU 則強烈建議安裝 GPU 版本的 TF Nightly，訓練速度跟 CPU 版本可以差到 10 倍以上。 深度學習專案步驟 好戲終於登場。 如同多數的深度學習專案，要訓練一個以 LSTM 為基礎的語言模型，你大致需要走過以下幾個步驟： 開發一個 DL 專案時我常用的流程架構 這個流程是一個大方向，依據不同情境你可能需要做些調整來符合自己的需求，且很多步驟需要重複進行。 這篇文章會用 TensorFlow 2.0 簡單地帶你走過所有步驟。 1. 定義問題及要解決的任務 很明顯地，在訓練模型前首先得確認我們的問題（Problem）以及想要交給機器解決的任務（Task）是什麼。 前面已經提過，我們的目標就是要找出一個天龍八部的語言模型（Language Model），讓該模型在被餵進一段文字以後，能吐出類似天龍八部的文章。 十分推薦李宏毅教授講解序列生成的影片 這實際上是一個 序列生成（Sequence Generation） 問題，而機器所要解決的任務也變得明確：給定一段文字單位的序列，它要能吐出下一個合理的文字單位。 這邊說的文字單位（Token）可以是 字（Character，如劍、寺、雲） 詞（Word，如吐蕃、師弟、阿修羅） 本文則使用「字」作為一個文字單位。現在假設有一個天龍八部的句子： 『六脈神劍經』乃本寺鎮寺之寶，大理段氏武學的至高法要。 這時候句子裡的每個字（含標點符號）都是一個文字單位，而整個句子就構成一個文字序列。我們可以擷取一部份句子： 『六脈神劍經』乃本寺鎮寺之寶，大理段氏武 接著在訓練模型時要求它讀入這段文字，並預測出原文裡頭出現的下一個字： 學 。 一旦訓練完成，就能得到你開頭看到的那個語言模型了。 2. 準備原始數據、資料清理 巧婦難為無米之炊，沒有數據一切免談。 我在網路上蒐集天龍八部原文，做些簡單的數據清理後發現整本小說總共約含 120 萬個中文字，實在是一部曠世巨作。儘管因為版權問題不宜提供下載連結，你可以 Google 自己有興趣的文本。 現在假設我們把原文全部存在一個 Python 字串 text 裡頭，則部分內容可能如下： # 隨意取出第 9505 到 9702 的中文字 print ( text [ 9505 : 9702 ]) 咱們見敵方人多，不得師父號令，沒敢隨便動手。」左子穆道：「嗯，來了多少人？」干光豪道：「大約七八十人。」左子穆嘿嘿冷笑，道：「七八十人，便想誅滅無量劍了？只怕也沒這麼容易。」 龔光傑道：「他們用箭射過來一封信，封皮上寫得好生無禮。」說著將信呈上。 左子穆見信封上寫著：「字諭左子穆」五個大字，便不接信，說道：「你拆來瞧瞧。」龔光傑道：「是！」拆開信封，抽出信箋。 那少女在段譽耳邊低聲道： 我們也可以看看整本小說裡頭包含多少中文字： n = len ( text ) w = len ( set ( text )) print ( f \"天龍八部小說共有 {n} 中文字\" ) print ( f \"包含了 {w} 個獨一無二的字\" ) 天龍八部小說共有 1235431 中文字 包含了 4330 個獨一無二的字 相較於英文只有 26 個簡單字母，博大精深的中文裡頭有非常多漢字。 如同 寫給所有人的自然語言處理與深度學習入門指南 裡頭說過的，要將文本數據丟入只懂數字的神經網路，我們得先做些前處理。 具體來說，得將這些中文字對應到一個個的索引數字（Index）或是向量才行。 我們可以使用 tf.keras 裡頭的 Tokenizer 幫我們把整篇小說建立字典，並將同樣的中文字對應到同樣的索引數字： import tensorflow as tf # 初始化一個以字為單位的 Tokenizer tokenizer = tf . keras \\ . preprocessing \\ . text \\ . Tokenizer ( num_words = num_words , char_level = True , filters = '' ) # 讓 tokenizer 讀過天龍八部全文， # 將每個新出現的字加入字典並將中文字轉 # 成對應的數字索引 tokenizer . fit_on_texts ( text ) text_as_int = tokenizer \\ . texts_to_sequences ([ text ])[ 0 ] # 隨機選取一個片段文本方便之後做說明 s_idx = 21004 e_idx = 21020 partial_indices = \\ text_as_int [ s_idx : e_idx ] partial_texts = [ tokenizer . index_word [ idx ] \\ for idx in partial_indices ] # 渲染結果，可忽略 print ( \"原本的中文字序列：\" ) print () print ( partial_texts ) print () print ( \"-\" * 20 ) print () print ( \"轉換後的索引序列：\" ) print () print ( partial_indices ) 原本的中文字序列： ['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。'] -------------------- 轉換後的索引序列： [557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2] 很明顯地，現在整部天龍八部都已經被轉成一個巨大的數字序列，每一個數字代表著一個獨立的中文字。 我們可以換個方向再看一次： 人類看的中文字 機器看的輸入索引 ------------------------------ 司 557 空 371 玄 215 雙 214 掌 135 飛 418 舞 1209 ， 1 逼 837 得 25 牠 1751 無 49 法 147 近 537 前 111 。 2 3. 建立能丟入模型的資料集 做完基本的數據前處理以後，我們需要將 text_as_int 這個巨大的數字序列轉換成神經網路容易消化的格式與大小。 text_as_int [: 10 ] [1639, 148, 3, 3, 280, 5, 192, 819, 374, 800] _type = type ( text_as_int ) n = len ( text_as_int ) print ( f \"text_as_int 是一個 {_type} \\n \" ) print ( f \"小說的序列長度： {n} \\n \" ) print ( \"前 5 索引：\" , text_as_int [: 5 ]) text_as_int 是一個 <class 'list'> 小說的序列長度： 1235431 前 5 索引： [1639, 148, 3, 3, 280] 在建立資料集時，你要先能想像最終交給模型的數據長什麼樣子。這樣能幫助你對數據做適當的轉換。 依照當前機器學習任務的性質，你會需要把不同格式的數據餵給模型。 在本文的序列生成任務裡頭，理想的模型要能依據前文來判斷出下一個中文字。因此我們要丟給模型的是一串代表某些中文字的數字序列： print ( \"實際丟給模型的數字序列：\" ) print ( partial_indices [: - 1 ]) print () print ( \"方便我們理解的文本序列：\" ) print ( partial_texts [: - 1 ]) 實際丟給模型的數字序列： [557, 371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111] 方便我們理解的文本序列： ['司', '空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前'] 而模型要給我們的理想輸出應該是向左位移一個字的結果： print ( \"實際丟給模型的數字序列：\" ) print ( partial_indices [ 1 :]) print () print ( \"方便我們理解的文本序列：\" ) print ( partial_texts [ 1 :]) 實際丟給模型的數字序列： [371, 215, 214, 135, 418, 1209, 1, 837, 25, 1751, 49, 147, 537, 111, 2] 方便我們理解的文本序列： ['空', '玄', '雙', '掌', '飛', '舞', '，', '逼', '得', '牠', '無', '法', '近', '前', '。'] 為什麼是這樣的配對？ 讓我們將輸入序列及輸出序列拿來對照看看： 司 空 玄 雙 掌 飛 舞 ， 逼 得 牠 無 法 近 空 玄 雙 掌 飛 舞 ， 逼 得 牠 無 法 近 前 從左看到右你會發現，一個模型如果可以給我們這樣的輸出，代表它： 看到第一個輸入字 司 時可以正確輸出 空 在之前看過 司 ，且新輸入字為 空 的情況下，可以輸出 玄 在之前看過 司空 ，且新輸入字為 玄 的情況下，可以輸出 雙 在之前看過 司空玄雙掌飛 ，且新輸入字為 舞 的情況下，可以輸出 ， 當一個語言模型可以做到這樣的事情，就代表它已經掌握了 訓練文本 （此文中為天龍八部）裡頭用字的統計結構，因此我們可以用它來產生新的天龍八部文章。 你現在應該也可以了解，這個語言模型是專為天龍八部的文本所誕生的。畢竟日常生活中，給你 舞 這個字，你接 ， 的機率有多少呢？ 為了讓你加深印象，讓我把序列擺直，再次列出模型的輸入以及輸出關係： 時間點 輸入字 輸入索引 輸出字 輸出索引 ------------------------------------- 1 司 557 空 371 2 空 371 玄 215 3 玄 215 雙 214 4 雙 214 掌 135 5 掌 135 飛 418 6 飛 418 舞 1209 7 舞 1209 ， 1 8 ， 1 逼 837 9 逼 837 得 25 10 得 25 牠 1751 11 牠 1751 無 49 12 無 49 法 147 13 法 147 近 537 14 近 537 前 111 每一列（row）是一個時間點，而 輸入索引 代表模型在當下時間吃進去的輸入 輸出索引 則代表我們要模型輸出的結果 輸入字・輸出字則只是方便我們理解對照，實際上模型只吃數字。 現在我們了解一筆輸入・輸出該有的數據格式了。兩者皆是一個固定長度的數字序列，而後者是前者往左位移一個數字的結果。 但這只是一筆數據（以下說的一筆數據，都隱含了輸入序列以及對應的輸出序列的 2 個數字序列）。 在有 GPU 的情況下，我們常常會一次丟一批（batch）數據，讓 GPU 可以平行運算，加快訓練速度。 現在假設我們想要一個資料集，而此資料集可以一次給我們 128 筆長度為 10 的輸入・輸出序列，則我們可以用 tf.data 這樣做： # 方便說明，實際上我們會用更大的值來 # 讓模型從更長的序列預測下個中文字 SEQ_LENGTH = 10 # 數字序列長度 BATCH_SIZE = 128 # 幾筆成對輸入/輸出 # text_as_int 是一個 python list # 我們利用 from_tensor_slices 將其 # 轉變成 TensorFlow 最愛的 Tensor <3 characters = tf \\ . data \\ . Dataset \\ . from_tensor_slices ( text_as_int ) # 將被以數字序列表示的天龍八部文本 # 拆成多個長度為 SEQ_LENGTH (10) 的序列 # 並將最後長度不滿 SEQ_LENGTH 的序列捨去 sequences = characters \\ . batch ( SEQ_LENGTH + 1 , drop_remainder = True ) # 天龍八部全文所包含的成對輸入/輸出的數量 steps_per_epoch = \\ len ( text_as_int ) // SEQ_LENGTH # 這個函式專門負責把一個序列 # 拆成兩個序列，分別代表輸入與輸出 # （下段有 vis 解釋這在做什麼） def build_seq_pairs ( chunk ): input_text = chunk [: - 1 ] target_text = chunk [ 1 :] return input_text , target_text # 將每個從文本擷取出來的序列套用上面 # 定義的函式，拆成兩個數字序列 # 作為輸入／輸出序列 # 再將得到的所有數據隨機打亂順序 # 最後再一次拿出 BATCH_SIZE（128）筆數據 # 作為模型一次訓練步驟的所使用的資料 ds = sequences \\ . map ( build_seq_pairs ) \\ . shuffle ( steps_per_epoch ) \\ . batch ( BATCH_SIZE , drop_remainder = True ) 這段建構 tf.data.Dataset 的程式碼雖然不短，但有超過一半是我寫給你的註解。 事實上用 tf.data 架構一個資料集並不難，且學會以後你每次都可用類似的方式呼叫 TensorFlow Data API 來處理 任何 文本數據，而不需要每次遇到新文本都從頭開始寫類似的功能（ batch 、 shuffle etc）。 再次提醒，如果你想自己動手可以參考 官方用 TensorFlow 2.0 訓練 LSTM 的 Colab 筆記本 。 雖然我不是酷拉皮卡，但如果要把上面 build_seq_pairs 的處理具現化的話，大概就像是下面這樣（假設序列長度為 6）： 擷取的片段序列 輸入/輸出序列 ------------------------------- -> 烏老大拱手還 | 烏老大拱手還禮 ----- | -> 老大拱手還禮 -> 星宿派人數遠 | 星宿派人數遠較 ----- | -> 宿派人數遠較 -> 過不多時，賈 | 過不多時，賈老 ----- | -> 不多時，賈老 你會發現針對序列長度 SEQ_LENGTH 為 6 的情況，我會刻意將天龍八部文本切成長度為 SEQ_LENGTH + 1 ：7 的句子，再從這些句子建立出輸入及輸出序列。 到此為止，我們已經用 tf.data 建立出一個可以拿來訓練語言模型的資料集了。 TensorFlow 2.0 預設就是 Eager Execution ，因此你不再需要使用老朋友 tf.Session() 或是 tf.placeholder 就能非常直覺地存取數據： # print 是用來幫你理解 tf.data.Dataset # 的內容，實際上存取資料集非常簡單 # 現在先關注下面的 print 結果 for b_inp , b_tar in ds . take ( 1 ): print ( \"起始句子的 batch：\" ) print ( b_inp , \" \\n \" ) print ( \"目標句子的 batch：\" ) print ( b_tar , \" \\n \" ) print ( \"-\" * 20 , \" \\n \" ) print ( \"第一個起始句子的索引序列：\" ) first_i = b_inp . numpy ()[ 0 ] print ( first_i , \" \\n \" ) print ( \"第一個目標句子的索引序列：\" ) first_t = b_tar . numpy ()[ 0 ] print ( first_t , \" \\n \" ) print ( \"-\" * 20 , \" \\n \" ) d = tokenizer . index_word print ( \"第一個起始句子的文本序列：\" ) print ([ d [ i ] for i in first_i ]) print () print ( \"第一個目標句子的文本序列：\" ) print ([ d [ i ] for i in first_t ]) 起始句子的 batch： tf.Tensor( [[1440 10 12 ... 1882 15 175] [ 157 16 212 ... 11 206 92] [ 36 14 36 ... 368 384 63] ... [ 61 8 3 ... 11 5 219] [ 123 189 587 ... 65 120 51] [ 1 5 620 ... 2 8 1272]], shape=(128, 10), dtype=int32) 目標句子的 batch： tf.Tensor( [[ 10 12 7 ... 15 175 99] [ 16 212 67 ... 206 92 1] [ 14 36 36 ... 384 63 2] ... [ 8 3 3 ... 5 219 1] [ 189 587 884 ... 120 51 196] [ 5 620 597 ... 8 1272 1275]], shape=(128, 10), dtype=int32) -------------------- 第一個起始句子的索引序列： [1440 10 12 7 63 19 17 1882 15 175] 第一個目標句子的索引序列： [ 10 12 7 63 19 17 1882 15 175 99] -------------------- 第一個起始句子的文本序列： ['陵', '道', '：', '「', '想', '來', '他', '嫌', '你', '本'] 第一個目標句子的文本序列： ['道', '：', '「', '想', '來', '他', '嫌', '你', '本', '事'] 為了讓你理解資料集回傳的內容，上面用了不少 print 。但事實上這個資料集 ds 負責的就是每次吐出 2 個 128 筆數據的 Tensor，分別代表輸入與輸出的批次數據（Batch）。 而每筆數據則包含了一個長度為 10 的數字序列，代表著天龍八部裡頭的一段文本。 減少一些 print ，你要從資料集 ds 取得一個 batch 的輸入／輸出非常地簡單： for b_inp , b_tar in ds . take ( 1 ): # 蒙多想去哪就去哪 # 想怎麼存取 b_iup, b_tar 都可以 print ( \"b_inp 是個 Tensor： \\n \" ) print ( b_inp ) print ( \" \\n b_tar 也是個 Tensor，\" ) print ( \"只是每個數字序列都是\" \"對應的輸入序列往左位\" \"移一格的結果 \\n \" ) print ( b_tar ) b_inp 是個 Tensor： tf.Tensor( [[ 2 953 1214 ... 1 52 219] [ 6 2 15 ... 36 189 5] [2456 1167 3142 ... 110 1186 56] ... [ 422 244 19 ... 2 8 46] [ 254 51 237 ... 123 64 27] [1561 25 55 ... 66 2 3]], shape=(128, 10), dtype=int32) b_tar 也是個 Tensor， 只是每個數字序列都是對應的輸入序列往左位移一格的結果 tf.Tensor( [[ 953 1214 41 ... 52 219 52] [ 2 15 189 ... 189 5 189] [1167 3142 1294 ... 1186 56 5] ... [ 244 19 145 ... 8 46 41] [ 51 237 202 ... 64 27 569] [ 25 55 9 ... 2 3 3]], shape=(128, 10), dtype=int32) 4. 定義能解決問題的函式集 呼！我們花了不少時間在建構資料集，是時候捲起袖子將這些資料丟入模型了！ 回想資料集內容，你現在應該已經很清楚我們想要模型解決的問題是什麼了：丟入一個數字序列，模型要能產生包含下個時間點的數字序列，最好是跟當初的 輸出 序列一模一樣！ 如同我們在 AI 如何找出你的喵 裡頭說過的： 任何類型的神經網路本質上都是一個映射函數。它們會在內部進行一連串特定的數據轉換步驟，想辦法將給定的輸入數據轉換成指定的輸出形式。 我們現在要做的就是定義一個神經網路架構，讓這個神經網路（或稱函式）幫我們把輸入的數字序列轉換成對應的輸出序列。 我們期待這個模型具有「記憶」，能考慮以前看過的所有歷史資訊，進而產生最有可能的下個中文字。 循環神經網路非常適合處理具有順序關係的數據 而在 自然語言處理與深度學習入門指南 我們也已經看到，循環神經網路中的 LSTM 模型非常適合拿來做這件事情。 因此雖然理論上你可以用任意架構的神經網路（如基本的前饋神經網路）來解決這個問題，使用 LSTM（或 GRU，甚至是 1D CNN）是一個相對安全的起手式。 使用 Keras 開發深度學習模型 （ 圖片來源 ） 在 TensorFlow 裡頭，使用 Keras API 建立一個神經網路就像是在疊疊樂，一層一層蓋上去： # 超參數 EMBEDDING_DIM = 512 RNN_UNITS = 1024 # 使用 keras 建立一個非常簡單的 LSTM 模型 model = tf . keras . Sequential () # 詞嵌入層 # 將每個索引數字對應到一個高維空間的向量 model . add ( tf . keras . layers . Embedding ( input_dim = num_words , output_dim = EMBEDDING_DIM , batch_input_shape = [ BATCH_SIZE , None ] )) # LSTM 層 # 負責將序列數據依序讀入並做處理 model . add ( tf . keras . layers . LSTM ( units = RNN_UNITS , return_sequences = True , stateful = True , recurrent_initializer = 'glorot_uniform' )) # 全連接層 # 負責 model 每個中文字出現的可能性 model . add ( tf . keras . layers . Dense ( num_words )) model . summary () 這邊我們建立了一個由 詞嵌入層 、LSTM 層以及全連接層組成的簡單 LSTM 模型。此模型一次吃 128 筆長度任意的數字序列，在內部做些轉換，再吐出 128 筆同樣長度，4330 維的 Tensor。 如果你還記得，4330 實際上是天龍八部裡頭所有出現過的中文字數目。 因此 事實上我們已經把本來看似沒有正解的生成問題轉變成一個 監督式 且有 4330 個 分類的問題 了。我們希望訓練模型，使得其每次預測出來的字都跟正確解答（即輸出序列裡的字）一樣。 值得一提的是，儘管這個神經網路（或稱映射函數）看起來非常有希望能解決我們的序列生成問題，我們並不僅僅是建立了 1 個映射函數而已。事實上，我們用 tf.keras 定義了一個有接近 1,300 萬參數的函式 集合 （Function set）。 這跟你懷疑一個資料集的特徵 x 跟目標值 y 成線性關係，然後想用 a * x + b = y 的直線去 fit y 的道理是一樣的。 你相信 a * x + b = y 形式的映射函數能幫你把輸入 x 有效地對應到目標 y ，你只是還不知道最佳的參數組合 (a, b) 該設多少罷了。 同理，很多研究結果顯示 LSTM 模型能很好地處理序列數據，我們只是還不知道最適合生成天龍八部文章的參數組合是什麼而已。 深度學習中我們常使用梯度下降與反向傳播來從函數集合中找出最好的函數（某個特定參數組合的神經網路架構） 參數 a 以及 b 有無限多種組合，而每一組 a 與 b 的組合都對應到一個實際的 函數 。每個函數都能幫你把 x 乘上 a 倍再加上 b 去 fit 目標值 y ，只是每個函數的表現不一而已。而把所有可能的函數放在一起，就是所謂的函數集合。 本文的 LSTM 模型架構因為參數組合無窮無盡，本身就像是一個巨大的函數空間。而我們得從裡頭找出能解決問題的特定函數（參數組合） 針對 a * x + b = y 這個簡單例子，我們可以直接用線性代數從整個函式集合裡頭瞬間找出最佳的函數 f （即最佳的 (a, b) ）。 而在深度學習領域裡頭，我們會透過 梯度下降（Gradient Descent） 以及 反向傳播算法（Backpropagation） 來幫我們在浩瀚無垠的函式集合（如本文中的 LSTM 網路架構）裡頭找出一個好的神經網路（某個 1,300 萬個參數的組合）。 深度學習框架 （ 圖片來源 ） 幸好我們後面會看到，像是 TensorFlow 、 Pytorch 等深度學習框架幫我們把這件事情變得簡單多了。 5. 定義評量函式好壞的指標 有了 資料集 以及 LSTM 模型架構 以後，我們得定義一個 損失函數（Loss Function） 。 在監督式學習裡頭，一個損失函數評估某個模型產生出來的預測結果 y_pred 跟正確解答 y 之間的差距。一個好的函式／模型，要能最小化損失函數。 有了損失函數以後，我們就能讓模型計算當前預測結果與正解之間的差異（Loss），據此調整模型內的參數以降低這個差異。 機器學習模型或 AI 不會幫我們定義損失函數，因為只有我們能決定什麼是對的，什麼是錯的（至少在 2019 年是這樣） 依照不同情境、不同機器學習任務你會需要定義不同的損失函數。 如同 前述 ，其實我們要 LSTM 模型做的是一個分類問題（Classification Problem）： 給定之前看過的文字序列以及當下時間點的新輸入字，從 4330 個字裡頭預測下一個出現的字。 因此本文的問題可以被視為一個有 4330 個分類（字）的問題。而要定義分類問題的損失相對簡單，使用 sparse_categorical_crossentropy 是個不錯的選擇： # 超參數，決定模型一次要更新的步伐有多大 LEARNING_RATE = 0.001 # 定義模型預測結果跟正確解答之間的差異 # 因為全連接層沒使用 activation func # from_logits= True def loss ( y_true , y_pred ): return tf . keras . losses \\ . sparse_categorical_crossentropy ( y_true , y_pred , from_logits = True ) # 編譯模型，使用 Adam Optimizer 來最小化 # 剛剛定義的損失函數 model . compile ( optimizer = tf . keras \\ . optimizers . Adam ( learning_rate = LEARNING_RATE ), loss = loss ) model.compile 讓我們告訴模型在訓練的時候該使用什麼 優化器（optimizers） 來最小化剛剛定義的 損失函數 。 完成這個步驟以後，我們就能開始訓練模型了。 6. 訓練並選擇出最好的函式 在完成前 5 個步驟以後，訓練一個 Keras 模型本身是一件非常簡單的事情，只需要呼叫 model.fit ： EPOCHS = 10 # 決定看幾篇天龍八部文本 history = model . fit ( ds , # 前面使用 tf.data 建構的資料集 epochs = EPOCHS ) Keras 模型在訓練時就會不斷吐出結果供你參考 但很多時候你需要跑很多次 fit 。 一般來說，你事先並不知道要訓練多少個 epochs 模型才會收斂，當然也不知道怎麼樣的超參數會表現最好。 大多時候，你會想要不斷地驗證腦中的點子、調整超參數、訓練新模型，並再次依照實驗結果嘗試新點子。 這時候 TensorFlow 的視覺化工具 TensorBoard 就是你最好的朋友之一： 利用 TensorBoard 記錄下實驗結果，方便記錄自己做了什麼實驗，什麼 work 什麼不 work TensorFlow 2.0 新增了 JupyterNotebook 的 Extension ，讓你可以直接在筆記本或是 Google Colab 上邊訓練模型邊查看結果。 跟以往使用 TensorBoard 一樣，你需要為 Keras 模型增加一個 TensorBoard Callback ： callbacks = [ tf . keras . callbacks \\ . TensorBoard ( \"logs\" ), # 你可以加入其他 callbacks 如 # ModelCheckpoint, # EarlyStopping ] history = model . fit ( ds , epochs = EPOCHS , callbacks = callbacks ) 接著在訓練開始之後（之前也行）載入 Extension 並執行 TensorBoard 即可： % load_ext tensorboard.notebook % tensorboard --logdir logs 除了確保模型有一直努力在降低損失函數以外，我們也可以觀察模型在訓練過程中生成的文章內容。比方說給定一個句子： 喬峯指著深谷， 模型在完全沒有訓練的情況下生成的結果為： 喬峯指著深谷，鑠淆孤癸抑私磚簧麥笠簸殯膽稼匿聲罪殖省膻臆啟殖 》斥酒燥弄咪薔鬃衝矚理蝗驗吞柢舌滴漂撿毛等櫈磁槃鞭爛辣諱輝母犢楊拜攜戛婉額虐延久鋒幟懸質迸飭南軌忸瑩娘檔麵獎逍菌包怖續敗倨凍赭彈暖顴衽劑街榻裝貨啕畿驛吳 這模型並沒有中邪。只不過模型中 1,300 萬個參數的值完全隨機，你可不能期待模型能做什麼有意義的數據處理。 而在模型看了 20 遍天龍八部以後產生的結果： 喬峯指著深谷，說道：「我不知道，不是你的好人，你就是你的好。」木婉清道：「他……你……我……我……師父是誰？」 段正淳道：「王姑娘，你還是不是？」段譽道：「你說過的話，他……我……你……你……」 那女郎道：「嗯 結果差強人意，「你我他」後面只會加一大堆點點點。 但如果你仔細觀察，其實也已經有不少值得注意的地方： 模型已經知道怎麼產生正確的人名 知道 道 後面要接冒號以及上括號 知道有上括號時後面應該要有下括號 知道要適時加入換行 這其實已經是不小的成就了！ 而在看過 100 遍天龍八部以後產生的結果： 喬峯指著深谷，往前走去。 段譽見到這等慘狀，心下大驚，當即伸手去撫摸她的頭髮，心想：「我想叫你滾出去！」一面說，一面擤了些鼻涕拋下。 那大漢掙扎著要站起身來，只見一條大漢身披獸皮，眼前青光閃閃，雙手亂舞 擤了些鼻涕拋下 很不段譽，但我還是笑了。 文章本身順暢很多，而且內容也豐富不少。另外用字也挺天龍八部的。 你應該也已經注意到，句子之間沒有太大的故事關聯性。而這邊帶出一個很重要的概念： 這個語言模型只能學會天龍八部裡頭字與字之間的統計關係，而無法理解金庸的世界觀。 因此不要期待模型每次都能產生什麼深具含義的結果。 儘管還不完美，到此為止我們手上已經有訓練過的模型了。讓我們拿它來產生新的文本了吧！ 7. 將函式 / 模型拿來做預測 大部分你在深度學習專案裡頭訓練出來的模型可以直接拿來做預測。 不過因為循環神經網路傳遞狀態的方式，一旦建好模型， BATCH_SIZE 就不能做變動了。但在實際生成文章時，我們需要讓 BATCH_SIZE 等於 1。 因此在這邊我們會重新建立一個一模一樣的 LSTM 模型架構，將其 BATCH_SIZE 設為 1 後讀取之前訓練時儲存的參數權重： # 跟訓練時一樣的超參數， # 只差在 BATCH_SIZE 為 1 EMBEDDING_DIM = 512 RNN_UNITS = 1024 BATCH_SIZE = 1 # 專門用來做生成的模型 infer_model = tf . keras . Sequential () # 詞嵌入層 infer_model . add ( tf . keras . layers . Embedding ( input_dim = num_words , output_dim = EMBEDDING_DIM , batch_input_shape = [ BATCH_SIZE , None ] )) # LSTM 層 infer_model . add ( tf . keras . layers . LSTM ( units = RNN_UNITS , return_sequences = True , stateful = True )) # 全連接層 infer_model . add ( tf . keras . layers . Dense ( num_words )) # 讀入之前訓練時儲存下來的權重 infer_model . load_weights ( ckpt_path ) infer_model . build ( tf . TensorShape ([ 1 , None ])) 除了 讀取權重 ，這段程式碼對你來說應該已經十分眼熟。有了 infer_model 以後，接著我們要做的就是： 將起始文本丟入模型 抽樣得到新的中文字 將新得到的字再丟入模型 重複上述步驟 而實際預測的流程大概就長這個樣子： 重複抽樣取得新的中文字 （ 圖片來源 ） 如同我們在 生成新的天龍八部橋段 所看到的，依照你設定的 生成長度 ，我們需要重複上述步驟數次。 而要執行一次的抽樣也並沒有非常困難： # 代表「喬」的索引 seed_indices = [ 234 ] # 增加 batch 維度丟入模型取得預測結果後 # 再度降維，拿掉 batch 維度 input = tf . expand_dims ( seed_indices , axis = 0 ) predictions = infer_model ( input ) predictions = tf . squeeze ( predictions , 0 ) # 利用生成溫度影響抽樣結果 predictions /= temperature # 從 4330 個分類值中做抽樣 # 取得這個時間點模型生成的中文字 sampled_indices = tf . random \\ . categorical ( predictions , num_samples = 1 ) 抽樣的程式碼為了方便解說有稍作刪減，如果你要實際動手跑看看，請參考官方的 Text generation with an RNN 。 這邊我想要你看到的重點是如何利用 生成溫度 temperature 的概念來影響最後的抽樣結果。 如同 demo 時說明的： 生成溫度是一個實數值，而當溫度越高，模型產生出來的結果越隨機、越不可預測 模型的輸出為一個 4330 維度的 Tensor，而其中的每一維都對應到一個中文字。維度值越大即代表該字被選到的機會越大。 而當我們把整個分佈 predictions 除以一個固定值 temperature 時，越大的值被縮減的程度越大，進而讓各維度之間的絕對差異變小，使得原來容易被選到的字被抽到的機會變小，少出現的字被選到的機會稍微提升。 溫度越高，分佈會變得越平滑，罕見字被選到的機會上升，生成結果越隨機 （ 圖片來源 ） 這就是為何我們會想手動調整生成溫度的原因。 如何使用 TensorFlow.js 跑模型並生成文章 雖然本文以天龍八部為例，事實上你已經了解如何使用 TensorFlow 2.0 來架構出一個能產生任意文本的 LSTM 模型了。 一般而言，只要你把剛剛生成文本的 Keras 模型儲存下來，接著就可以在任何機器或雲端平台（如 GCP、AWS）上進行生成： infer_model . save ( \"model.h5\" ) 最近適逢 TensorFlow.js 推出 1.0.0 版本 ，我決定嘗試使用 tfjs-converter 將 Keras 模型轉換成 TensorFlow.js 能夠運行的格式： tensorflowjs_converter \\ --input_format = keras \\ model.h5 \\ tfjs_model_folder 轉換完成後會得到 tfjs 的模型，接著只要把它放到伺服器或是 Github 上就能在任何靜態網頁上載入模型： model = tf . loadLayersModel ( \"url\" ); const output = model . predict ( input ); 我們在 由淺入深的深度學習資源整理 就曾介紹過 TensorFlow.js ，他們有很多有趣的 Demos ，想要在瀏覽器上實作 AI 應用的你可以去了解一下。 使用 TensorFlow.js 好處在於： 隱私有保障。使用者上傳、輸入的內容不會被上傳到伺服器 開發者不需租借伺服器或是建置 API 端點，無部署成本 當你能把模型讀入瀏覽器以後，只要將我們剛剛在前面介紹過的 Python 邏輯利用 TensorFlow.js API 實現即可。 熟悉 JavaScript 的你甚至還可以 直接在瀏覽器上訓練類似本文的 LSTM 模型並生成文章 。 結語 感謝你花費那麼多時間閱讀本文！ 回顧一下，我們在文中談了非常多的東西： 如何利用深度學習 7 步驟開發 AI 應用 定義問題及要解決的任務 準備原始數據、資料清理 建立能丟入模型的資料集 定義能解決問題的函式集 定義評量函式好壞的指標 訓練並選擇出最好的函式 將函式 / 模型拿來做預測 了解如何利用深度學習解決序列生成任務 熟悉 TensorFlow 2.0 的重要功能 tf.keras tf.data TensorBoard 我們也看到你可以如何運用 tfjs-converter 將 Python 與 JavaScript 這兩個世界結合起來，建立可以給任何人在任何裝置上執行的 AI 應用。 除了可以被用來解決「被動」的分類、迴歸問題，近年深度學習在「主動」的 生成任務 上也展現了卓越的成果。廣為人知的應用有 Google 的 DeepDream 、神經風格轉換以及最近 NVIDIA 將塗鴉轉成風景照 的例子。 GauGAN 能幫助一般人繪出美麗圖片，也能讓藝術家更快將點子實現出來 （ 圖片來源 ） 就像本文的天龍八部生成，儘管還未臻完美，讓機器自動生成全新、沒人看過的事物一直是人類追求的夢想之一。但這些人工智慧（ A rtifical I ntelligence）的研究並不是一味地追求如何 取代 人類智慧；反之，AI 更像是 增強 我們的智慧（ A ugmented I ntelligence）： 最好的 AI 是為了讓我們的生活充滿更多智慧，而非取代我們的智慧。AI 能擴充我們對世界的想像，讓我們看到更多不同的可能性。 我在 直觀理解 GPT-2 語言模型並生成金庸武俠小說 一文使用整整 14 部金庸小說來訓練模型 能一路聽我碎碎唸到這裡，代表你對 AI 以及深度學習的應用是抱持著很大的興趣的。希望在此之後你能運用本文學到的知識與技術，實踐你的瘋狂點子並分享給我知道。另外，如果你有興趣了解如何使用更進階、更強大的語言模型來生成比 LSTM 還厲害的金庸小說，可以參考另篇文章： 直觀理解 GPT-2 語言模型並生成金庸武俠小說 。 就這樣啦！現在我得回去看還沒看完的天龍八部了。 致敬 僅用這篇微不足道的文章向 金庸 致敬，感謝他帶給我們那麼多膾炙人口的故事。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html","loc":"https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html"},{"title":"我從 AI For Everyone 學到的 10 個重要 AI 概念","text":"在這個人機共存的年代，每個人都應該去嘗試瞭解並運用人工智慧這個超能力。思考自己未來在這個變化快速的世界的定位。 曾經領導 Google Brain 的 吳恩達 教授這幾天公開了新的 Coursera 課程： AI For Everyone 。這堂課不談技術術語，專注在與非技術人士以及企業經理人說明： 何謂 AI 如何建立 AI 專案 如何在企業內部建立 AI 基礎 AI 與社會的關係 課程內容精要，總結了不少他多年在 Google Brain、百度裡領導 AI 團隊所累積的寶貴經驗。這堂課也提到了不少 AI Transformation Playbook 裡頭的內容。 Coursera 上的 AI For Everyone （ 圖片來源 ） 雖然課程中很多時候是以 CEO 或是企業管理者的角度說明 AI 概念，但我認為每個人都可以用 個人 角度，從本課學到不少有用的建議以及思考框架。有了這些概念，可以幫助我們在這個變化快速的 AI 潮流中掌握好自己手上的船舵並順利航行。 本文將列舉出我認為本課中最值得記住的 10 個 AI 概念，希望能讓你馬上學到些東西。 值得一提的是，這篇不少概念是我自己的心得總結，而你在上完課後肯定會有其他重要見解。事實上，我會推薦你在閱讀本文後就找時間實際去上這堂課，或是透過 其他方式 進一步了解 AI。 30 秒 AI 大局觀 以下就是 10 個我認為 AI For Everyone 這堂課傳達的重要概念懶人包。如果你一秒鐘幾十萬上下，可以只看這節就好： 講到 AI，我們通常是指狹義的 AI 而非終結者 多數 AI 應用是讓機器學會一個對應關係 大數據、神經網路及運算能力是 AI 成功關鍵 只需花費你 1 秒的任務，大都可由 AI 自動化 對 AI 的態度不應過度樂觀，但也不必太悲觀 AI 偏見難解，但或許比消除人類偏見簡單 擁抱 AI 的最好方法是將其與領域專業結合 機器學習和資料科學的產出分別是系統和洞見 AI 時代，你得思考未來自己想要扮演的角色 終身學習在這個年代前所未有地重要 是的，既然是 AI For Everyone，自然沒有什麼艱深內容。但就像吳恩達教授在課程裡頭所說的，我相信這些基本的核心思想可以引導我們在這個 AI 時代更有方向且順利地前進。 本文接著會搭配課程投影片，針對上面提到的一些概念做點簡單的補充說明，供你參考。 講到 AI，我們通常是指狹義的 AI 而非終結者 現在媒體整天報導的人工智慧（ A rtifical I ntelligence, AI）應用如： 智慧音響 自動駕駛 人臉辨識 圖像分類 推薦系統 機器翻譯 背後皆是狹義的 AI（ A rtificial N arrow I ntelligence, ANI）。 儘管很多 AI 應用的表現甚至已經比人類還優秀，這些 AI 基本上都專注在完成「特定」的任務；這跟科幻電影如魔鬼終結者裡頭，能跟人類以一樣的方式思考並做「任何」事情的通用 AI（ A rtificial G eneral I ntelligence, AGI）是有很大差異的。 ANI 與 AGI 的差異 （ 圖片來源 ） 儘管開發出 AGI 是很多研究者的終極夢想，但事實上現行的科技離實現 AGI 還有好一段距離。 多數 AI 應用是讓機器學會一個對應關係 如同我們在以前的文章裡頭看過的： 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南 AI 如何找出你的喵：直觀理解卷積神經網路 從彼此學習 - 淺談機器學習以及人類學習 大部分的機器學習以及 AI 應用本質上都是讓電腦學會一個映射函數（Mapping Function），幫我們將輸入的數據 A 對應到理想的輸出 B： 郵件分類：電子郵件 -> 是否為垃圾郵件 語音辨識：音訊檔案 -> 文本 機器翻譯：英文文本 -> 中文文本 抽離技術細節，許多 AI 應用事實上就是一個個幫我們將輸入 A 轉換成輸出 B 的映射函數 （ 圖片來源 ） 要實現這種 AI 應用，最常被使用的方法是 監督式學習（Supervised Learning） ：給予機器大量的成對數據，告訴它什麼樣的 A 要對應到什麼樣的 B，並讓機器最後自己學會如何將任意的 A 轉換成理想的 B，達到自動化的目的。 現在多數的 AI 應用是透過人工神經網路，讓機器學會如何將輸入 A 轉成輸出 B （ 圖片來源 ） 大數據、神經網路及運算能力是 AI 成功關鍵 要實現能幫助人類做複雜判斷的 AI 技術有很多種，但近年真正讓 AI 大紅大紫的是 深度學習（Deep Learning） 以及 人工神經網路（Artifical Neural Network） 。 Google 的 TensorFlow 團隊讓你可以在瀏覽器裡頭體驗深度學習以及神經網路 （ 圖片來源 ） 值得一提的是，你或許常聽到「神經網路跟人腦運作方式相同」的這種說法，但事實上如果你問相關人士對這種意見的看法的話，得到的答案常常是「兩者天差地遠」。 儘管神經網路的運作方式跟我們神奇的大腦不完全一致，搭配大量數據以及前面提到的監督式學習，越大的神經網路通常可以在特定任務有越好的表現。 先不論所需的計算資源，「越大量的數據以及越大型的神經網路能帶來更好的表現」這件事情對許多大企業來說，是件美好到不行的事情 （ 圖片來源 ） 雖然這樣的現象令人振奮，但別忘記 大型神經網路的運作 大量數據的處理 這兩件事情都意味著需要更大量的電腦運算能力。而很多時候一般人是沒有這樣的運算資源的。 值得慶幸的是，很多以深度學習為基礎的 AI 常常有個很好的特性：透過 遷移學習（Transfer Learning） ，我們能將事先已經用大量計算資源做訓練，並在任務 A 表現優異的 AI 做些簡單修改，就能讓修改過後的 AI' 能在相似的任務 B 也表現不錯。 一個利用遷移學習，把在圖像辨識中表現優異的 AI 拿來辨識貓咪的例子 （ 圖片來源 ） 這時候就算你只有少量數據以及不多的計算資源，也能利用 AI 完成以往難以想像的任務。 只需花費你 1 秒的任務，（未來）大都可由 AI 自動化 這項概念是吳恩達教授在課程裡所提到的「一秒原則」，可以讓你用來判斷一個任務是否能用 AI 做自動化的準則。 一秒原則 （ 圖片來源 ） 透過監督式學習以及大量成對 A & B 數據，我們可以讓很多以往被認為非常複雜，但人腦僅需 1 秒鐘就能解決的任務透過 AI 來自動化，讓我們的生活更加輕鬆。 當然，這個簡化的原則並不是放諸四海皆準，但可以做為一個不錯的參考基準。 對 AI 的態度不應過度樂觀，但也不必太悲觀 儘管我們已經清楚現代 AI 的威力，仍需注意 AI 並不是萬能藥，無法（完美地）解決或自動化所有人類的課題。 比方說 有研究嘗試把自然語言轉成 SQL ，但短期內一個資料科學家自己寫 SQL 查詢數據可能還是比較有效率。儘管 AI 不能（完美地）做到任何事情，我們也不該對 AI 失望，斷定下一個 AI 冬天必定會到來。 AI 的金髮女孩原理：對 AI 的態度不要過熱（樂觀），也不要過冷（悲觀），而是要剛剛好 （ 圖片來源 ） 現在可以肯定的是 AI 已經，而且也會繼續改變我們未來以及下一代的生活型態。 最重要的是理性地理解 AI 能做到什麼，在能活用的時候善加利用它，同時不抱著「 AI 能解決所有問題」的不切實際幻想。 小知識：著名格林童話故事「金髮女孩與三隻熊」，講述金髮小女孩走進了三隻熊的房子，她發現當中有三碗粥，一碗太熱，一碗太冷，最後她揀選了不冷不熱的第三碗。之後她又試了三張椅子及睡床，最後她揀選了最合適的小椅子及睡床坐下及睡覺。 AI 偏見難解，但或許比消除人類偏見簡單 在利用監督式學習的方式訓練 AI 的時候，我們常常會使用現實世界的資料讓機器學習。 好消息是因為現在數位化以及網際網路的發達，我們有非常多數據可以交給 AI 學習；壞消息是這些數據時常反映了人類數十年甚至幾個世紀的偏見。 用這些數據訓練出來的 AI 系統就像是面照妖鏡，也會不可避免地學會這些偏見（Bias）。 您的瀏覽器不支援影片標籤，請留言通知我：S 在我們將具有人類偏見的數據交給 AI 學習時，不可避免地會創造出具有偏見的系統 知名的例子有： 以白人照片訓練出來的人臉辨識系統在辨識深色膚色的人種時表現很差 自動化雇用的 AI 系統對女性存有偏見 銀行的自動信用評比 AI 系統對某些族群產生偏見 以下則是另一個課堂中提出的例子： 讓 AI 從維基百科學習英文詞彙之間的統計關係後，發現 AI 認為「男人之於電腦工程師」的關係等於「女人之於家庭主婦」 （ 圖片來源 ） 上例或許稱不上歧視，但很明顯是偏見，一種長久存在於人類社會的性別偏見。 因為很多時候這些 AI 系統是學習一種統計關係，因此在此例中，AI 只是忠誠地呈現我們社會的用字習慣罷了。 要消除 AI 的這些偏見並不容易，但仔細想想，這可能比消除人們腦中數十年的偏見要來的簡單，而且振奮人心。 您的瀏覽器不支援影片標籤，請留言通知我：S 在享有強大 AI 科技的同時，我們希望最終也能將人類的偏見從這些 AI 系統中摒除 這件事情當然不簡單，但卻非常值得一試。 當然，你可以選擇不思考這些 AI 倫理、偏見問題，相信建立 AI 系統的這些工程師們立意良善以及夠細心，能幫我們將 AI 系統裡的偏見移除，並讓其做出最合適的判斷。 儘管如此，意識到再厲害的 AI 系統內部也可能存在如同人類的偏見，進而導致各種不公平的社會問題這件事情也是很有幫助的。 擁抱 AI 的最好方法是將其與領域專業結合 想要學習 AI，不需要打掉重練。 雖然現在 AI 相關領域十分熱門，究其根本也就只是一種工具/技術。而且 AI 技術接下來會越來越平民化，上手的門檻會越來越低。 因此比起現在轉行當 AI 工程師，你要先做的應該是想辦法利用自己工作累積的領域知識（Domain Knowledge）以及洞見（Insight），找出能應用 AI 改善的地方，進而創造出專屬於你或企業的競爭優勢。 機器學習和資料科學的產出分別是系統和洞見 機器學習（ M achine L earning, ML）以及資料科學（ D ata S cience, DS）這兩個詞彙常常結伴出現，且依照不同企業其定義都有所不同。因此，不在這塊領域裡的人常常不知道兩者的差異。 一般來說，在企業內的 ML 專案大都分為 3 個階段： 收集數據 訓練模型 部署模型 一般的 ML 專案的最終產物為機器學習模型，或是能夠持續運作的 AI 系統 （ 圖片來源 ） 而 DS 專案的步驟則為： 收集數據 分析數據 建議行動/假說 一般的 DS 專案的最終產物為有用的假說或是洞見 （ 圖片來源 ） 兩者皆需原始數據做為輸入，且皆有機會使用 AI / ML 技術來解決、分析問題，但最終的產出形式時常不同。 總結來說，ML 專案較注重在軟體工程方面，且最終希望產出一個以 AI 為基礎的線上系統；DS 專案的結果則可能是一份幫助經營者做重大投資決策的投影片報告。 AI 時代，你得思考未來自己想要扮演的角色 AI 目前正是顯學，不少人決定進入這塊領域，而現在跟 AI 相關的職業就有好多種，比方說： 資料科學家 機器學習工程師 機器學習研究者 軟體工程師 資料工程師 AI 專案管理人 等等。而且隨著 AI 的影響力持續擴大，未來可能還會出現新的相關職業。 AI 時代裡有各式各樣的相關職業，找尋最適合你的職業很重要 （ 圖片來源 ） 我們在這邊不會一一列出每個職業的工作內容，但忠實讀者會發現，事實上我們在 數據科學 MMORPG 上線！你，選好自己的角色了嗎？ 一文中就已經討論過類似的話題了。 要踏入 AI 這塊領域，除了資料科學家以外，你還有很多選擇。思考你的強處以及興趣所在，選擇最適合的職業發揮所長是最理想的。 終身學習在這個年代前所未有地重要 如同課程中吳恩達教授所說的，你並不需要取得一個 AI master 才能開始進行 AI 專案。很多時候利用 線上課程 或是 網路上的深度學習資源 就可以開始你的第一個 AI 專案了。 事實上，學習 AI For Everyone 這堂課就是一個不錯的開始。網路上也有很多優質的部落格或教學文章等待你的探索。 AI 領域近年發展神速，要學習 AI，用上一代「讀幾年書，出來用一輩子」的概念是行不通的。台大電機系的 李宏毅教授 就曾說過：「在深度學習的領域，超過五年就是遠古時代了」。 因此如果你決定踏上學習 AI 的這條路，就做好跟我一起終身學習的心理準備吧！ 結語 看到這裡，相信你已經了解 AI For Everyone 裡頭 10 個最重要的概念了，恭喜！ 這些概念大多是我將課程裡頭擷取出的核心概念，佐以自己從事資料科學家以來的心得感想。希望閱讀完此文的你有學到點東西，或是獲得些啟發。 課堂內容是主菜，我的個人心得是調味料，希望你喜歡這道菜，並分享給朋友知道 為了幫助你回憶，現在讓我再次將本文提到的概念一一列出： 講到 AI，我們通常是指狹義的 AI 而非終結者 多數 AI 應用是讓機器學會一個對應關係 大數據、神經網路及運算能力是 AI 成功關鍵 只需花費你 1 秒的任務，大都可由 AI 自動化 對 AI 的態度不應過度樂觀，但也不必太悲觀 AI 偏見難解，但或許比消除人類偏見簡單 擁抱 AI 的最好方法是將其與領域專業結合 機器學習和資料科學的產出分別是系統和洞見 AI 時代，你得思考未來自己想要扮演的角色 終身學習在這個年代前所未有地重要 AI 當然有其侷限性，但只要你能找出應用它的任務，就能將其轉換成你的超能力，可能性無窮大。 跟隨完我的思路歷程，現在輪到你動腦回答下面問題了： 你有什麼個人或是企業的課題，是可以透過 AI 改善或是提升價值的呢？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html","loc":"https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html"},{"title":"由淺入深的深度學習資源整理","text":"h3 { margin-top: 3rem; } h3 a { font-size: 18px; } 不聞不若聞之，聞之不若見之，見之不若知之，知之不若行之，學至於行之而止矣。 ── 《荀子．儒效》 這段話翻成白話文就是「沒聽過比不上聽過；聽過比不上實際看過；看過則比不上實際了解；而了解又不如動手實踐。唯有身體力行才能真正地學到東西。」 這句古老的諺語向我們傳達了「實踐」的重要以及學習的幾個過程。 做為一門學問， 深度學習 也是同樣道理。 僅說自己對深度學習有興趣或是有關注（聞、見），但卻沒有實際花時間去深入了解或實際應用（知、行）是無法真正學會深度學習的。 雖說如此，不了解深度學習能拿來做什麼的人或許還不少。 我嘗試將自己在學習過程中蒐集到的重要資源由淺入深地做些整理。 希望透過此文，能讓在各個學習階段的你都能從這裡獲得些什麼，並實際動手學習、探索發展快速的深度學習世界。 本文內容會持續被更新，你可以定期回來看看或是關注這個 Github Repo 。 這裡紀錄了我在學習 深度學習 時蒐集的一些線上資源。內容由淺入深，而且會不斷更新，希望能幫助你順利地開始學習：） 本文章節 遊玩空間 線上課程 實用工具 其他教材 優質文章 經典論文 其他整理 遊玩空間 這節列舉了一些透過瀏覽器就能馬上開始遊玩 / 體驗深度學習的應用。作為這些應用的使用者，你可以先高層次、直觀地了解深度學習能做些什麼。之後有興趣再進一步了解背後原理。 這小節最適合： 想要快速體會深度學習如何被應用在真實世界的好奇寶寶 想要直觀理解 類神經網路（Artifical Neural Network） 運作方式的人 想從別人的深度學習應用取得一些靈感的開發者 Deep Playground ConvNetJS Deep Playground 由 Tensorflow 團隊 推出，模擬訓練一個類神經網路的過程並了解其運作原理 可以搭配這篇 Introduction to Neural Networks: Playground Exercises 學習 ConvNetJS 訓練類神經網路來解決經典的 MNIST 手寫數字辨識問題 、 圖片生成 以及 增強式學習 由 Tesla 的 AI 負責人 Andrej Karpathy 建立 Magenta Google AI Experiments Magenta 一個利用 機器學習 來協助人們進行音樂以及藝術創作的開源專案 可以在網站上的 Demo 頁面 嘗試各種由深度學習驅動的音樂 / 繪畫應用（如彈奏鋼琴、擊鼓） Google AI Experiments 這邊展示了接近 40 個利用圖片、語言以及音樂來與使用者產生互動的機器學習 Apps，值得慢慢探索 知名例子有 Quick Draw 以及 Teachable Machine ，將在下方介紹 Quick Draw Teachable Machine Quick Draw 由 Google 推出的知名手寫塗鴉辨識，使用的神經網路架構有常見的 卷積神經網路 CNN 以及 循環神經網路 RNN 該深度學習模型會不斷將最新的筆觸當作輸入來預測使用者想畫的物件。你會驚嘆於她精準且即時的判斷 Teachable Machine 利用電腦 / 手機上的相機來訓練能將影像對應到其他圖片、音訊的神經網路，饒富趣味 透過這例子，你將暸解機器學習的神奇之處以及其侷限所在 Fast Neural Style TensorFlow.js Fast Neural Style 展示如何使用 WebGL 在瀏覽器快速地進行 神經風格轉換 Neural Style Transfer 你可以選擇任何一張圖片，並在此網站上將其畫風轉變成指定的藝術照 Deepart.io 也提供類似服務 TensorFlow.js TensorFlow.js 頁面有多個利用 JavaScript 實現的深度學習應用，如上圖中的 人類姿勢估計 Human Pose Estimation 。 你可以在該應用裡頭打開自己的攝影機，看該應用能不能偵測到你與朋友的姿勢。 GAN Lab Talk to Transformer GAN Lab 對抗生成網路（ G enerative A dversarial N etwork，簡稱GAN） 是非監督式學習的一種方法，通過讓兩個神經網路相互博弈的方式進行學習。此網站以 TensorFlow.js 實作 GAN 中兩個神經網路的學習過程，幫助有興趣的你更直觀地理解神奇的 GAN 的運作方式 Talk to Transformer 展示了一個由 OpenAI 推出，名為 GPT-2 的無監督式語言模型 。該模型以 Google 發表的神經網路架構 Transformer 為基底，在給定一段魔戒或是復仇者聯盟的文字內容，該模型可以自己生成唯妙唯俏的延伸劇情。你也可以嘗試 AllenAI GPT-2 Explorer 來觀察 GPT-2 預測下個字的機率。 想要深入了解 Transformer 或 GPT-2，推薦閱讀： 淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中 直觀理解 GPT-2 語言模型並生成金庸武俠小說 The Illustrated GPT-2 (Visualizing Transformer Language Models) NVIDIA AI PLAYGROUND Grover NVIDIA AI PLAYGROUND 提供 GauGAN 的線上展示，讓你可以利用簡單的筆觸來生成真實世界的風景圖片，也能上傳自己的圖片做風格轉換 提供 Image Impainting 服務，讓使用者自由抹去部分圖片並讓 AI 自動生成被抹去的區塊 Grover 一個偵測 / 生成神經假新聞（Neural Fake News）的研究，其網頁展示如何自動生成假新聞。 Waifu Vending Machine This Waifu Does Not Exist Waifu Vending Machine Waifu 來自日文 ワイフ，指的是一些非常受到歡迎、且被不少玩家/觀眾視為妻子的動漫女性角色。 Sizigi Studios 團隊利用 GAN 隨機初始 16 名虛擬動漫角色，讓使用者可以進一步依照喜愛來創造專屬於自己的 Waifu。 Waifu Vending Machine 產生的 Waifu 品質很高，使用者可以下載並分享自己創造的 Waifu，也可以選擇購買印製該 Waifu 的海報與抱枕。 This Waifu Does Not Exist 以 Nvidia 的 StyleGAN 隨機生成的 Waifu（右圖左側）。作者 Gwern 同時也使用 開源的小型 GPT-2 隨機生成一段動漫劇情（右圖右側）。自釋出後已超越一百萬使用者拜訪該網站。 你也可以用大螢幕查看作者的另個相關網站： These Waifus Do Not Exist ，用全畫面一次「觀賞」數十名隨機生成的 Waifus。 AI Notes Anomagram AI Notes AI Notes 是 吳恩達的 Deep Learning 專項課程 的輔助教材，使用數學證明以及由 TensorFlow.js 建立的線上 demo 讓你可以直觀地學習 如何初始化神經網路權重 及 如何最佳化模型權重 縮圖為 Parameter optimization in neural networks 單元中使用不同 Optimiziers 訓練模型的線上 demo Anomagram Anomagram 是一個以 Tensorflow.js 實作，可以建立、訓練並測試能夠用來做異常檢測的 Autoencoder。 線上課程 看完 遊玩空間 的大量實際應用，相信你已經迫不及待地想要開始學習強大的深度學習技術了。 這節列舉了一些有用的線上課程以及學習教材，幫助你掌握深度學習的基本知識（沒有特別註明的話皆為免費存取）。 另外值得一提的是，大部分課程都要求一定程度的 Python 程式能力。 李宏毅教授的機器學習 / 深度學習課程 Deep Learning Specialization @ Coursera 李宏毅教授的機器學習 / 深度學習課程 大概是全世界最好、最完整的深度學習 中文 學習資源，且作業皆提供 Colab 筆記本範例。 影片內容涵蓋基本理論（約 10 小時觀看時間）一直到進階的 生成對抗網路 GAN 以及 強化學習 RL 。 想學語音辨識或是自然語言處理則可參考教授的 用深度學習處理人類語言 。 李宏毅机器学习笔记(LeeML-Notes，簡體) 則將教授上課的影片內容轉為筆記，方便瀏覽課程內容。 Deep Learning Specialization @ Coursera 原 Google Brain 的 吳恩達 教授開授的整個深度學習專項課程共分五堂課，從 神經網路的基礎 到能夠進行機器翻譯、語音辨識的 序列模型 ，每堂課預計 1 個月完成，收費採訂閱制 程式作業會交互使用 Numpy 、 Keras 以及 TensorFlow 來實作深度學習模型 Practical Deep Learning For Coders @ fast.ai Deep Learning @ Kaggle Learn Practical Deep Learning For Coders @ fast.ai 7 週課程，一週約需安排 10 小時上課。該課程由 傑里米·霍華德 來講解深度學習，其在知名數據建模和數據分析競賽平台 Kaggle 維持兩年的世界第一 Deep Learning @ Kaggle Learn 14 堂課程，主要使用 TensorFlow 實作深度學習模型 內容主要專注在 電腦視覺（Computer Vision） 以及如何應用 遷移學習（Transfer Learning） Elements of Artificial Intelligence MIT Deep Learning Elements of Artificial Intelligence 芬蘭最高學府 赫爾辛基大學 推出的 AI 課程。此課程目的在於讓所有人都能了解 AI，不需要任何程式經驗。這堂課非常適合完全沒有接觸過深度學習或是相關領域的人 課程分 6 個部分，包含「何謂 AI ？」、「真實世界的 AI」、「機器學習」以及「神經網路」等章節 MIT Deep Learning 麻省理工學院推出的深度學習課程，內容包含深度學習基礎、深度強化學習以及自動駕駛相關知識。 Github Repo 包含了多個教學筆記本，值得參考。 上圖是 DeepTraffic ，由 MIT 的研究科學家 Lex Fridman 推出的一個深度強化學習競賽。此競賽目標是建立一個可以在高速公路上駕駛汽車的神經網路。你可以在 這裡 看到線上 Demo 以及詳細說明。 6.S191: Introduction to Deep Learning AI For Everyone MIT 6.S191 Introduction to Deep Learning 麻省理工學院推出的另一堂基礎深度學習課程，介紹深度學習以及其應用。內容涵蓋機器翻譯、圖像辨識以及更多其他應用。此課程使用 Python 以及 TensorFlow 來實作作業，並預期學生具備基礎的微積分（梯度下降、鏈鎖律）以及線性代數（矩陣相乘）。 AI For Everyone Coursera 課程。 吳恩達 教授在這堂簡短的課程裡頭，針對非技術人士以及企業經理人說明何謂 AI、如何建立 AI 專案以及闡述 AI 與社會的關係。此課程十分適合沒有技術背景的讀者。 從 AI For Everyone 學到的 10 個重要 AI 概念 則是我個人上完課後整理的心得分享。 CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS224n: Natural Language Processing with Deep Learning 由 史丹佛 AI 實驗室的 Christopher Manning 教授 從語言學、計算機科學的角度講述自然語言處理的所有必要知識，是想要打好 NLP 基礎的人不可不學的一堂課。課程約有 20 部影片，每部約長 1.5 小時。 CS231n: Convolutional Neural Networks for Visual Recognition 由 史丹佛 Vision Lab 的李飛飛（Fei-Fei Li）教授 等人以 圖像分類 任務為軸心，講述卷積神經網路以及所有電腦視覺的相關基礎知識。這是想要學會使用（卷積）神經網路來處理圖像數據的人不可不學的一堂課。 Youtube 上有 16 部 2017 年的課程錄影 ，每部約長 1 小時。 課程中也包含了不少線上展示，如 線性分類器的 loss 視覺化 、 kNN demo 以及圖像分類的 CIFAR-10 demo 。 實用工具 這節列出一些在你的深度學習路上可以幫得上些忙的工具。 Colaboratory TensorBoard Colaboratory 由 Google 提供的雲端 Jupyter 筆記本環境，讓你只要用瀏覽器就能馬上開始訓練深度學習模型。你甚至還可以使用一個免費的 Tesla K80 GPU 或 TPU 來加速訓練自己的模型 該計算環境也能與自己的 Google Drive 做連結，讓運算雲端化的同時將筆記本 / 模型結果都同步到自己的筆電上 TensorBoard TensorBoard 是一個視覺化工具，方便我們了解、除錯並最佳化自己訓練的深度學習模型 除了 TensorFlow 以外，其他基於 Python 的機器學習框架大多也可以透過 tensorboardX 來使用 TensorBoard Embedding Projector Lucid Embedding Projector 我們時常需要將圖片、文字轉成 高維數字向量 Embedding 以供神經網路處理，而 Projector 能將此高維向量投影到 2、3 維空間上方便我們理解這些數據 Projector 網站讓你在線上探索幾個常見的資料集，但事實上你也可以 利用 Tensorboard 來視覺化自己的數據 。 Lucid Lucid 是一個嘗試讓神經網路變得更容易解釋的開源專案，裡頭包含了很多視覺化神經網路的筆記本 你可以直接在 Colab 上執行這些筆記本並了解如何視覺化神經網路 Papers with Code What-If Tool Papers with Code 將機器學習的學術論文、程式碼實作以及 SOTA 的評價排行榜全部整理匯總在一起的網站，非常適合想要持續追蹤學術及業界最新研究趨勢的人 在這邊可以瀏覽包含電腦視覺、自然語言處理等各大領域在不同任務上表現最好的論文、實作以及資料集 What-If Tool 一個與 TensorBoard 以及 Jupyter Notebook 整合的探索工具，讓使用者不需寫程式碼就能輕鬆觀察機器學習模型的內部運作以及嘗試各種 What-if 問題（如果 ~ 會怎麼樣？） 基本上就是用來觀察 已訓練 的模型在測試資料集上的表現。利用此工具，使用者可以了解（不僅限於）以下的問題：模型在各類別數據上的表現有無差距？模型是否存在偏見？應該如何調整 Native / Positive False 的比例？ 此工具的一大亮點在於讓非專業領域人士也能探索、理解 ML 模型表現。且只要給定模型與資料集, 就不需要每次為了 What-if 問題就寫用過即丟的程式碼 BertViz ML Visuals BertViz BertViz 是一個視覺化自注意力機制的工具，可以用來理解如 BERT 、 GPT-2 及 RoBERTa 等知名 NLP 模型的內部運作 以下則是幾篇透過 BertViz 來直觀解說 BERT 與 GPT-2 的文章 進擊的 BERT：NLP 界的巨人之力與遷移學習 直觀理解 GPT-2 語言模型並生成金庸武俠小說 ML Visuals ML Visuals 是一個社群開源項目，提供超過 100 個常見的機器學習概念 / 深度學習架構圖，可讓任何人在學術論文或是文章直接使用這些圖表。 所有圖表都可以直接從 Google slide 上觀看並使用。建議前往 Github repo 查看最新版本。 其他教材 除了 線上課程 以外，網路上還有無數的學習資源。 這邊列出一些推薦的深度學習教材，大多數皆以數據科學家常用的 Jupyter 筆記本的方式呈現。 你可以將感興趣的筆記本導入 實用工具 裡提到的 Colaboratory（Colab） ，馬上開始學習。 Seedbank Deep Learning with Python Seedbank 讓你可以一覽 Colab 上超過 100 個跟機器學習相關的筆記本，並以此為基礎建立各種深度學習應用 熱門筆記本包含 神經機器翻譯 、 音樂生成 以及 DeepDream 因為是 Google 服務，筆記本大多使用 TensorFlow 與 Keras 來實現模型 Deep Learning with Python Keras 作者 François Chollet 在 Deep Learning with Python 一書中用到的所有筆記本。每個筆記本裡頭都清楚地介紹該如何使用 Keras 來實現各種深度學習模型，十分適合第一次使用 Python 實現深度學習的讀者 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南 一文的 Keras 程式碼大多基於此 繁體中文的翻譯書籍則為 Deep learning 深度學習必讀 - Keras 大神帶你用 Python 實作 Keras 在 TensorFlow 2.0 中 為其最重要的高層次 API Stanford CS230 Cheatsheets practicalAI Stanford CS230 Cheatsheets 史丹佛大學的 深度學習課程 CS230 釋出的深度學習小抄總結了目前最新的 卷積神經網路 及 循環神經網路 知識，還包含了 訓練深度學習時需要使用到的技巧 ，十分強大 此小抄最適合已經熟悉基礎知識的同學隨時複習運用。你也可以從他們的 Github Repo 下載包含上述所有內容的 超級 VIP 小抄 除了深度學習以外，你也可以查看 CS229 機器學習課程的小抄 practicalAI 在 Github 上超過 1 萬星的 Repo。除了深度學習，也有介紹 Python 基礎 及 Pandas 的使用方式 使用 PyTorch 框架來實現深度學習模型，且所有內容都是 Jupyter 筆記本，可以讓你在 Colab 或本地端執行 AllenNLP Demo Hands-on Machine Learning 2 AllenNLP Demo 清楚地展示了如 機器理解 、 命名實體識別 等多個自然語言處理任務的情境。每個任務的情境包含了任務所需要的輸入、SOTA 模型的預測結果以及模型內部的注意力機制，對理解一個 NLP 任務的實際應用情境有很大幫助 AllenNLP 是一個由 AI2 以 PyTorch 實現的自然語言處理函式庫 Hands-on Machine Learning 2 前 YouTube 影片分類 PM Aurélien Geron 教你如何透過 Scikit-Learn、Keras 以及 TensorFlow 2 來進行機器學習以及深度學習任務與應用的筆記本彙整。 第二版專注在 TensorFlow 2，其 Github repo 已有超過 6 千顆星， 第一版 則有高達 2 萬星。 優質文章 這邊列舉了一些幫助我釐清重要概念的部落格以及網站，希望能加速你探索這個深度學習世界。 只要 Google 一下就能發現這些部落格裡頭很多文章都有中文翻譯。但為了尊重原作者，在這邊都列出原文連結。 Distill 用非常高水準且互動的方式來說明複雜的深度學習概念。 Yoshua Bengio 、 Ian Goodfellow 及 Andrej Karpathy 等知名人士皆參與其中 R2D3: 圖解機器學習 利用非常直覺易懂的視覺化來說明機器學習，連結為中文版 Christopher Olah's blog 詳細解釋不少深度學習概念。作者在 這篇 就詳細地解釋了 長短期記憶 LSTM 的概念與變形；在 這篇 則解釋何為 CNN 的卷積運算 Jay Alammar's blog 以清楚易懂的視覺化解釋深度學習概念。 這篇 用大量易懂的動畫說明 神經機器翻譯 ，而在 這篇 則介紹如何利用如 ELMo 、 BERT 等預先訓練過的強大模型在自然語言處理進行 遷移學習 Andrej Karpathy's blog 現為 Tesla AI 負責人的 Andrej Karpathy 在 這篇 明確說明何謂循環神經網路 RNN。文中提供不少應用實例及視覺化來幫助我們理解 RNN 模型究竟學到了什麼，是學習 RNN 的朋友幾乎一定會碰到的一篇文章 經典論文 這邊依發表時間列出深度學習領域的經典 / 重要論文。 為了幫助你快速掌握論文內容以及歷年的研究趨勢，每篇論文下會有非常簡短的介紹（WIP）。 但我們推薦有興趣的人自行閱讀論文以深入了解。 自然語言處理 Natural Language Processing (NLP) 2003/02 A Neural Probabilistic Language Model 2013/01 Efficient Estimation of Word Representations in Vector Space 2013/08 Generating Sequences With Recurrent Neural Networks 2014/09 Neural Machine Translation by Jointly Learning to Align and Translate 2015/08 Effective Approaches to Attention-based Neural Machine Translation 2015/12 Semi-supervised Sequence Learning 推出一套無監督式的預訓練方法。使用無標籤數據訓練後的 RNN 模型在之後的監督式任務表現更好 2017/06 Attention Is All You Need Google 推出新的神經網路架構 Transformer 。這個基於自注意力機制的架構特別適合語言理解任務 2017/06 One Model To Learn Them All 2017/08 Learned in Translation: Contextualized Word Vectors 監督式預訓練。透過 BiLSTM 與 Encoder-Decoder 架構預先訓練機器翻譯任務並將訓練後的 Encoder 拿來做特徵擷取。將 Encoder 的輸出作為語境向量（Context Vectors, CoVe）處理下游任務 2018/01 Universal Language Model Fine-tuning for Text Classification 2018/02 Deep contextualized word representations ELMo 詞向量 ，利用兩獨立訓練的 LSTM 獲取雙向訊息 2018/06 Improving Language Understanding by Generative Pre-Training OpenAI 利用無監督式預訓練以及 Transformer 架構訓練出來的模型表現在多個 NLP 任務表現良好。約使用 8 億詞彙量的資料集 2018/10 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Google 暴力美學。利用深層 Transformer 架構、2 個精心設計的預訓練任務以及約 33 億詞彙量的資料集訓練後，得到表現卓越的語言代表模型，打破 11 項 NLP 任務紀錄 2019/05 MASS: Masked Sequence to Sequence Pre-training for Language Generation Microsoft 利用 Encoder-Decoder 架構以及連續遮罩（consecutive mask）將 BERT 推廣到自然語言生成（NLG）類型任務 2019/05 Unified Language Model Pre-training for Natural Language Understanding and Generation 預訓練階段利用不同遮罩控制 context，同時訓練雙向 LM、單向 LM 以及 Seq2Seq LM。其產生的預訓練模型可以處理 NLU 以及 NLG 任務，並在不加入外部數據的情況下打敗 BERT 在 GLUE 的紀錄 電腦視覺 Computer Vision (CV) 類神經網路架構 Neural Network Architecture 1998/01 Gradient-Based Learning Applied to Document Recognition (LeNet-5) 2012/12 ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) 2014/06 DeepFace: Closing the Gap to Human-Level Performance in Face Verification (DeepFace) 2014/09 Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) 2014/09 Goint deeper with convolutions (GoogLeNet) 2014/11 Fully Convolutional Networks for Semantic Segmentation 2015/05 U-Net: Convolutional Networks for Biomedical Image Segmentation (U-Net) 2015/12 Deep Residual Learning for Image Recognition (ResNet) 2017/04 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (MobileNets) 2017/07 ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices (ShuffleNet) 資料集 Dataset 2009/06 ImageNet: A Large-Scale Hierarchical Image Database (ImageNet) 物體偵測與切割 Object Detection and Segmentation 2013/11 Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN) 2013/12 OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks (OverFeat) 2015/04 Fast R-CNN 2015/06 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Faster R-CNN) 2015/06 You Only Look Once: Unified, Real-Time Object Detection (YOLO) 2015/12 SSD: Single Shot MultiBox Detector (SSD) 2016/12 YOLO9000: Better, Faster, Stronger (YOLOv2) 2017/03 Mask R-CNN 2018/04 YOLOv3: An Incremental Improvement (YOLOv3) 生成模型 Generative Models 2014/06 Generative Adversarial Networks (GAN) 2015/13 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN) 2017/01 Wasserstein GAN (WGAN) 2017/03 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) 其他整理 這邊列出其他優質的資源整理網站 / Github Repo，供你繼續探索深度學習。 deep-learning-ocean 整理了不少深度學習資源，但最值得參考的是數據集以及論文的分類整理。 待辦事項 還有不少內容正在整理，以下是目前我們打算增加的一些項目： 深度學習中英術語對照表 值得追蹤的業界 / 學界影響人物清單 無圖的資源列表版本 一些 Jupyter Notebook 範例 而我們也會持續將新資源加入如 實用工具 、 優質文章 等列表裡頭。 如何貢獻 非常歡迎你一起加入改善這個 Repo，讓更多人有方向地學習 Deep Learning：） 如果你有 其他值得推薦的深度學習資源 針對此 Repo 內容的改善建議 其他任何你想得到的東西 都歡迎你 提出新的 Issue 來讓我們知道。 如果是想增加新資源的話，只附上連結也是沒有問題的，謝謝！ 最後，如果你覺得本文實用，還請幫我分享此文並給 Github Repo 一個小星星。 這樣可以讓更多人注意到這些寶貴資源的存在並開始有方向的學習，謝謝！ 有再多資源，沒有親自動手做都是無法真正地學到東西的。因此，最後的最後讓我再次強調主動學習的重要： 告訴我資訊，我只會忘記；教導我知識，我會記得；讓我實際參與，我將能真正地學到東西。 ── 班傑明·富蘭克林 所以，現在就開始學習吧！","tags":"Miscellaneous","url":"https://leemeng.tw/deep-learning-resources.html","loc":"https://leemeng.tw/deep-learning-resources.html"},{"title":"進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南","text":"希望這篇文章能成為你前往自然語言處理世界的最佳橋樑。 自從 11 月從比利時 EMNLP 回來後，最近工作之餘都在學習 自然語言處理 （Natural Language Processing, 後簡稱為 NLP）。 上上週陰錯陽差地參加了一個 Kaggle 競賽。在該比賽中，我實際應用到不少前陣子所學的 NLP 知識，也獲得不少心得。 因此我想借此機會，在文中鉅細靡遺地介紹自己在這次比賽運用以及學到的 NLP 概念，希望能幫助更多對人工智慧、 深度學習 或是 NLP 有興趣但卻不知如何開始的你，在閱讀本故事之後能得到一些啟發與方向，並展開自己的 NLP 之旅。 雖然不是必備，但有點程式經驗會讓你比較好理解本文的內容，因為在文中有不少 Python 程式碼；另外，如果你熟悉 深度學習（Deep Learning） 以及 神經網路（Neural Network） ，那你可以趁機複習一些以前學過的東西。 依據維基百科，NLP 的定義為： 自然語言處理（NLP）是計算機科學以及人工智慧的子領域，專注在如何讓計算機處理並分析大量（人類的）自然語言數據。NLP 常見的挑戰有語音辨識、自然語言理解、機器翻譯以及自然語言的生成。 在這篇文章裡頭，我將描述如何利用最近學到的 NLP 知識以及深度學習框架 Keras 來教會神經網路如何辨別眼前的假新聞。 儘管此文的 NLP 任務是假新聞分類，你將可以把從此文學到的基礎知識運用到如機器翻譯、教機器寫詩、語音辨識等大部分的 NLP 任務。我也會在文末附上 推薦的學習資源以及文章 供你進一步探索。 如果你已經準備好展開一趟刺激的 NLP 冒險之旅的話，就繼續往下閱讀吧！ 本文章節 30 秒重要訊息 意料之外的 Kaggle 競賽 假新聞分類任務 用直覺找出第一條底線 資料前處理：讓機器能夠處理文字 有記憶的循環神經網路 記憶力好的 LSTM 細胞 詞向量：將詞彙表達成有意義的向量 一個神經網路，兩個新聞標題 深度學習 3 步驟 進行預測並提交結果 我們是怎麼走到這裡的 3 門推薦的線上課程 結語：從掌握基礎到運用巨人之力 本文編排已將手機讀者放在第一位，但我仍然建議你使用較大的螢幕閱讀。 使用畫面左側的章節傳送門能讓你更輕鬆地在各章節之間跳轉（目前手機不支援，抱歉） 30 秒重要訊息 沒錯，光看上面的章節數，你應該了解無法在 10 分鐘內 KO 這篇文章，但我相信這篇文章會是你學習 NLP 基礎的最短捷徑之一。 針對那些時間寶貴的你，我在這邊直接列出本文想傳達的 3 個重要訊息： 深度學習發展神速，令人不知從何開始學習。但你總是要 從某個地方開始好好地學習基礎 NLP 接下來的發展只會更加快速，就連一般人也能弄出厲害的語言處理模型 站在巨人的肩膀之上，活用前人成果與經驗能讓你前進地更快，更有效率 這些陳述看似陳腔濫調，但希望好奇心能讓你實際閱讀本文，找出構成這些結論的蛛絲馬跡。 讓我們開始吧！ 意料之外的 Kaggle 競賽 Kaggle 是一個資料科學家以及機器學習愛好者互相切磋的數據建模和數據分析競賽平台。 本文提到的 Kaggle 競賽是 WSDM - Fake News Classification 。 此競賽的目的在於想辦法自動找出假新聞以節省人工檢查的成本。資料集則是由中國的手機新聞應用： 今日頭條 的母公司 字節跳動 所提出的。（知名的抖音也是由該公司的產品） 本文的 Kaggle 競賽 （ 圖片來源 ） 而因為我所任職的 SmartNews 主打產品也是手機新聞應用（主要針對日本與美國用戶），像是這種哪個企業又辦了 Kaggle 競賽、又開發什麼新功能等等的消息都會在公司內部流動。 話雖如此，在我從同事得知這個為期一個月的競賽時，事實上離截止時間只剩一個禮拜了！（傻眼） 今年 10 月底參加的 EMNLP （ 圖片來源 ） 但心念一轉，想說從 EMNLP 會議 回來後也學了一些不少 NLP 知識，不仿就趁著這個機會，試著在一週內兜出個模型來解決這個問題。 名符其實的「志在參加」。 假新聞分類任務 既然決定要參加了，當然得看看資料集長的什麼樣子。訓練資料集（Training Set）約有 32 萬筆數據、測試資料集（Test Set）則約為 8 萬筆。而訓練資料集一部份的內容如下所示： 要了解此資料集，讓我們先專注在第一列（Row），大蒜與地溝油新聞的每一個欄位。 （部分讀者可能會對簡體中文表示意見，但請體諒我沒有辦法事先將此大量數據全部轉為繁體） 第一欄位 title1_zh 代表的是「已知假新聞」 A 的中文標題： 用大蒜鉴别地沟油的方法,怎么鉴别地沟油 而第二欄位 title2_zh 則是一筆新的新聞 B 的中文標題，我們還不知道它的真偽： 翻炒大蒜可鉴别地沟油 要判斷第二欄中的新聞標題是否為真，我們可以把它跟已知的第一篇假新聞做比較，分為 3 個類別： unrelated ：B 跟 A 沒有關係 agreed ：B 同意 A 的敘述 disagreed ：B 不同意 A 的敘述 如果新聞 B 同意假新聞 A 的敘述的話，我們可以將 B 也視為一個假新聞；而如果 B 不同意假新聞 A 的敘述的話，我們可以放心地將 B 新聞釋出給一般大眾查看；如果 B 與 A 無關的話，可以考慮再進一步處理 B。（這處理不在本文討論範疇內） 如果 B 新聞「同意」假新聞 A 的話，我們大可將 B 新聞也視為假新聞，最後將其屏除 接著看到資料集（下圖）第一列最右邊的 label 欄位為 agreed ，代表 B 同意 A 的敘述，則我們可以判定 B 也是假新聞。 這就是一個簡單的「假新聞分類問題」：給定一個成對的新聞標題 A & B，在已知 A 為假新聞的情況下，預測 B 跟 A 之間的關係。其關係可以分為 3 個類別： unrelated agreed disagreed 順帶一提，上圖同時包含了 3 個類別的例子供你了解不同分類的實際情況。 第 3、 4 欄位則為新聞標題的英文翻譯。而因為該翻譯為機器翻譯，不一定能 100% 正確反映本來中文新聞想表達的意思，因此接下來的文章會忽視這兩個欄位，只使用簡體中文的新聞標題來訓練 NLP 模型。 用直覺找出第一條底線 現在任務目標很明確了，我們就是要將有 32 萬筆數據的訓練資料集（Training Set）交給我們的 NLP 模型，讓它「閱讀」每一列裡頭的假新聞 A 與新聞 B 的標題並瞭解它們之間的關係（不相關、B 同意 A、B 不同意 A）。 理想上，在看過一大堆案例以後，我們的模型就能夠「學會」一些法則，讓它在被給定一組從來沒看過的假新聞標題 A 以及新聞標題 B 的情況下，也能正確判斷新聞 A 與新聞 B 的關係。 而所謂的「模型從來沒看過的數據」，指的當然就是 8 萬筆的測試資料集（Test Set）了。 我們利用訓練資料集教模型學習，並用測試資料集挑戰模型 （ 圖片來源 ） 這樣的陳述是一個非常典型的 機器學習（Machine Learning, ML） 問題。我們當然希望不管使用什麼樣的模型，該模型都能夠幫我們減少人工檢查的成本，並同時最大化分類的準確度。 但在開始使用任何 ML 方法之前，為了衡量我們的自動化模型能提供多少潛在價值，讓我們先找出一個簡單方法作為底線（Baseline）。 這張圖顯示了訓練資料集（Training Set）裏頭各個分類所佔的比例。是一個常見的 Unbalanced Dataset：特定的分類佔了數據的大半比例。 我們可以看到接近 70 % 的「成對新聞」都是不相關的。這邊的「成對新聞」指的是資料集裡，每一行的假新聞標題 A 以及對應的標題 B 所組成的 pairs。 現在假設測試資料集（Test Set）的數據分佈跟訓練資料集相差不遠，且衡量一個分類模型的指標是準確度（Accuracy）：100 組成對新聞中，模型猜對幾組。 這時候如果要你用一個簡單法則來分類所有成對新聞，並同時最大化準確度，你會怎麼做？ 對沒錯，就是全部猜 unrelated 就對了！ 事實上，此競賽在 Evaluation 階段使用 Weighted Categorization Accuracy ，來稍微調降猜對 unrelated 的分數。畢竟（1）能正確判斷出兩個新聞是 unrelated 跟（2）能判斷出新聞 B disagreed 假新聞 A 的價值是不一樣的。（後者的價值比較高，因為比較稀有） 但使用 多數票決（Majority Votes） 的簡單方法還是能得到 0.666 的成績（滿分為 1）： 不過當你前往該 Kaggle 排行榜 的時候，卻會發現不少人低於這個標準： 第一次參加 Kaggle 的人可能會覺得這現象很奇怪。 但這是由於 Kaggle 競賽 1 天只能提交 2 次結果，因此通常不會有人浪費提交次數來上傳「多數票決」的結果（儘管分數會上升，大家還是會想把僅僅 2 次的上傳機會用來測試自己的 ML 模型的準確度）；另外也是因為不少人是上傳 1、2 次就放棄比賽了。 但如果你的 ML 或深度學習模型怎樣都無法超過一個簡單法則的 baseline 的話，或許最後上傳該 baseline 的結果也不失為提升排名的最後手段（笑） 找出 Baseline，可以讓我們判斷手上訓練出來的機器學習模型有多少潛在價值、值不值得再繼續花費自己的研究時間與電腦計算能力。 現在我們知道，要保證做出來的模型有點價值，最少要超過 baseline 才可以。以本文來說，就是多數票決法則得到的 0.666 準確度。 （ baseline 的定義依照研究目的以及比較方法而有所不同） 資料前處理：讓機器能夠處理文字 要讓電腦或是任何 NLP 模型理解一篇新聞標題在說什麼，我們不能將自己已經非常習慣的語言文字直接扔給電腦，而是要轉換成它熟悉的形式：數字。 因此這章節就是介紹一系列的數據轉換步驟，來將人類熟悉的語言如： 用大蒜鉴别地沟油的方法,怎么鉴别地沟油 轉換成人腦不易理解，但很「機器友善」的數字序列（Sequence of Numbers）： [217, 1268, 32, 1178, 25, 489, 116] 如果你對此步驟已經非常熟悉，可以假設我們已經對數據做完必要的處理，直接跳到下一章的 有記憶的循環神經網路 。 這章節的數據轉換步驟包含： 文本分詞（Text Segmentation） 建立字典並將文本轉成數字序列 序列的 Zero Padding 將正解做 One-hot Encoding 如果你現在不知道上述所有詞彙的意思，別擔心！ 你接下來會看到文字數據在丟入機器學習 / 深度學習模型之前，通常需要經過什麼轉換步驟。搭配說明，我相信你可以輕易地理解以下每個步驟的邏輯。 在這之前，先讓我們用 Pandas 將訓練資料集讀取進來： import pandas as pd train = pd . read_csv ( TRAIN_CSV_PATH , index_col = 0 ) train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tid1 tid2 title1_zh title2_zh title1_en title2_en label id 0 0 1 2017养老保险又新增两项，农村老人人人可申领，你领到了吗 警方辟谣\"鸟巢大会每人领5万\" 仍有老人坚持进京 There are two new old-age insurance benefits f... Police disprove \"bird's nest congress each per... unrelated 3 2 3 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小 \"If you do not come to Shenzhen, sooner or lat... Shenzhen's GDP outstrips Hong Kong? Shenzhen S... unrelated 1 2 4 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 GDP首超香港？深圳澄清：还差一点点…… \"If you do not come to Shenzhen, sooner or lat... The GDP overtopped Hong Kong? Shenzhen clarifi... unrelated 跟我們在 Kaggle 預覽的數據一致。不過為了畫面簡潔，讓我們只選取 2 個中文新聞標題以及分類結果（Label）的欄位： cols = [ 'title1_zh' , 'title2_zh' , 'label' ] train = train . loc [:, cols ] train . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title1_zh title2_zh label id 0 2017养老保险又新增两项，农村老人人人可申领，你领到了吗 警方辟谣\"鸟巢大会每人领5万\" 仍有老人坚持进京 unrelated 3 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小 unrelated 1 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 GDP首超香港？深圳澄清：还差一点点…… unrelated 有了必要的欄位以後，我們可以開始進行數據的前處理了。 文本分詞 文本分詞（Text Segmentation） 是一個將一連串文字切割成多個有意義的單位的步驟。這單位可以是 一個中文漢字 / 英文字母（Character） 一個中文詞彙 / 英文單字（Word） 一個中文句子 / 英文句子（Sentence） 依照不同的 NLP 任務會有不同切割需求，但很常見的切法是以單字（Word）為單位，也就是 Word Segmentation。 以英文來說，Word Segmentation 十分容易。通常只要依照空白分割，就能得到一個有意義的詞彙列表了（在這邊讓我們先無視標點符號）： text = 'I am Meng Lee, a data scientist based in Tokyo.' words = text . split ( ' ' ) words ['I', 'am', 'Meng', 'Lee,', 'a', 'data', 'scientist', 'based', 'in', 'Tokyo.'] 但很明顯地，中文無法這樣做。這時候我們將藉助 Jieba 這個中文斷詞工具，來為一連串的文字做有意義的切割： import jieba.posseg as pseg text = '我是李孟，在東京工作的數據科學家' words = pseg . cut ( text ) [ word for word in words ] [pair('我', 'r'), pair('是', 'v'), pair('李孟', 'nr'), pair('，', 'x'), pair('在', 'p'), pair('東京', 'ns'), pair('工作', 'vn'), pair('的', 'uj'), pair('數據', 'n'), pair('科學家', 'n')] 如上所示，Jieba 將我們的中文文本切成有意義的詞彙列表，並為每個詞彙附上對應的詞性（Flag）。 假設我們不需要標點符號，則只要將 flag == x 的詞彙去除即可。 我們可以寫一個很簡單的 Jieba 斷詞函式，此函式能將輸入的文本 text 斷詞，並回傳除了標點符號以外的詞彙列表： def jieba_tokenizer ( text ): words = pseg . cut ( text ) return ' ' . join ([ word for word , flag in words if flag != 'x' ]) 我們可以利用 Pandas 的 apply 函式，將 jieba_tokenizer 套用到所有新聞標題 A 以及 B 之上，做文本分詞： train [ 'title1_tokenized' ] = \\ train . loc [:, 'title1_zh' ] \\ . apply ( jieba_tokenizer ) train [ 'title2_tokenized' ] = \\ train . loc [:, 'title2_zh' ] \\ . apply ( jieba_tokenizer ) 新聞標題 A 的斷詞結果如下： train . iloc [:, [ 0 , 3 ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title1_zh title1_tokenized id 0 2017养老保险又新增两项，农村老人人人可申领，你领到了吗 2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗 3 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 1 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 2 \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 9 \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油 用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油 新聞標題 B 的結果則為： train . iloc [:, [ 1 , 4 ]] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title2_zh title2_tokenized id 0 警方辟谣\"鸟巢大会每人领5万\" 仍有老人坚持进京 警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京 3 深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小 深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小 1 GDP首超香港？深圳澄清：还差一点点…… GDP 首 超 香港 深圳 澄清 还 差 一点点 2 去年深圳GDP首超香港？深圳统计局辟谣：还差611亿 去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿 9 吃了30年食用油才知道，一片大蒜轻松鉴别地沟油 吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油 太棒了，將新聞標題切割成一個個有意義的詞彙以後，我們就能進入下一個步驟了！ 另外值得一提的是，不管最後是使用哪種切法，切完之後的每個文字片段在 NLP 領域裡頭習慣上會被稱之為 Token。（如上例中的警方、GDP） 建立字典並將文本轉成數字序列 當我們將完整的新聞標題切成一個個有意義的詞彙（Token）以後，下一步就是將這些詞彙轉換成一個數字序列，方便電腦處理。 這些數字是所謂的索引（Index），分別對應到特定的詞彙。 為了方便你理解這小節的概念，想像個極端的例子。假設我們現在就只有一個新聞標題：「狐狸被陌生人拍照」。 這時候要怎麼將這個句子轉成一個數字的序列呢？跟上一小節相同，我們首先會對此標題做斷詞，將句子分成多個有意義的詞彙： text = '狐狸被陌生人拍照' words = pseg . cut ( text ) words = [ w for w , f in words ] words ['狐狸', '被', '陌生人', '拍照'] 有了詞彙的列表以後，我們可以建立一個字典 word_index 。 該 dict 裏頭將上面的 4 個詞彙當作鍵值（Key），每個鍵值對應的值（Value）則為不重複的數字： word_index = { word : idx for idx , word in enumerate ( words ) } word_index {'狐狸': 0, '被': 1, '陌生人': 2, '拍照': 3} 有了這個字典以後，我們就能把該句子轉成一個數字序列： print ( words ) print ([ word_index [ w ] for w in words ]) ['狐狸', '被', '陌生人', '拍照'] [0, 1, 2, 3] 簡單明瞭，不是嗎？ 如果來了一個新的句子：「陌生人被狐狸拍照」，我們也能利用手上已有的字典 word_index 如法炮製： text = '陌生人被狐狸拍照' words = pseg . cut ( text ) words = [ w for w , f in words ] print ( words ) print ([ word_index [ w ] for w in words ]) ['陌生人', '被', '狐狸', '拍照'] [2, 1, 0, 3] 在這個簡單的狐狸例子裡頭， word_index 就是我們的字典；我們利用該字典，將 1 句話轉成包含多個數字的序列，而每個數字實際上代表著一個 Token。 同理，我們可以分 4 個步驟將手上的新聞標題全部轉為數字序列： 將已被斷詞的新聞標題 A 以及新聞標題 B 全部倒在一起 建立一個空字典 查看所有新聞標題，裏頭每出現一個字典裡頭沒有的詞彙，就為該詞彙指定一個字典裡頭還沒出現的索引數字，並將該詞彙放入字典 利用建好的字典，將每個新聞標題裡頭包含的詞彙轉換成數字 這種文字前處理步驟因為出現頻率實在太過頻繁，Keras 有專門的文字前處理模組來提升我們的效率： import keras MAX_NUM_WORDS = 10000 tokenizer = keras \\ . preprocessing \\ . text \\ . Tokenizer ( num_words = MAX_NUM_WORDS ) Tokenizer 顧名思義，即是將一段文字轉換成一系列的詞彙（Tokens），並為其建立字典。這邊的 num_words=10000 代表我們限制字典只能包含 10,000 個詞彙，一旦字典達到這個大小以後，剩餘的新詞彙都會被視為 Unknown，以避免字典過於龐大。 如同上述的步驟 1，我們得將新聞 A 及新聞 B 的標題全部聚集起來，為它們建立字典： corpus_x1 = train . title1_tokenized corpus_x2 = train . title2_tokenized corpus = pd . concat ([ corpus_x1 , corpus_x2 ]) corpus . shape (641086,) 因為訓練集有大約 32 萬列（Row）的成對新聞（每一列包含 2 筆新聞：A & B），因此將所有新聞放在一起的話，就有 2 倍的大小。而這些文本的集合在習慣上被稱作語料庫（Text Corpus），代表著我們有的所有文本數據。 以下是我們語料庫的一小部分： pd . DataFrame ( corpus . iloc [: 5 ], columns = [ 'title' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } title id 0 2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗 3 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 1 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 2 你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港 9 用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油 有了語料庫以後，接下來就是呼叫 tokenizer 為我們查看所有文本，並建立一個字典（步驟 2 & 3）： tokenizer . fit_on_texts ( corpus ) 以我們的語料庫大小來說，這大約需時 10 秒鐘。而等到 tokenizer 建好字典以後，我們可以進行上述第 4 個步驟，請 tokenizer 利用內部生成的字典分別將我們的新聞標題 A 與 新聞 B 轉換成數字序列： x1_train = tokenizer \\ . texts_to_sequences ( corpus_x1 ) x2_train = tokenizer \\ . texts_to_sequences ( corpus_x2 ) 讓我們看看結果： len ( x1_train ) 320543 x1_train [: 1 ] [[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]] x1_train 為一個 Python list ，裡頭包含了每一筆假新聞標題 A 對應的數字序列。 讓我們利用 tokenizer.index_word 來將索引數字對應回本來的詞彙： for seq in x1_train [: 1 ]: print ([ tokenizer . index_word [ idx ] for idx in seq ]) ['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗'] 輕鬆寫意，不是嗎？ 到此為止，我們已經將所有新聞標題轉換成電腦容易處理的數字序列，進入下個步驟！ 序列的 Zero Padding 雖然我們已經將每個新聞標題的文本轉為一行行的數字序列，你會發現每篇標題的序列長度並不相同： for seq in x1_train [: 10 ]: print ( len ( seq ), seq [: 5 ], ' ...' ) 14 [217, 1268, 32, 1178, 5967] ... 19 [4, 10, 47, 678, 2558] ... 19 [4, 10, 47, 678, 2558] ... 19 [4, 10, 47, 678, 2558] ... 9 [31, 320, 3372, 3062, 1] ... 19 [4, 10, 47, 678, 2558] ... 6 [7, 2221, 1, 2072, 7] ... 19 [4, 10, 47, 678, 2558] ... 14 [1281, 1211, 427, 3, 3244] ... 9 [31, 320, 3372, 3062, 1] ... 最長的序列甚至達到 61 個詞彙： max_seq_len = max ([ len ( seq ) for seq in x1_train ]) max_seq_len 61 而為了方便之後的 NLP 模型處理（見 循環神經網路 一章），一般會設定一個 MAX_SEQUENCE_LENGTH 來讓所有序列的長度一致。 長度超過此數字的序列尾巴會被刪掉；而針對原來長度不足的序列，我們則會在詞彙前面補零。Keras 一樣有個方便函式 pad_sequences 來幫助我們完成這件工作： MAX_SEQUENCE_LENGTH = 20 x1_train = keras \\ . preprocessing \\ . sequence \\ . pad_sequences ( x1_train , maxlen = MAX_SEQUENCE_LENGTH ) x2_train = keras \\ . preprocessing \\ . sequence \\ . pad_sequences ( x2_train , maxlen = MAX_SEQUENCE_LENGTH ) 一般來說 MAX_SEQUENCE_LENGTH 可以設定成最長序列的長度（此例中的 61）。但這邊為了讓模型可以只看前 20 個詞彙就做出判斷以節省訓練時間，我們先暫時使用 20 這個數字。 讓我們看看經過 Zero Padding 的第一篇假新聞標題 A 變成什麼樣子： x1_train [ 0 ] array([ 0, 0, 0, 0, 0, 0, 217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13], dtype=int32) 你可以清楚看到，因為該新聞標題原本的序列長度並沒有達到剛剛設定的 MAX_SEQUENCE_LENGTH ，因此在總長度為 20 的序列中，前面 6 個值被 Keras 補上 0 以說明該序列中的前 6 個詞彙並不存在。 我們還可以發現，所有的新聞標題都被轉成長度為 20 的數字序列了： for seq in x1_train + x2_train : assert len ( seq ) == 20 print ( \"所有新聞標題的序列長度皆為 20 !\" ) 所有新聞標題的序列長度皆為 20 ! 再看一次轉換後的新聞標題： x1_train [: 5 ] array([[ 0, 0, 0, 0, 0, 0, 217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13], [ 0, 4, 10, 47, 678, 2558, 4, 166, 34, 17, 47, 5150, 63, 15, 678, 4502, 3211, 23, 284, 1181], [ 0, 4, 10, 47, 678, 2558, 4, 166, 34, 17, 47, 5150, 63, 15, 678, 4502, 3211, 23, 284, 1181], [ 0, 4, 10, 47, 678, 2558, 4, 166, 34, 17, 47, 5150, 63, 15, 678, 4502, 3211, 23, 284, 1181], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 320, 3372, 3062, 1, 95, 98, 3372, 3062]], dtype=int32) 在這邊，可以看到前 5 個新聞標題都已經各自被轉換為長度為 20 的數字序列，而序列裡頭的每個數字則對應到字典裡頭一個特定的 Token，整整齊齊。 到此為止，我們已經將原本以自然語言呈現的新聞標題轉換成機器容易理解的數字序列了。很神奇，不是嗎？ 喔不過，別忘了還有 label 這個文字欄位等著我們的處理。 將正解做 One-hot Encoding 到目前為止，我們已經將所有的新聞標題以數字型態表示，只剩分類欄位 label 要進行從文本到數字的轉換了： train . label [: 5 ] id 0 unrelated 3 unrelated 1 unrelated 2 unrelated 9 agreed Name: label, dtype: object 不過 label 的處理相對簡單。跟新聞標題相同，我們一樣需要一個字典將分類的文字轉換成索引： import numpy as np # 定義每一個分類對應到的索引數字 label_to_index = { 'unrelated' : 0 , 'agreed' : 1 , 'disagreed' : 2 } # 將分類標籤對應到剛定義的數字 y_train = train . label . apply ( lambda x : label_to_index [ x ]) y_train = np . asarray ( y_train ) \\ . astype ( 'float32' ) y_train [: 5 ] array([0., 0., 0., 0., 1.], dtype=float32) 現在每個分類的文字標籤都已經被轉成對應的數字，接著讓我們利用 Keras 做 One Hot Encoding ： y_train = keras \\ . utils \\ . to_categorical ( y_train ) y_train [: 5 ] array([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 1., 0.]], dtype=float32) 上述矩陣的每一列即為 1 個 label，而你可以看到現在每個 label 都從 1 個數字變成一個 3 維的向量（Vector）。 每 1 維度則對應到 1 個分類： [1, 0, 0] 代表 label 為 unrelated [0, 1, 0] 代表 label 為 agreed [0, 0, 1] 代表 label 為 disagreed 用這樣的方式表達 label 的好處是我們可以把分類結果想成機率分佈。 [1, 0, 0] 就代表一組新聞標題 A、B 為 unrelated 的機率等於 100 %。 One Hot Encoding 示意圖 在 決定如何衡量模型的表現 一節我們會看到，給定一組新聞標題 A、B，我們的模型會預測此成對標題屬於每個分類的機率值，比方說 [0.7, 0.2, 0.1] 。而此預測結果代表模型認為這 2 個新聞標題的關係有 70 % 的機率為 unrelated 、20 % 的機率是 agreed 而 10 % 為 disagreed 。 因此，如果正解也事先用同樣的方式表達的話，會讓我們比較好計算以下兩者之間的差距： 正確的分類的機率分佈（ [1, 0, 0] ） 模型預測出的機率分佈（ [0.7, 0.2, 0.1] ） 在知道預測結果跟正確解答之間差距多少之後，深度學習模型就會自動修正學習方向，想盡辦法拉近這個差距。 好，到此為止所有的數據都已經被我們轉換成方便機器使用的格式了。最後，讓我們將整個資料集拆成 訓練資料集 & 驗證資料集 以方便之後測試模型的效能。 （別哀號，我保證這是最後的前處理步驟了！） 切割訓練資料集 & 驗證資料集 這部分很簡單，我們只需決定要將整個訓練資料集（Training Set）的多少比例切出來當作驗證資料集（Validation Set）。此例中我們用 10 %。 但為何要再把本來的訓練資料集切成 2 個部分呢？ 一般來說，我們在訓練時只會讓模型看到訓練資料集，並用模型沒看過的驗證資料集來測試該模型在真實世界的表現。（畢竟我們沒有測試資料集的答案） 我們會反覆在 Train / Valid Set 上訓練並測試模型，最後用 Test Set 一決生死 （ 圖片來源 ） 等到模型在驗證資料集也表現得夠好後，便在最終的測試資料集（Test Set）進行最後一次的預測並將該結果上傳到 Kaggle。 要了解為何我們需要驗證資料集可以查看 這邊的討論 。 簡而言之，當你多次利用驗證資料集的預測結果以修正模型，並讓它在該資料集表現更好時， 過適（Overfitting） 的風險就已經產生了。 反覆利用驗證資料集的結果來修正模型表現，事實上就等於讓模型「偷看」到驗證資料集本身的資訊了 儘管你沒有直接讓模型看到驗證資料集（Validation Set）內的任何數據，你還是間接地洩漏了該資料集的重要資訊：你讓模型知道怎樣的參數設定會讓它在該資料集表現比較好，亦或表現較差。 因此有一個完全跟模型訓練過程獨立的測試資料集（Test Set）就顯得重要許多了。（這也是為何我到現在都還沒有碰它的原因） 機器學習模型努力從夏令營（訓練及驗證資料集）學習技能，並在真實世界（測試資料集）展示其學習結果。 回歸正題，要切訓練資料集 / 驗證資料集， scikit-learn 中的 train_test_split 函式是一個不錯的選擇： from sklearn.model_selection \\ import train_test_split VALIDATION_RATIO = 0.1 # 小彩蛋 RANDOM_STATE = 9527 x1_train , x1_val , \\ x2_train , x2_val , \\ y_train , y_val = \\ train_test_split ( x1_train , x2_train , y_train , test_size = VALIDATION_RATIO , random_state = RANDOM_STATE ) 在這邊，我們分別將新聞標題 A x1_train 、新聞標題 B x2_train 以及分類標籤 y_train 都分成兩個部分：訓練部分 & 驗證部分。 以假新聞 A 的標題 x1_train 為例，本來完整 32 萬筆的 x1_train 會被分為包含 90 % 數據的訓練資料集 x1_train 以及 10 % 的驗證資料集 x1_val 。 print ( \"Training Set\" ) print ( \"-\" * 10 ) print ( f \"x1_train: {x1_train.shape} \" ) print ( f \"x2_train: {x2_train.shape} \" ) print ( f \"y_train : {y_train.shape} \" ) print ( \"-\" * 10 ) print ( f \"x1_val: {x1_val.shape} \" ) print ( f \"x2_val: {x2_val.shape} \" ) print ( f \"y_val : {y_val.shape} \" ) print ( \"-\" * 10 ) print ( \"Test Set\" ) Training Set ---------- x1_train: (288488, 20) x2_train: (288488, 20) y_train : (288488, 3) ---------- x1_val: (32055, 20) x2_val: (32055, 20) y_val : (32055, 3) ---------- Test Set 我們可以看到，切割後的訓練資料集有 288,488 筆資料。每一筆資料裡頭，成對新聞標題 A & B 的長度皆為 20 個 Tokens，分類結果則有 3 個；驗證資料集的內容一模一樣，僅差在資料筆數較少（32,055 筆）。 到此為此，資料前處理大功告成！ 既然我們已經為機器準備好它們容易理解的數字序列資料，接著就讓我們來看看要使用怎麼樣的 NLP 模型來處理這些數據。 有記憶的循環神經網路 針對這次的 Kaggle 競賽，我們將使用 循環神經網路（Recurrent Neural Network, 後簡稱 RNN） 來處理剛剛得到的序列數據。 RNN 是一種有「記憶力」的神經網路，其最為人所知的形式如下： 如同上圖等號左側所示，RNN 跟一般深度學習中常見的 前饋神經網路（Feedforward Neural Network, 後簡稱 FFNN） 最不一樣的地方在於它有一個迴圈（Loop）。 要了解這個迴圈在 RNN 裏頭怎麼運作，現在讓我們想像有一個輸入序列 X（Input Sequence）其長相如下： X = [ x0, x1, x2, ... xt ] 不同於 FFNN，RNN 在第一個時間點 t0 並不會直接把整個序列 X 讀入。反之，在第一個時間點 t0 ，它只將該序列中的第一個元素 x0 讀入中間的細胞 A。細胞 A 則會針對 x0 做些處理以後，更新自己的「狀態」並輸出第一個結果 h0 。 在下個時間點 t1 ，RNN 如法炮製，讀入序列 X 中的下一個元素 x1 ，並利用剛剛處理完 x0 得到的細胞狀態，處理 x1 並更新自己的狀態（也被稱為記憶），接著輸出另個結果 h1 。 剩下的 xt 都會被以同樣的方式處理。但不管輸入的序列 X 有多長，RNN 的本體從頭到尾都是等號左邊的樣子：迴圈代表細胞 A 利用「上」一個時間點（比方說 t1 ）儲存的狀態，來處理當下的輸入（比方說 x2 ）。 但如果你將不同時間點（ t0 、 t1 ...）的 RNN 以及它的輸入一起截圖，並把所有截圖從左到右一字排開的話，就會長得像等號右邊的形式。 將 RNN 以右邊的形式表示的話，你可以很清楚地了解，當輸入序列越長，向右展開的 RNN 也就越長。（模型也就需要訓練更久時間，這也是為何我們在資料前處理時 設定了序列的最長長度 ） 為了確保你 100 % 理解 RNN，讓我們假設剛剛的序列 X 實際上是一個內容如下的英文問句： X = [ What, time, is, it, ? ] 而且 RNN 已經處理完前兩個元素 What 和 time 了。 則接下來 RNN 會這樣處理剩下的句子： RNN 一次只讀入並處理序列的「一個」元素 （ 圖片來源 ） 現在你可以想像為何 RNN 非常適合拿來處理像是自然語言這種序列數據了。 就像你現在閱讀這段話一樣，你是由左到右逐字在大腦裡處理我現在寫的文字，同時不斷地更新你腦中的記憶狀態。 每當下個詞彙映入眼中，你腦中的處理都會跟以下兩者相關： 前面所有已讀的詞彙 目前腦中的記憶狀態 當然，實際人腦的閱讀機制更為複雜，但 RNN 抓到這個處理精髓，利用內在迴圈以及細胞內的「記憶狀態」來處理序列資料。 RNN 按照順序，處理一連串詞彙的機制跟我們理解自然語言的方式有許多相似之處 到此為止，你應該已經了解 RNN 的基本運作方式了。現在你可能會問：「那我們該如何實作一個 RNN 呢？」 好問題，以下是一個簡化到不行的 RNN 實現： state_t = 0 for input_t in input_sequence : output_t = f ( input_t , state_t ) state_t = output_t 在 RNN 每次讀入任何新的序列數據前，細胞 A 中的記憶狀態 state_t 都會被初始化為 0。 接著在每個時間點 t ，RNN 會重複以下步驟： 讀入 input_sequence 序列中的一個新元素 input_t 利用 f 函式將當前細胞的狀態 state_t 以及輸入 input_t 做些處理產生 output_t 輸出 output_t 並同時更新自己的狀態 state_t 不需要自己發明輪子，在 Keras 裏頭只要 2 行就可以建立一個 RNN layer： from keras import layers rnn = layers . SimpleRNN () 使用深度學習框架可以幫我們省下非常多的寶貴時間並避免可能的程式錯誤。 我們後面還會看到，一個完整的神經網路通常會分成好幾層（layer）：每一層取得前一層的結果作為輸入，進行特定的資料轉換後再輸出給下一層。 常見的神經網路形式。圖中框內有迴圈的就是 RNN 層 （ 圖片來源 ） 好啦，相信你現在已經掌握基本 RNN 了。事實上，除了 SimpleRNN 以外，Keras 裡頭還有其他更常被使用的 Layer，現在就讓我們看看一個知名的例子：長短期記憶。 記憶力好的 LSTM 細胞 讓我們再看一次前面的簡易 RNN 實作： state_t = 0 # 細胞 A 會重複執行以下處理 for input_t in input_sequence : output_t = f ( input_t , state_t ) state_t = output_t 在了解 RNN 的基本運作方式以後，你會發現 RNN 真正的魔法，事實上藏在細胞 A 的 f 函式裏頭。 要如何將細胞 A 當下的記憶 state_t 與輸入 input_t 結合，才能產生最有意義的輸出 output_t 呢？ 在 SimpleRNN 的細胞 A 裡頭，這個 f 的實作很簡單。而這導致其記憶狀態 state_t 沒辦法很好地「記住」前面處理過的序列元素，造成 RNN 在處理後來的元素時，就已經把前面重要的資訊給忘記了。 這就好像一個人在講了好長一段話以後，忘了前面到底講過些什麼的情境。 長短期記憶（Long Short-Term Memory, 後簡稱 LSTM） 就是被設計來解決 RNN 的這個問題。 如下圖所示，你可以把 LSTM 想成是 RNN 中用來實現細胞 A 內部處理邏輯的一個特定方法： 以抽象的層次來看，LSTM 就是實現 RNN 中細胞 A 邏輯的一個方式 （ 圖片來源 ） 基本上一個 LSTM 細胞裡頭會有 3 個閘門（Gates）來控制細胞在不同時間點的記憶狀態： Forget Gate：決定細胞是否要遺忘目前的記憶狀態 Input Gate：決定目前輸入有沒有重要到值得處理 Output Gate：決定更新後的記憶狀態有多少要輸出 透過這些閘門控管機制，LSTM 可以將很久以前的記憶狀態儲存下來，在需要的時候再次拿出來使用。值得一提的是，這些閘門的參數也都是神經網路自己訓練出來的。 下圖顯示各個閘門所在的位置： LSTM 細胞頂端那條 cell state 正代表著細胞記憶的轉換過程 （ 圖片來源 ） 想像 LSTM 細胞裡頭的記憶狀態是一個包裹，上面那條直線就代表著一個輸送帶。 LSTM 可以把任意時間點的記憶狀態（包裹）放上該輸送帶，然後在未來的某個時間點將其原封不動地取下來使用。 因為這樣的機制，讓 LSTM 即使面對很長的序列數據也能有效處理，不遺忘以前的記憶。 因為效果卓越，LSTM 非常廣泛地被使用。事實上，當有人跟你說他用 RNN 做了什麼 NLP 專案時，有 9 成機率他是使用 LSTM 或是 GRU（LSTM 的改良版，只使用 2 個閘門） 來實作，而不是使用最簡單的 SimpleRNN 。 因此，在這次 Kaggle 競賽中，我們的第一個模型也將使用 LSTM。而在 Keras 裡頭要使用 LSTM 也是輕鬆寫意： from keras import layers lstm = layers . LSTM () 現在，既然我們已經有了在 資料前處理步驟 被轉換完成的序列數據，也決定好要使用 LSTM 作為我們的 NLP 模型，接著就讓我們試著將這些數據讀入 LSTM 吧！ 詞向量：將詞彙表達成有意義的向量 在將序列數據塞入模型之前，讓我們重新檢視一下數據。比方說，以下是在訓練資料集裡頭前 5 筆的假新聞標題 A： for i , seq in enumerate ( x1_train [: 5 ]): print ( f \"新聞標題 {i + 1}: \" ) print ( seq ) print () 新聞標題 1: [ 0 0 0 185 300 72 4029 37 1 121 250 95 30 511 92 2358 33 2565 19 55] 新聞標題 2: [ 0 0 0 0 0 0 0 0 0 0 0 0 0 7149 54 130 8454 3404 6172 66] 新聞標題 3: [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 87 6339 59 5236 2848] 新聞標題 4: [ 0 0 0 0 0 0 0 59 18 1780 1 63 30 2526 1017 1466 25 11 139 50] 新聞標題 5: [ 0 0 0 0 0 25 9 24 1402 12 667 63 64 483 9523 303 1402 18 332 3258] 你可以看到，每個新聞標題都被轉成長度為 20 的數字序列。裡頭的每個數字都代表著一個詞彙（ 0 代表 Zero Padding ）。 x1_train . shape (288488, 20) 而我們在訓練資料集則總共有 288,488 筆新聞標題，每筆標題如同剛剛所說的，是一個包含 20 個數字的序列。 當然，我們可以用 tokenizer 裡頭的字典 index_word 還原文本看看： for i , seq in enumerate ( x1_train [: 5 ]): print ( f \"新聞標題 {i + 1}: \" ) print ([ tokenizer . index_word . get ( idx , '' ) for idx in seq ]) print () 新聞標題 1: ['', '', '', '皮肤', '白', '到', '逆', '天', '的', '范冰冰', '美白', '方法', '大', '揭秘', '做', '面膜', '个', '小动作', '就', '可以'] 新聞標題 2: ['', '', '', '', '', '', '', '', '', '', '', '', '', '张家口', '一个', '男子', '持', '猛', '踹', '孩子'] 新聞標題 3: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '种', '杂草', '农民', '年收入', '几十万'] 新聞標題 4: ['', '', '', '', '', '', '', '农民', '能', '享受', '的', '10', '大', '政府', '特别', '补助', '农村', '人', '得', '知道'] 新聞標題 5: ['', '', '', '', '', '农村', '这', '3', '类人', '有', '近', '10', '万', '宅基地', '补偿款', '还有', '类人', '能', '免费', '住房'] 其他新聞標題像是： 訓練資料集中的新聞標題 B x2_train 驗證資料集中的新聞標題 A x1_val 驗證資料集中的新聞標題 B x2_val 也都是以這樣的數字序列形式被儲存。 但事實上要讓神經網路能夠處理標題序列內的詞彙，我們要將它們表示成向量（更精準地說，是 張量：Tensor ），而不是一個單純數字。 如果我們能做到這件事情，則 RNN 就能用以下的方式讀入我們的資料： 注意：在每個時間點被塞入 RNN 的「詞彙」不再是 1 個數字，而是一個 N 維向量（圖中 N 為 3） （ 圖片來源 ） 所以現在的問題變成： 「要怎麼將一個詞彙表示成一個 N 維向量 ？」 其中一個方法是我們隨便決定一個 N，然後為語料庫裡頭的每一個詞彙都指派一個隨機生成的 N 維向量。 假設我們現在有 5 個詞彙： 野狼 老虎 狗 貓 喵咪 依照剛剛說的方法，我們可以設定 N = 2，並為每個詞彙隨機分配一個 2 維向量後將它們畫在一個平面空間裡頭： 這些代表詞彙的向量被稱之為詞向量，但是你可以想像這樣的隨機轉換很沒意義。 比方說上圖，我們就無法理解： 為何「狗」是跟「老虎」而不是跟同為犬科的「野狼」比較接近？ 為何「貓」的維度 2 比「狗」高，但卻比「野狼」低？ 維度 2 的值的大小到底代表什麼意義？ 「喵咪」怎麼會在那裡？ 這是因為我們只是將詞彙隨機地轉換到 2 維空間，並沒有讓這些轉換的結果（向量）反應出詞彙本身的語意（Semantic）。 一個理想的轉換應該是像底下這樣： 在這個 2 維空間裡頭，我們可以發現一個好的轉換有 2 個特性： 距離有意義：「喵咪」與意思相近的詞彙「貓」距離接近，而與較不相關的「狗」距離較遠 維度有意義：看看（狗, 貓）與（野狼, 老虎）這兩對組合，可以發現我們能將維度 1 解釋為貓科 VS 犬科；維度 2 解釋為寵物與野生動物 如果我們能把語料庫（Corpus）裏頭的每個詞彙都表示成一個像是這樣有意義的詞向量，神經網路就能幫我們找到潛藏在大量詞彙中的語義關係，並進一步改善 NLP 任務的精準度。 好消息是，大部分的情況我們並不需要自己手動設定每個詞彙的詞向量。我們可以隨機初始化所有詞向量（如前述的隨機轉換），並利用平常訓練神經網路的 反向傳播算法（Backpropagation） ，讓神經網路自動學到一組適合當前 NLP 任務的詞向量（如上張圖的理想狀態）。 反向傳播讓神經網路可以在訓練過程中修正參數，持續減少預測錯誤的可能性 （ 圖片來源 ） 在 NLP 裏頭，這種將一個詞彙或句子轉換成一個實數詞向量（Vectors of real numbers）的技術被稱之為 詞嵌入（Word Embedding） 。 而在 Keras 裡頭，我們可以使用 Embedding 層來幫我們做到這件事情： from keras import layers embedding_layer = layers . Embedding ( MAX_NUM_WORDS , NUM_EMBEDDING_DIM ) MAX_NUM_WORDS 是我們的字典大小（10,000 個詞彙）、 NUM_EMBEDDING_DIM 則是詞向量的維度。常見的詞向量維度有 128、256 或甚至 1,024。 Embedding 層一次接收 k 個長度任意的數字序列，並輸出 k 個長度相同的序列。輸出的序列中，每個元素不再是數字，而是一個 NUM_EMBEDDING_DIM 維的詞向量。 假如我們將第一筆（也就是 k = 1）假新聞標題 A 丟入 Embedding 層，並設定 NUM_EMBEDDING_DIM 為 3 的話，原來的標題 A： 新聞標題: [ 0, 0, 0, 185, 300, 72, 4029, 37, 1, 121, 250, 95, 30, 511, 92, 2358, 33, 2565, 19, 55, ] 就會被轉換成類似以下的形式： 新聞標題: [ [0.212, 0.111, 0.666], [0.212, 0.111, 0.666], [0.212, 0.111, 0.666], [0.528, 0.344, 0.452], [0.163, 0.93, 0.58], [0.527, 0.262, 0.246], [0.077, 0.695, 0.776], [0.624, 0.962, 0.96], [0.456, 0.927, 0.404], [0.353, 0.119, 0.108], [0.805, 0.969, 0.725], [0.379, 0.265, 0.473], [0.436, 0.186, 0.738], [0.923, 0.287, 0.967], [0.477, 0.614, 0.838], [0.089, 0.328, 0.993], [0.887, 0.913, 0.885], [0.604, 0.118, 0.646], [0.907, 0.52, 0.437], [0.443, 0.432, 0.498], ] 序列裡頭的每個數字（即詞彙）都被轉換成一個 3 維的詞向量，而相同數字則當然都會對應到同一個詞向量（如前 3 個 0 所對應到的詞向量）。 Keras 的 Embedding Layer 讓我們可以輕鬆地將詞彙轉換成適合神經網路的詞向量 （ 圖片來源 ） 有了這樣的轉換，我們就能將轉換後的詞向量丟入 RNN / LSTM 裏頭，讓模型逐步修正隨機初始化的詞向量，使得詞向量裡頭的值越來越有意義。 有了兩個新聞標題的詞向量，接著讓我們瞧瞧能夠處理這些數據的神經網路架構吧！ 一個神經網路，兩個新聞標題 一般來說，多數你見過的神經網路只會接受一個資料來源： 輸入一張圖片，判斷是狗還是貓 輸入一個音訊，將其轉成文字 輸入一篇新聞，判斷是娛樂還是運動新聞 單一輸入的神經網路架構可以解決大部分的深度學習問題。但在這個 Kaggle 競賽裡頭，我們想要的是一個能夠讀入成對新聞標題，並判斷兩者之間關係的神經網路架構： 不相關（unrelated） 新聞 B 同意 A（agreed） 新聞 B 不同意 A（disagreed） 要怎麼做到這件事情呢？ 我們可以使用 孿生神經網路（Siamese Network） 架構： 使用孿生神經網路架構來處理同類型的 2 個新聞標題 這張圖是本文最重要的一張圖，但現在你只需關注紅框的部分即可。剩餘細節我會在後面的 定義神經網路的架構 小節詳述。 重複觀察幾次，我相信你就會知道何謂孿生神經網路架構：一部份的神經網路（紅框部分）被重複用來處理多個不同的資料來源（在本篇中為 2 篇不同的新聞標題）。 而會想這樣做，是因為不管標題內容是新聞 A 還是新聞 B，其標題本身的語法 & 語義結構大同小異。 神經網路說到底，就跟其他機器學習方法相同，都是對輸入進行一連串有意義的數據轉換步驟。神經網路將輸入的數據轉換成更適合解決當前任務的數據格式，並利用轉換後的數據進行預測。 以這樣的觀點來看的話，我們並不需要兩個不同的 LSTM 來分別將新聞 A 以及新聞 B 的詞向量做有意義的轉換，而是只需要讓標題 A 與標題 B 共享一個 LSTM 即可。畢竟，標題 A 跟標題 B 的數據結構很像。 如果我們只寫一個 Python 函式就能處理 2 個相同格式的輸入的話，為何要寫 2 個函式呢？ 孿生神經網路也是相同的概念。 孿生神經網路名稱概念來自 Siamese Twins，這是發生在美國 19 世紀的一對連體泰國人兄弟的故事。你可以想像孿生神經網路架構裡頭也有 2 個一模一樣的神經網路雙胞胎。（感謝網友 Hu Josh 糾正） （ 圖片來源 ） 好了，在了解如何同時讀入 2 個資料來源後，就讓我們實際用 Keras 動手將此模型建出來吧！ 深度學習 3 步驟 深度學習以及 NLP 領域的學問博大精深，但一般來說，當你想要實際動手寫出一個神經網路的時候，有 3 個基本步驟可以 follow： 用深度學習框架 Keras 來實作深度學習的基本 3 步驟 （ 圖片來源 ） 定義神經網路的架構 決定如何衡量模型的表現 訓練模型並挑選最好的結果 接下來你會看到，大約 80 % 的程式碼會花在實作第一個步驟。剩餘 2 個步驟在使用 Keras 的情況下非常容易就能實現；但後面我們也會談到，你將花 80 % 的時間在最後一個步驟上面。 首先，先讓我們進入第一步驟。 定義神經網路的架構 在實作之前，先讓我們回顧一下前面段落看到的模型架構： 本文用來實現假新聞分類的神經網路架構 從左到右掃過一遍，你可以很清楚地發現我們需要以下 5 個元素來完成這個模型： 兩個新聞標題（兩個長度為 20 的數字序列） 一個詞嵌入層：將數字序列轉換為詞向量序列 一個 LSTM 層：讀入前層的詞向量並萃取標題語義 一個串接層：將兩個新聞標題的處理結果（也是向量）串接成一個向量 一個全連接層：將前層的向量轉換為 3 個分類的預測機率 有些層我們已經在前面章節看過 Keras 的實現，比方說 詞嵌入層 以及 LSTM 層 。剩下的串接層以及全連結層在 Keras 也都有現成的模組可供使用。 另外值得一提的是，圖上的每個層（Layer）以及向量右下的灰字都對應了底下 Python 程式碼裡頭的變數名稱： 灰字代表程式碼裡頭對應的變數名稱 因此，如果等等你不了解底下某個特定的變數所代表的意義，可以回來利用這張架構圖來釐清概念。 以下就是此模型的 Keras 實作： # 基本參數設置，有幾個分類 NUM_CLASSES = 3 # 在語料庫裡有多少詞彙 MAX_NUM_WORDS = 10000 # 一個標題最長有幾個詞彙 MAX_SEQUENCE_LENGTH = 20 # 一個詞向量的維度 NUM_EMBEDDING_DIM = 256 # LSTM 輸出的向量維度 NUM_LSTM_UNITS = 128 # 建立孿生 LSTM 架構（Siamese LSTM） from keras import Input from keras.layers import Embedding , \\ LSTM , concatenate , Dense from keras.models import Model # 分別定義 2 個新聞標題 A & B 為模型輸入 # 兩個標題都是一個長度為 20 的數字序列 top_input = Input ( shape = ( MAX_SEQUENCE_LENGTH , ), dtype = 'int32' ) bm_input = Input ( shape = ( MAX_SEQUENCE_LENGTH , ), dtype = 'int32' ) # 詞嵌入層 # 經過詞嵌入層的轉換，兩個新聞標題都變成 # 一個詞向量的序列，而每個詞向量的維度 # 為 256 embedding_layer = Embedding ( MAX_NUM_WORDS , NUM_EMBEDDING_DIM ) top_embedded = embedding_layer ( top_input ) bm_embedded = embedding_layer ( bm_input ) # LSTM 層 # 兩個新聞標題經過此層後 # 為一個 128 維度向量 shared_lstm = LSTM ( NUM_LSTM_UNITS ) top_output = shared_lstm ( top_embedded ) bm_output = shared_lstm ( bm_embedded ) # 串接層將兩個新聞標題的結果串接單一向量 # 方便跟全連結層相連 merged = concatenate ( [ top_output , bm_output ], axis =- 1 ) # 全連接層搭配 Softmax Activation # 可以回傳 3 個成對標題 # 屬於各類別的可能機率 dense = Dense ( units = NUM_CLASSES , activation = 'softmax' ) predictions = dense ( merged ) # 我們的模型就是將數字序列的輸入，轉換 # 成 3 個分類的機率的所有步驟 / 層的總和 model = Model ( inputs = [ top_input , bm_input ], outputs = predictions ) 這段程式碼的確不短，但有將近一半是我寫給你的註解。而且這段程式碼的邏輯跟上面的架構圖一模一樣，只差架構圖是從左到右、程式碼是從上到下而已。 為了確保用 Keras 定義出的模型架構跟預期相同，我們也可以將其畫出來： from keras.utils import plot_model plot_model ( model , to_file = 'model.png' , show_shapes = True , show_layer_names = False , rankdir = 'LR' ) 除了模型架構以外，我們還可以看到所有層的輸入 / 輸出張量（Tensor）的維度。在 Keras 裏頭，張量的第 1 個維度通常為樣本數（比方說 5 則新聞標題），而 None 則代表可以指定任意值。 最重要的是，這個用 Keras 定義出來的模型，跟我們之前想像中的孿生神經網路可以說是一模一樣： 我沒有騙你，對吧？ 現在你應該發現，只要擁有前面幾章學到的 NLP 知識以及基礎 Python 程式能力，要建立一個像這樣看似複雜的孿生 LSTM（Siamese LSTM）神經網路其實也並沒有那麼困難。 事實上，使用 Keras 建立深度學習模型這件事情感覺上就像是在玩疊疊樂一樣，一層加上一層： 一位研究生利用 Keras 做深度學習的心得 （ 圖片來源 ） 全連接層 唯一沒有在前面章節提到的是 全連接層（Fully Connected Layer） 以及其使用的 Softmax 函式 。 全連接層顧名思義，代表該層的每個神經元（Neuron）都會跟前一層的所有神經元享有連結： 因為只需要預測 3 個分類，本文的全連接層只有 3 個神經元 而為了確認我們計算的參數量無誤，還可以使用 model.summary() 來看每一層的參數量以及輸出的張量（Tensor）長相： model . summary () 全連接層在最下面。而因為其與前一層「緊密」連接的緣故，它在 Keras 裏頭被稱為 Dense 層。它也是最早出現、最簡單的神經網路層之一。 Param # 則紀錄了每一層所包含的模型參數（Parameters）。在機器學習的過程中，這些參數都會不斷地被調整，直到能讓模型能做出很好的預測。詞嵌入層有最多的參數，因為我們要為 字典裡頭的每個詞彙都建立一個 256 維度的詞向量，因此參數量為 10,000 * 256。 這張表另外一個值得注意的地方是所有層的 Output Shape 的第一個維度都是 None 。而 None 代表著可以是任意的數字。 在 Keras 裡頭，第一個維度代表著樣本數（#Samples），比方說前 9,527 筆新聞標題 A 的數字序列的 shape 應該要是 （9527, 20） ： x1_train [: 9527 ] . shape (9527, 20) 嗯，結果跟我們想像的一樣。 而之所以每層的樣本數為 None 是因為 Keras 為了因應在不同場合會丟入不同數量的樣本需求。比方說，在訓練時你可能會一次丟 32 筆資料給模型訓練，但在預測的時候一次只丟 16 筆資料。 Softmax 函式 Softmax 函式一般都會被用在整個神經網路的最後一層上面，比方說我們這次的全連接層。 Softmax 函式能將某層中的所有神經元裡頭的數字作正規化（Normalization）：將它們全部壓縮到 0 到 1 之間的範圍，並讓它們的和等於 1。 Softmax 能將多個數字作正規化，讓它們的值為 1 （ 圖片來源 ） 因為 所有數值都位於 0 到 1 之間 所有數值相加等於 1 這兩個條件恰好是機率（Probability）的定義，Softmax 函式的運算結果可以讓我們將每個神經元的值解釋為對應分類（Class）的發生機率。 以我們的假新聞分類任務來說的話，每個值就各代表以下分類的發生機率： 不相關： 0.46 新聞 B 同意新聞 A：0.34 新聞 B 不同意新聞 B：0.20 如果現在是在做預測且我們只能選出一個分類當作答案的話，我們可以說這次的分類結果最有可能是「不相關」這個類別，因為其發生機率最高。 在定義好模型以後，我們就可以進入下個步驟：定義衡量模型好壞的指標。 決定如何衡量模型的表現 為了讓機器自動「學習」，我們得給它一個 損失函數（Loss Function） 。 給定一個正確解答 y 以及模型預測的結果 y_head ，我們的模型透過損失函數就能自動計算出現在的預測結果跟正解的差距為多少。 透過損失函數的回饋，模型會盡全力修正參數，以期將此損失函數的值下降到最低（也就是讓預測結果 y_head 跟正解 y 越來越接近）。 圖中的拋物線即為損失函數 J(w)。當參數 w 有不同值時，損失函數的值也有所不同。模型會持續修正參數 w 以期最小化損失函數 （ 圖片來源 ） 那你會問，在假新聞分類裡頭，我們應該使用什麼損失函數呢？ 我們在 將正解做 One-hot Encoding 一節有稍微提到，我們會希望 正確的分類的機率分佈 P1（例： [1, 0, 0] ） 模型預測出的機率分佈 P2（例： [0.7, 0.2, 0.1] ） 這 2 個機率分佈的「差距」越小越好。而能計算 2 個機率分佈之間的差距的 交叉熵（Cross Entropy） 就是這次的分類問題中最適合的損失函數。 交叉熵能幫我們計算兩個機率分佈的差距，適合作為分類問題的損失函數 （ 圖片來源 ） 在 Keras 裏頭，我們可以這樣定義模型的損失函數： model . compile ( optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) categorical_crossentropy 即是我們剛剛所說的交叉熵，而 accuracy 則是準確度，會被我們用來在訓練過程中了解模型的表現情況。 精準度的定義為： # 模型預測正確的樣本數 ------------------- # 總樣本數 雖然有了交叉熵來當作我們模型的損失函數，但是實際上模型要如何更新裡頭的參數呢？我們需要一個 優化器（Optimizer） 來做到這件事情。 不同優化器透過調整參數來降低損失函數的情形，就像是在想辦法往溜滑梯的低處滑一樣 （ 圖片來源 ） 雖然我們有很多種優化器，但它們基本上都是從 梯度下降法（Gradient Descent） 延伸而來。 在上圖的不同位置，梯度下降法會重新計算每個參數對損失函數的梯度（斜率）。接著梯度下降法會利用該梯度來修正參數，使得使用新參數算出來的損失函數的值能夠持續往下降。 不同優化器則有各自往下滑的秘方，比方說自動調整 Learning rate 。 現在就先讓我們使用 RMSProp 優化器 。而在有了損失函數以及優化器以後，我們就可以正式開始訓練模型了！ 訓練模型並挑選最好的結果 這步驟很直觀，我們就是實際使用 model.fit 來訓練剛剛定義出來的孿生 LSTM 模型： # 決定一次要放多少成對標題給模型訓練 BATCH_SIZE = 512 # 決定模型要看整個訓練資料集幾遍 NUM_EPOCHS = 10 # 實際訓練模型 history = model . fit ( # 輸入是兩個長度為 20 的數字序列 x = [ x1_train , x2_train ], y = y_train , batch_size = BATCH_SIZE , epochs = NUM_EPOCHS , # 每個 epoch 完後計算驗證資料集 # 上的 Loss 以及準確度 validation_data = ( [ x1_val , x2_val ], y_val ), # 每個 epoch 隨機調整訓練資料集 # 裡頭的數據以讓訓練過程更穩定 shuffle = True ) 這邊特別值得拿出來提的是以下兩個參數： BATCH_SIZE NUM_EPOCHS 依照我們前面對損失函數（Loss Function）的說明，理論上模型是把訓練資料集裡頭的 32 萬筆資料全部看完一遍之後，再更新一次參數以降低損失函數。 但是這樣太曠日廢時，訓練可能要花很久才能完成。 實務上都是每次只放入幾筆訓練數據，讓模型看完這些資料後就做一次參數的更新。而這個「幾筆」，就是 BATCH_SIZE 。 依照 BATCH_SIZE 的大小，梯度下降（Gradient Descent, 後稱 GD）可以概括為 3 個類別： GD（ BATCH_SIZE = 訓練資料集大小，且這時不稱為 batch） Mini-batch GD（ BATCH_SIZE 通常為一個較小的 2 的倍數） SGD（ BATCH_SIZE = 1） 想像損失函數是個越往裡面值就越低的碗，梯度下降就是要想辦法到達中心點 （ 圖片來源 ） 如上圖所示，下方的 GD 因為在每次更新參數前都會看完訓練資料集裡頭所有的數據，因此它更新參數的方向是最可靠的。但要往前走一步就就得看完 32 萬筆數據，未免成本也太大。 另一個極端是上方的 SGD：模型每看完 1 個訓練數據就嘗試更新權重，而因為單一一筆訓練數據並不能很好地代表整個訓練資料集，前進的方向非常不穩定。 隨機梯度下降（SGD）與 Mini-batch 梯度下降的比較 （ 圖片來源 ） 因此我們常常採用的是中庸之道： Mini-batch GD 的方式來訓練模型，而這靠的是指定 model.fit 函式裡頭的 batch_size 。 NUM_EPOCHS 則很容易理解：你希望模型不只將 32 萬筆的訓練數據都看過一遍，而是每一筆資料還要多看過好幾次，以讓模型確確實實地從它們身上學到東西。 NUM_EPOCHS = 10 的意思就代表模型會重複看整個訓練資料集 10 次。 接著讓我們看看 Keras 的訓練過程： 利用 Keras 訓練神經網路的過程 因為模型的目標就是要最小化損失函數（Loss Function），你可以觀察到當模型看過越多訓練資料集（Training Set）的數據以後，損失值（loss）就越低，分類的準確度（acc）則越高。 這代表我們的模型越來越熟悉訓練資料集裡頭的數據，因此在訓練資料集裡頭的表現越來越好。 如果依照準確度以及損失值分別畫圖的話則會長這樣： 很明顯地，我們的神經網路有過適（Overfittng）的問題：儘管在訓練資料集表現得非常好（準確度超過 90 %、損失小於 0.2），在從沒看過的驗證資料集的表現就相對遜色不少。且在第 6 個 epoch 之後驗證資料集的準確度 val_acc 就沒什麼在上升，驗證集的損失 val_loss 則已經逐漸上升。 這代表模型利用從訓練資料集學到的模式（Pattern）還無法非常精準地預測沒見過的事物。 用 Keras 來實作深度學習的基本 3 步驟 如同我們在 這章節一開頭 所說的，雖然第 3 步驟：「訓練模型並挑選最好的結果」的 Keras 實作非常簡單（基本上就是 model.fit( ...) ），但實際上在一個機器學習 / 深度學習專案裡頭，你將會花 80 % 的時間在這個步驟裡頭調整參數，想辦法找到一個最棒的模型。 儘管如此，我們現在最想知道的還是這個模型在真實世界（也就是測試資料集）到底能表現多好，因此先讓我們試著拿這個簡單模型來做預測吧！ 進行預測並提交結果 就跟我們對訓練 / 驗證資料集做的 資料前處理 一樣，要對測試資料集（Test Set）做預測，我們得先將裡頭的文本數據通通轉換成能夠丟進模型的數字序列資料。 首先，讓我們把測試資料集讀取進來： import pandas as pd test = pd . read_csv ( TEST_CSV_PATH , index_col = 0 ) test . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tid1 tid2 title1_zh title2_zh title1_en title2_en id 321187 167562 59521 萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大 辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？ egypt 's presidential election failed to win m... Lyon! Lyon officials have denied that Felipe F... 321190 167564 91315 萨达姆被捕后告诫美国的一句话，发人深思 10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国 A message from Saddam Hussein after he was cap... The Top 10 Americans believe that the Lizard M... 321189 167563 167564 萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗 萨达姆被捕后告诫美国的一句话，发人深思 Will the United States wage war on Iraq withou... A message from Saddam Hussein after he was cap... 測試資料集跟訓練資料集的唯一差別只在沒有 label 欄位，因此我們只需要將當初在 資料前處理 章節使用的步驟原封不動地套用在測試資料集即可。 你可以趁機複習一下有哪些步驟： # 以下步驟分別對新聞標題 A、B 進行 # 文本斷詞 / Word Segmentation test [ 'title1_tokenized' ] = \\ test . loc [:, 'title1_zh' ] \\ . apply ( jieba_tokenizer ) test [ 'title2_tokenized' ] = \\ test . loc [:, 'title2_zh' ] \\ . apply ( jieba_tokenizer ) # 將詞彙序列轉為索引數字的序列 x1_test = tokenizer \\ . texts_to_sequences ( test . title1_tokenized ) x2_test = tokenizer \\ . texts_to_sequences ( test . title2_tokenized ) # 為數字序列加入 zero padding x1_test = keras \\ . preprocessing \\ . sequence \\ . pad_sequences ( x1_test , maxlen = MAX_SEQUENCE_LENGTH ) x2_test = keras \\ . preprocessing \\ . sequence \\ . pad_sequences ( x2_test , maxlen = MAX_SEQUENCE_LENGTH ) # 利用已訓練的模型做預測 predictions = model . predict ( [ x1_test , x2_test ]) 這些步驟現在對你來說應該都已經不再陌生。 讓我們看一下從模型得到的預測結果長什麼樣子： predictions [: 5 ] 跟我們之前討論過的一樣，模型針對每一筆成對新聞標題的輸入，會回傳給我們 3 個分類的機率值。 現在，我們只要將機率值最大的類別當作答案，並將這個結果轉回對應的文本標籤即可上傳到 Kaggle： index_to_label = { v : k for k , v in label_to_index . items ()} test [ 'Category' ] = [ index_to_label [ idx ] for idx in np . argmax ( predictions , axis = 1 )] submission = test \\ . loc [:, [ 'Category' ]] \\ . reset_index () submission . columns = [ 'Id' , 'Category' ] submission . head () 得到上面的 DataFrame 以後，我們可以將其儲存成 CSV 並上傳到 kaggle，而結果如下： 我們的 NLP 模型第一次的結果 如果你還記得我們在 用直覺找出第一條底線 的章節內容的話，就會知道這並不是應該多好的預測結果，但的確比多數票決好了一點點。 不過不需要操之過急，因為任何機器學習專案都是一個持續重複改善的迴圈。在第一次預測就做出完美結果的情況很少，重點是持續改善。 在第一次提交結果以後，我們還可以做非常多事情來嘗試改善模型效能： 改變字典詞彙量、序列長度 改變詞向量的維度 嘗試 預先訓練的詞向量 如 ELMo 、 GloVe 調整 LSTM 層的輸出維度 使用不同優化器、調整 Learning rate 改變神經網路架構如使用 GRU 層 ... 能改善準確度的方式不少，但因為牽涉範圍太廣，請容許我把它們留給你當做回家作業。 走到這裡代表你已經完整地經歷了一個 NLP 專案所需要的大部分步驟。在下一節．讓我們回顧一下在這趟旅程中你所學到的東西。 我們是怎麼走到這裡的 在這趟 NLP 旅程裏頭，我們學會了不少東西。 現在的你應該已經了解： NLP 中常見的數據前處理以及實踐方法 詞向量以及詞嵌入的基本概念 神經網路常見的元件如全連接層、簡單 RNN 以及 LSTM 能讀多個資料來源的孿生神經網路架構 如何用 Keras 建構一個完整的神經網路 深度學習 3 步驟：建模、定義衡量指標以及訓練模型 梯度下降、優化器以及交叉熵等基本概念 如何利用已訓練模型對新數據做預測 呼，這可真了不起，值得慶祝！ 能閱讀到這裡，我相信你對深度學習以及 NLP 領域是抱著不少興趣的。而為了讓你在閱讀本文以後能夠繼續探索這個世界，在下一章節我則會介紹 3 門非常推薦的線上課程。 最後，我則會在文末總結一下自己的心得。 現在，先看看有哪些課程吧！ 3 門推薦的線上課程 為了奠定 NLP 的基礎，這一個月我一邊複習舊教材，一邊看了不少教學文章以及線上課程。 截至目前，我認為有 3 個 CP 值十分高的課程值得推薦給你： 台大電機系李宏毅教授的 深度學習課程 奠定理論基礎 Coursera 的 Deep Learning 專項課程 理論 70 % + 實作 30 % Deep Learning with Python 注重程式實作 這邊說的 CP 值高（對，我知道你最愛 CP 值）指的是能用最少的時間、精力以及金錢來確確實實地學好 NLP 的理論及實作基礎。 李宏毅教授的 Youtube 播放清單 （ 圖片來源 ） 李宏毅教授 的機器學習課程內行的都知道，大概是全世界最好、最完整的 Deep Learning 中文學習資源。李教授在課程中廣徵博引學術論文，但卻同時非常淺顯易懂。你可以在這邊看到 教授所有的 Youtube 課程播放清單 。 就我所知，教授在台大上課很注重實作，有不少作業需要完成，但因為線上只有影片可以查看，因此我將其分類為「奠定理論基礎」。 Deep Learning Specialization （ 圖片來源 ） 原 Google Brain 的 吳恩達教授 的 Deep Learning 專項課程 則是 Coursera 上最受歡迎的深度學習課程。跟我們這篇文章最相關的 NLP 技術則被涵蓋在該專項課程的最後一堂課： Sequence Models 。 我在大約一年前完成包含 卷積神經網路 CNN 的前四堂課，而因為課程上線已有一段時間，現在影片大都有簡體或繁體中文的字幕，不太需要煩惱聽不懂英文。 Deep Learning with Python Video Edition 的作者 François Chollet 為軟體工程師出身，設計出知名深度學習框架 Keras ，目前則在 Google AI 工作。 該書以 Programmer 的角度出發，提供了利用 Keras 實現各種 NLP 任務的範例，十分適合在熟悉深度學習理論後想要實作的人閱讀。 就算你不想花錢買書或是訂閱 O'Relly Online ，你也可以在他有 5,000 多顆星的 Github Repo deep-learning-with-python-notebooks 看到跟該課程相關的所有 Jupyter Notebooks。 這些課程可以說是幫助我完成這篇文章以及 Kaggle 競賽的最大功臣，而我也希望能透過這篇文章的微薄之力讓你知道他們的存在，並隨著他們繼續你從這裏開始的 NLP 探險。 當然，除了以上 3 堂課程，你還可以在 由淺入深的深度學習資源整理 一文看到更多我整理的深度學習資源。你也可以直接前往 Github Repo 查看。 結語：從掌握基礎到運用巨人之力 網路上多的是專業的 NLP 教學文章或論文探討，但平易近人的中文文章卻少之又少。 在文章開頭我說： 希望這篇文章能成為你前往自然語言處理世界的最佳橋樑。 這野心聽起來很狂妄，但至少我已經嘗試用最平易近人的詞彙向你介紹這個 NLP 世界的一丁點基礎知識，而我也希望你真的學到了些什麼、獲得些啟發。 現在深度學習以及 NLP 領域實在發展太快，就算一個人有興趣也常常不知從何開始學起。 事實上，NLP 的發展速度還在加快，而這既是好消息也是壞消息。 NLP 如果是輛衝往未來的火車的話，深度學習就是它的引擎，而我們的數據是它的燃料。另外，多數人還沒有登上這台火車 這邊說的 NLP，其實更適合用人工智慧取代。 對還沒掌握機器學習 / 深度學習 / NLP 知識的人來說，這些技術只會離自己越來越遠，最後遠到只會在 新聞報導 或科幻小說上看到，儘管被這些技術驅動的龐大系統每天影響著他們的生活。 至於那些已經掌握這些知識的人，透過運用 如遷移學習等巨人之力 ，就連一般人也能做到以前憑自己力量做不到的事情。 比方說利用 Google 在今年 11 月公開的龐大 語言代表模型 BERT ，我不費吹灰之力就在本文的 Kaggle 競賽 達到 85 % 的正確率，距離第一名 3 %，總排名前 30 %。 我們之前設計的 LSTM 模型則僅有 67 % 準確度。 並不是說只要用新的語言代表模型就好，這篇學的東西都不重要。事實上正好相反：正是因為有了此篇的 NLP 基礎知識，才讓我得以順利地運用該巨人之力。如果你有興趣深入了解，可以接著閱讀 進擊的 BERT：NLP 界的巨人之力與遷移學習 ，用最直觀的方式理解並實際運用這個巨人之力。 BERT 是在 NLP 領域裡家喻戶曉的語言代表模型 （ 圖片來源 ） 深度學習以及 NLP 領域發展快速，但你總要從某個地方開始好好地學習基礎，而且越快開始越好。 所以我留給你的最後一個問題就是： 你，打算什麼時候出發？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html","loc":"https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html"},{"title":"Chartify：讓數據科學家效率加倍的 Python 資料視覺化工具","text":"如果你正在學習 Python 和/或 資料視覺化，看這篇就對了。 身為一個數據科學家（ D ata S cientist, DS），使用 R 語言或是 Python 來視覺化手邊的數據可說是件稀鬆平常，但卻不一定愉快的工作。很多時候你得要花個 10 到 20 分鐘「微整型」自己的圖表，寫一大堆瑣碎的 code 就只是為了調整 X 軸的數字格式，或是改變長條圖的方向。 Chartify 來拯救世界 Spotify 最近開源 了一個簡單卻同時強大的 Python 資料視覺化 Package： Chartify 。我在工作中實際使用後覺得非常方便，所以想在這邊跟你分享我的經驗、並展示如何實際利用 Chartify 來視覺化一些台灣的 Open Data，讓你快速上手 Chartify。 小提醒：Dark Mode 顯示的圖表顏色會跟真實圖表有所差距。 Chartify 圖表 （ 圖片來源 ） Chartify 有 3 大優點： 預設就很漂亮的圖表樣式 簡單直覺的 API 內建非常詳細的使用說明 透過此文你將會發現，相較於 Matplotlib 或是 ggplot2 等客製化能力強大但學習成本較高的繪圖工具，學習 Chartify 的投資報酬率非常地高，且你馬上就能將其實際應用在自己的工作裡頭。 這篇適合誰 只要你在學習資料科學或是 Python，這篇基本上都適合你： 菜鳥 DS：從頭了解如何使用 Chartify 及 Python 來產生漂亮圖表 老手 DS：利用 Chartify 來有效率地解決你目前 80 % 的繪圖需求 所有想要快速學會如何使用 Python 來做資料視覺化的你 事不宜遲，讓我們馬上開始使用 Chartify 吧！ Python 函式庫 接下來會畫不少圖，但在這篇文章我們只需要 Pandas 以及 Chartify 就能完成所有圖表，輕鬆寫意。 import pandas as pd import chartify 你可以先跟著此文了解 Chartify 功能，等到自己手癢時再參考 官方 repo 安裝。 範例資料及 Tidy 格式 為了讓你有資料可以馬上開始嘗試各種圖表，Chartify 貼心地內建了一些簡單數據。你可以利用以下的方式將其取出並建立一個 Pandas 的 DataFrame ： df = chartify . examples . example_data () df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date country fruit unit_price quantity total_price 0 2017-10-21 US Banana 0.303711 4 1.214846 1 2017-05-30 JP Banana 0.254109 4 1.016436 2 2017-05-21 CA Banana 0.268635 4 1.074539 3 2017-09-18 BR Grape 2.215277 2 4.430554 4 2017-12-08 US Banana 0.308337 5 1.541687 稍微檢視一下，你應該可以看出 df 這個 DataFrame 裡頭紀錄了不同國家 country 在不同日期 date 下，各個水果 fruit 的單價 unit_price 以及總價格 total_price 。 但更重要的是， df 的格式是 Tidy 格式。要使用 Chartify 畫圖，基本上你的 DataFrame 都該轉換成 Tidy 格式。Tidy 格式又是什麼呢？依照 R 語言 ggplot2 的作者 Hadley Wickham 的解釋： 1 個變數只存在一欄裡頭（Column） 一列（Row）則代表 1 個觀測結果 比方說，彙整資料的 Pivot Table 大多不是 Tidy 格式： df . pivot_table ( values = [ 'unit_price' ], index = 'country' , columns = [ 'fruit' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } unit_price fruit Apple Banana Grape Orange country BR 0.978156 0.252695 2.022562 0.501151 CA 1.009426 0.249756 1.997512 0.504068 GB 1.007317 0.241405 2.002525 0.501009 JP 0.988600 0.249436 2.060732 0.499539 US 0.987463 0.252078 1.970634 0.504666 在這邊，明明都屬於 fruit 這個變數底下的 Apple 、 Banana 等水果，其單價 unit_price 卻都自成一欄，而這會造成我們畫圖的困難。簡而言之，只要是你想拿來畫圖的變數，其值都應該只存在一個 column 裏頭。想了解更多 Tidy 格式的細節，可以查看 這篇文章 。 第一個 Chartify Chart 現在，讓我們馬上用 Chartify 針對水果的單價 unit_price 以及總價格 total_price 這 2 個變數畫一個簡單的散佈圖（Scatter Plot）： ch = chartify . Chart () ch . plot . scatter ( data_frame = df , x_column = 'unit_price' , y_column = 'total_price' ) ch . show ( 'png' ) 就算完全不熟 Chartify 或 Python，相信你也可以從程式碼感覺出來，使用 Chartify 畫圖非常地直覺。基本上定義好一個 Chart 物件 ch ，並使用 ch.plot.圖表類型 的語法即可繪製各種美麗圖表。 最後，你會需要一個 ch.show() 來告訴 Chartify 將圖渲染（Render）出來。而這邊使用 png 只是告訴 Chartify 將圖表輸出為圖片而非 HTML，方便此部落格的使用。 再看一次剛剛的程式碼： ch = chartify . Chart () ch . plot . scatter ( data_frame = df , x_column = 'unit_price' , y_column = 'total_price' ) ch . show ( 'png' ) 基本上所有 Chartify 圖表都需要數據（Pandas 的 DataFrame df ），以及要在 X 及 Y 軸上呈現的變數/欄位名稱。 參數 x_column 及 y_column 的名稱就暗示著你，1 個 column 對應到 1 個變數。在這邊我們將水果單價 unit_price 對應到 X 軸、總價格 total_price 對應到 Y 軸。 你現在應該可以想像，如果我們手上的數據不是 df 而是前面的 df.pivot_table 的話，就無法輕易地用 Chartify 畫出這個散佈圖了。 事實上，就算你不是用 Chartify 繪圖，Tidy 格式的數據也是比較推薦的。我們後面會看到，Tidy Data 可以讓你非常輕鬆地探索各個變數之間的關係。 Chartify 提醒你提供重要訊息 雖說「一張圖勝過千言萬語」，有時為了讓你的分析結果更容易被其他人理解、吸收，你需要加上以下訊息： 圖表標題、副標 X、Y 軸標籤 圖片的數據來源 重新檢視剛剛的散佈圖，你會發現 Chartify 跟其他繪圖工具相比非常地貼心。它在對應的位置直接列出程式碼，告訴你每個位置的文字該怎麼修改。 比方說你想修改標題（title）成「水果單價與總價格關係」，只要按照上圖指示在 ch.show() 指令之前輸入： ch . set_title ( '水果單價與總價格關係' ) 即可改變標題。 現在讓我們從善如流，依照 Chartify 的提示，將所有必要資訊填入看看： ch = chartify . Chart () ch . plot . scatter ( data_frame = df , x_column = 'unit_price' , y_column = 'total_price' ) # 這部分程式碼都已經被寫在圖上 # 照抄即可：） ch . set_title ( '水果單價與總價格關係' ) ch . set_subtitle ( 'Chartify 內建數據' ) ch . set_source_label ( 'Chartify' ) ch . axes . set_xaxis_label ( '單價' ) ch . axes . set_yaxis_label ( '總價格' ) ch . show ( 'png' ) 感覺充實許多了，不是嗎？ 值得一提的是，現在數據科學家經常利用 Jupyter Notebook 進行分析，而裡頭的程式碼及圖表 常常在不同 Notebook 之間被搬來搬去 。為了讓你設計的圖表即使離開原來的 Notebook 也能讓人看得懂，加入如標題等資訊能大大提升圖表的可讀性。 凡事有例外，或是懶了 雖然我強烈建議標題一定要有，但有時候如果畫的圖只是要給你自己看，或者是你不需要標註副標和資料來源時，則可以將 Chart 的 blank_labels 設定為 True 來隱藏所有文字： ch = chartify . Chart ( blank_labels = True ) ch . plot . scatter ( data_frame = df , x_column = 'unit_price' , y_column = 'total_price' ) ch . set_title ( '水果單價與總價格關係' ) ch . show ( 'png' ) 貼心提示一鍵消失，看來其他人得猜猜到底水果單價是 X 還是 Y 軸了！ 在對 Chartify 有了最基本的了解以後，讓我們以一些台灣的公開數據做點有趣的資料視覺化吧！ 每年有多少人拜訪台灣？ 台灣以風景、小吃以及人情味著名，但你曉得每年有多少人來台灣嗎？ 甚至再分細一點，在這些來台灣旅遊的人們當中，有多少是從歐洲來的呢？而又有多少是華僑呢？ 為了回答這些問題，我們可以利用 政府資料開放平臺 裡頭的 歷年來台旅客國籍統計 數據： df . head ( 6 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } continent year visits 0 亞洲地區 2002 1689854 1 歐洲地區 2002 209110 2 美洲地區 2002 439403 3 華僑 2002 539164 4 大洋洲 2002 48334 5 非洲地區 2002 9479 上面這個 DataFrame 實際上包含了從 2002 到 2016 年各大洲及華僑的訪台總人次。 值得一提的是，如果你實際下載 該資料集 ，會發現其格式很不 tidy。這個 DataFrame 事實上是經過大約 30 行 Pandas 的前處理而來，但為了讓你能專注在 Chartify 本身，在本文裡會省略所有資料前處理的說明。 接著讓我們以 Chartify 來視覺化此 DataFrame： ch = chartify . Chart ( x_axis_type = 'datetime' ) ch . plot . area ( data_frame = df , x_column = 'year' , y_column = 'visits' , color_column = 'continent' , stacked = True ) # 所有繪圖工具都需要這些說明 ch . set_title ( '歷年各大洲及華僑來台旅客人次' ) ch . set_subtitle ( '西元 2002 至 2016 年' ) ch . set_source_label ( '政府資料開放平臺' ) ch . axes . set_xaxis_label ( '西元年份' ) ch . axes . set_yaxis_label ( '來台旅客人次' ) ch . show ( 'png' ) 你可以觀察到，自 2009 年起訪台人數成長加快，而在約 2014 年時來台旅客人次突破 1,000 萬。另外你可以看到華僑的訪台人數增幅十分顯著。 回到 Chartify 本身。 扣除你在不管使用什麼繪圖工具都需要設定的標題、X 與 Y 軸標籤，事實上我們只用了 1 個關鍵的 ch.plot.area 函式，就畫出這張幾乎已經不需再做「微整型」的疊加區域圖了。 讓我們再仔細檢視一下 ch.plot.area ： ch . plot . area ( data_frame = df , x_column = 'year' , y_column = 'visits' , color_column = 'continent' , stacked = True ) 如同上一個散佈圖 ch.plot.scatter 的例子，我們告訴 Chartify df 是我們的 DataFrame，並將年份變數 year 對應到 X 軸、訪客人數變數 visits 對應到 Y 軸。 接著我們告訴 Chartify，我們想要依照洲 continent 來畫不同「顏色」的子區域，並透過 stacked=True 來將這些子區域疊加起來。 另外因為我們的 X 軸是時間類型的變數（年份），為了幫助 Chartify 畫圖，我們在第一行設定 Chart 時需要將 x_axis_type 設定為 datetime ： ch = chartify.Chart( x_axis_type='datetime') 自此大功告成，你只需要再輸入 ch.show() ，美麗的疊加區域圖自動產生。不需要再花時間調整一大堆瑣碎的樣式（style）就能將結果跟別人分享，正是 Chartify 強大與貼心之處。 一個參數就能轉變圖表 當然，我們也可以選擇不將各大洲的區域疊加起來，讓它們從同樣的基準點渲染。 這時只需要將上張圖的 ch.plot.area 函式裡頭的 stacked=True 拿掉即可： 各自渲染的區域圖讓我們很清楚地看到：從 2012 年開始，光是華僑的訪台人數就超越整個亞洲訪客人次，成為來台旅客的最主要來源，佔了將近一半。 回到 Chartify 本身，你應該已經發現自己甚至不需花時間來手動調整各個子區域的透明度（alpha），省了不少時間。 所以大家來台灣做什麼？ 我們剛剛利用區域圖（Area Plot）觀察了歷年各大洲訪台人數的變化，你會不會好奇他們都來這邊做什麼？ 至少我是蠻好奇的。這次一樣讓我們從 政府資料開放平臺 取得數據。 歷年來台旅客來台目的統計 看起來是一個不錯的選擇： df . head ( 8 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } continent purpose visits 0 亞洲地區 其他 1531958 1 亞洲地區 展覽 10814 2 亞洲地區 探親 221338 3 亞洲地區 會議 47428 4 亞洲地區 業務 530369 5 亞洲地區 求學 53118 6 亞洲地區 觀光 7192696 7 亞洲地區 醫療 37072 雖然上面只列出亞洲地區，事實上這個 df 裡頭記載在 2016 年依照不同目的，各大洲訪台旅客的人數。讓我們再次使用 Chartify 來做視覺化吧！ （當然，這個 DataFrame 也經過前處理，被轉換成適合繪圖的 Tidy 格式：一欄一變數） ch = chartify . Chart ( blank_labels = True , x_axis_type = 'categorical' ) ch . plot . bar_stacked ( data_frame = df , categorical_columns = 'continent' , stack_column = 'purpose' , numeric_column = 'visits' , normalize = True ) # 將圖例移到圖表下方，方便閱讀 ch . set_legend_location ( 'outside_bottom' ) # Chartify 支持 Chain Operation ( ch . set_title ( '2016 年各大洲旅客來台目的統計' ) . set_subtitle ( '目的百分比（％）' ) . axes . set_xaxis_label ( '各大洲' ) . axes . set_yaxis_label ( '來台旅客目的佔比（％）' ) . show ( 'png' )) 底下標籤從左到右分別對應到 bar chart 中每個 bar 由下到上的部分。 你從這張圖可以觀察到不少有趣的現象： 亞洲國家超級喜歡來台灣觀光 歐洲及非洲的來台求學比例較其他洲高 美洲國家來台探親的需求意外地高 非洲地區的「其他」目的耐人尋味 要利用 Chartify 做出一個像這樣訊息豐富的圖表也十分直覺： ch = chartify . Chart ( blank_labels = True , x_axis_type = 'categorical' ) ch . plot . bar_stacked ( data_frame = df , categorical_columns = 'continent' , stack_column = 'purpose' , numeric_column = 'visits' , normalize = True ) 我想看過上個區域圖，你應該已經可以輕鬆地掌握這段程式碼的意涵，不過讓我再囉唆一下。 為了在 X 軸上顯示多個長條且 1 個長條能代表 1 個洲，我在 chartify.Chart 裡頭設置了 x_axis_type='categorical' 。這是因為洲 continent 是一個分類型變數（Categorical Variables），且之後將被顯示在 X 軸上。 接著我在 ch.plot.bar_stacked 函式裏頭告訴 Chartify 分類變數為 continent ，且我們想要畫出訪台人數的實際數字 visits ，再用不同顏色表達不同目的 purpose 。 另外因為我們想觀察的是每一洲裡頭，各個目的佔該洲全部目的的比例，使用 normalize=True 來將每個長條顯示成百分比。 最後再加上 ch.show() 就大功告成： 一個字母就能翻轉世界 在很多繪圖工具像是 Matplotlib，要將長條圖轉向是一個非常不直覺的事情。但在 Chartify 裡，你只要改變一個英文字母。 上張圖的 chartify.Chart ： ch = chartify . Chart ( blank_labels = True , x_axis_type = 'categorical' ) 我們只要將其中的 x_axis_type 改成 y_axis_type ，其他部分都不變，Chartify 就知道該轉換方向了： ch = chartify . Chart ( blank_labels = True , y_axis_type = 'categorical' ) 而本來的長條圖會變成這樣： 樣式也完全不需調整，再次省了我們不少時間。你甚至可以看出大洋洲的來台訪客之中，還有一點點的醫療需求。 讓我們再看一個用 Chartify 展示公開數據的例子。 在台北市，哪些地區比較多意外事故？ 這問題重要嗎？嗯 .. 雖然我現在在東京，知道這件事情或許可以增加我回台之後在台北街道上的存活率，何樂而不為？ 臺北市行政區劃 要回答這個問題，我們可以從 臺北市政府資料開放平台 裡頭的 交通事故資料 找出一些端倪： df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } district sex injury 0 南港區 女性 799 1 萬華區 女性 1084 2 松山區 女性 1146 3 大同區 女性 1336 4 士林區 女性 1660 這個 DataFrame 裡頭紀錄了台北市在整個 2017 年裡，各個行政區因為交通事故而造成的受傷人數，男女分計。 讓我們試著用 Chartify 視覺化這個 DataFrame： ch = chartify . Chart ( x_axis_type = 'categorical' , y_axis_type = 'categorical' ) ch . plot . heatmap ( data_frame = df , y_column = 'sex' , x_column = 'district' , color_column = 'injury' , text_column = 'injury' , color_palette = 'Reds' , text_format = ' {:,.0f} ' ) ( ch . set_title ( '2017 年台北市各行政區交通意外受傷人數' ) . set_subtitle ( '性別分計' ) . set_source_label ( \"Data.Taipei\" ) . axes . set_xaxis_label ( '性別' ) . axes . set_yaxis_label ( '行政區' ) . show ( 'png' )) 結果跟你想像的一樣嗎？ 普遍來說，男性的受傷人數較女性來得高（2 倍以上），而看起來在中山區的時候可能要特別注意交通安全！ 當然，上面這句是玩笑話。因為我們手上並沒有數據能用來計算各個區域的受傷率，所以不能說中山區的受傷總人數第一名就一定比較不安全。但這個 demo 想說明的重點是，透過 Chartify 你可以在非常短的時間內產生這種非常高品質的圖表，學習的投資報酬率十分地高。 Chartify 心法及資料視覺化 另外看了這麼多的實例，相信產生上張圖的程式碼也不需要我再逐行解釋了。 事實上，你可能已經看出用 Chartify 繪圖的套路了： 定義一個 Chart 物件 決定 X、Y 軸的變數型態（如 categorical ） 決定圖表類型（如 scatter 、 heatmap 等） 決定各個變數要對應到什麼 視覺變數 （如 X 軸、顏色等） 加入標題與 X、Y 軸標籤 渲染圖表，開心分享結果 我自己把上述非常簡化的步驟稱為 Chartify 心法 ，但事實上不管你用什麼繪圖工具都會走過類似的步驟。只是其他工具使用上可能沒有那麼直覺，讓你需要不斷地「微整型」圖表，導致見樹不見林的窘境。 這篇文章因為篇幅有限，沒有辦法面面俱到，但如果你對第 4 點的視覺變數以及如何增加 Data Ink 有興趣，可以參考我之前寫的 淺談資料視覺化以及 ggplot2 實踐 結語 呼！我們的 Chartify 旅程到此告一段落了！ 這是一篇長度出乎自己當初預期的文章。但我相信（也希望你認同），閱讀此篇的投資報酬率是非常地高的。不管你是老手 / 菜鳥資料科學家或只是對 Python 有興趣，應該都能從這篇學到一些資料視覺化的基礎以及 Chartify 的使用方式。 目前的 Chartify 當然不能解決所有繪圖需求，但已經能解決最常見的 80 % 使用案例，且我對其發展的前景也抱持著樂觀態度。 接下來你可以： 前往 官方 Github 查看 Chartify 的安裝以及其他指南 利用本章學到的基礎，探索 Chartify 提供的 教學 Notebook 以及瀏覽 所有範例圖表 實際應用本文所學在自己的生活或是工作裡頭，快速提升效率 分享本文給可能會對 Python 或資料視覺化感興趣的朋友，讓他們感謝你 就是這樣啦！ 由衷地感謝你的閱讀，我們下次見！ Chartify 畫廊 雖然現在沒有什麼圖，但之後我會將利用 Chartify 視覺化 Open Data 的圖表以及程式碼放在這邊供你參考。 也非常歡迎留言或是寄信給我，分享你實際應用 Chartify 以後做出來的圖表以及數據，我會將你的圖及名字附在這裡：） Lollipop Chart df = pd . read_csv ( 'dataset/titanic-train.csv' ) df_group = df . groupby ([ 'Pclass' , 'Sex' ])[ 'Survived' ] . sum () . reset_index () df_group . Pclass . replace ({ 1 : '頭等艙' , 2 : '二等艙' , 3 : '三等艙' }, inplace = True ) df_group . Sex . replace ({ 'female' : '女性' , 'male' : '男性' }, inplace = True ) ch = chartify . Chart ( blank_labels = True , y_axis_type = 'categorical' ) ch . style . set_color_palette ( 'categorical' , [ 'OrangeRed' , 'Mediumblue' ]) ch . plot . lollipop ( data_frame = df_group , categorical_columns = [ 'Pclass' , 'Sex' ], numeric_column = 'Survived' , color_column = 'Sex' , categorical_order_by = 'labels' , categorical_order_ascending = True ) ch . set_title ( '1912 年鐵達尼號旅客存活人數' ) ch . set_subtitle ( '分艙等、性別統計' ) ch . set_source_label ( 'Kaggle' ) ch . axes . set_xaxis_label ( '存活人數' ) ch . axes . set_yaxis_tick_orientation ([ 'horizontal' , 'horizontal' ]) ch . show ( 'png' ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/chartify-a-simple-yet-powerful-python-data-visualization-tool-which-boost-your-productivity-as-a-data-scientist.html","loc":"https://leemeng.tw/chartify-a-simple-yet-powerful-python-data-visualization-tool-which-boost-your-productivity-as-a-data-scientist.html"},{"title":"我在比利時 EMNLP 之旅中學到的 3 堂課","text":"我想將最近在比利時 布魯塞爾 參加自然語言處理的頂級會議 EMNLP 2018 的心得記錄下來並與你分享。 EMNLP 2018 會場門口 這篇文章會把我數天參加會議時所感受到的個人想法總結成最重要的 3 個 lessons。希望你在閱讀我的故事以後，一樣也能從中獲得一些啟發。 最後在文末，我則會說明這些想法將如何影響今後部落格的走向。如果你是忠實讀者，或許也會有興趣了解這個故事：） 計畫之外的 NLP 之旅 今年 10 月底，我跟公司請了一個禮拜的假參加在布魯塞爾舉辦的 EMNLP 會議 。 EMNLP 作為世界頂尖的 自然語言處理 會議之一，每年都有無以數計的專業人士聚集在此，與他人分享自己最新的研究成果。與大多數國際會議相同， 深度學習（Deep Learning） 的蹤影基本上無所不在。 你可以在 這邊看到所有議程以及論文 。 Workshops / Tutorials 第一天報到的人龍 不過老實說，在同事提及此會議之前，我並沒有聽過它的名號，更不用說考慮報名參加了。我本來預計是要參加跟資料工程相關的 DataEngConf 會議。（你可能已經從 我寫過的幾篇資料工程文章 了解我對 DE 的興趣） 但想説難得有機會深入了解現在 NLP 的研究趨勢，況且人多有個照應，稍微衡量一下就決定參加 EMNLP 了。 當時的我還不曉得，這趟旅程為自己帶來的收穫，比原先預想地來得多。 幾萬呎高空上的兩本書 這趟旅行從我坐上飛機就開始了。 從東京到比利時，飛行距離大約有 1 萬公里，直飛也需快 12 小時。在長途飛行中沒有網路，因此我這次決定帶兩本買了好一陣子都沒翻開的書，計畫去程與回程各看一本。 中途因為沒有任何如 Facebook 通知的干擾，我在飛機快抵達布魯塞爾時消化完《你要如何衡量你的人生？》的內容。 這次跟我一起去比利時的兩本書 （ 圖片來源 ） 透過此書我學到，那些你在職場或人生中設定的策略以及計劃，在實際展開行動去執行它們之前，什麼都不是。為了達成你要的目標，你得實際分配精力、時間等資源在上面，而非只是空想或說大話。 在回東京時，我則閱讀了 《華頓商學院最受歡迎的談判課》 。戴蒙教授用深入淺出的說明以及大量真實案例，再次提醒我在進行溝通或談判時，以「人」為本、設身處地的重要性。 這兩本書值得推薦，但在這裏，重點其實並不在於這兩本書的內容。在閱讀完《你要如何衡量你的人生？》時飛機正好抵達比利時，我則驚覺： 為何我當初在買書時早已預感能從此書獲得許多寶貴的思想，卻拖到現在才閱讀？ 這正是「沒有下定決心分配資源以執行策略」的活生生例子。 你的書櫃上是否也放了不少買了卻沒看的書？當初買書時你期望透過書本學到什麼？沒看完的原因又是什麼呢？ 如果「沒時間」是你的理由，那麼正說明了你跟當時的我一樣，沒有下定決心將自己的資源（時間）花在執行策略上面（看書變聰明、豐富人生）。 資訊爆炸時代，我們的閱讀時間變得極度零碎，也難以長時間集中自己的注意力。很多時候跟看 Youtube 影片比起來，我們會覺得讀一本書的「投資報酬率」太低：花費時間太多，帶來的刺激太少。 但其實並不是那麼一回事。好的書籍能改變你的一生，讓你終身受惠。而這次的機上閱讀帶給我的第一堂課即是： 閱讀好書是最好的長期投資，能豐富並讓你的人生更好。確保你會實際安排時間與精力去閱讀自己感興趣的書籍。 的確，你不需要像我一樣在幾萬呎高空上閱讀才能得到一樣的感想。但多虧了長途飛行給的專注時間，讓我在這趟旅行的一開始就重新體會到這件重要的事情。 從零開始的 NLP 之路 到達布魯塞爾的當天飄著綿綿細雨 下了飛機，坐地鐵來到市區，EMNLP 會議也即將拉開序幕。讓我們回到 NLP 的話題。 儘管我一直以來都對 NLP 抱持著不少興趣，過去卻沒有認真去了解近年深度學習在 NLP 領域的快速發展以及創新。 因此我明白，以自己當下幾乎是 0 的 NLP 知識水平，要在像 EMNLP 這種專業的會場內頭，迅速理解演講者們的論文發表這件事情的難度是很高的。 EMNLP 其中一個會議廳 基於這樣的背景，我將此次參加會議的目標設定為「掌握 NLP 基本概念以及關鍵字」。 為了達到這樣的目標，我有一個非常 naive 的策略，其分為三個步驟： 選擇有興趣的 Sessions 聆聽 聽到不熟悉的關鍵字就把它們記下來 Session 結束後 Google 這些關鍵字 來比利時，用功之餘也不能錯過淡菜及啤酒 當然你可以想像得到，一開始的幾場演講，作者的一句話或是一張投影片就能讓我打下無數關鍵字。 不過會議每進行一天，我就記越少關鍵字。這並不稀奇，畢竟大部分論文運用的「基本」 NLP 概念是相通的，而我也逐漸熟悉這些概念。（謝了，Google！） 為了讓你實際感受一下，以下節錄一些被我紀錄下來的關鍵字： Recurrent Neural Network（RNN） LSTM BiLSTM GRU SRNN Word Embedding ELMo GloVe BERT Evaluation / Dataset BLEU SQuAD NLP Tasks Named Entity Recognition Machine Translation Question Answering Text Summarization Style Transfer Reading Comprehension Open / Closed Domain Conversation Attention Mechanism Transformer ... 如果你平常有在接觸 NLP 領域，可能都已經對這些詞彙朗朗上口；但假如你跟當初參加會議時的我一樣，對 NLP 有興趣但卻什麼都不知道的話也別擔心，之後我會在其他文章解釋這些 NLP 術語並附上最好的學習資源。 EMNLP 當然不只談了上述東西，但以上詞彙應該沒有人會否認是現在 NLP 研究/應用領域裡頭常用的關鍵字。別忘了我們的目標是「掌握 NLP 基本概念以及關鍵字」。即使是 NLP 初學者如我，先了解這些詞彙的意義以及背後的理論，也能讓你對現在的 NLP 領域有個「還可以」的掌握。 這個「高頻關鍵字策略」很簡單，就跟我們從小學外語的方式如出一轍。在初學語言時，比較有效率的學習方法通常是先拿起「英文高頻 5000 單字」或是「常用日本會話 1000 句」來看，而不是去背一輩子可能看不到 5 次的「 火山矽肺症 」英文。 下個小節你會看到，這個策略的效果還不賴。 美術館驗收學習成果 為期數天的 EMNLP 會議裡的某一天晚上，在 皇家美術館 有一個與會者專屬的 Social Event。此活動讓所有人都可以欣賞到創作時期橫跨 15 世紀到 21 世紀的 20,000 件藝術作品。 路易．大衛的《馬拉之死》 除了欣賞如法國新古典主義畫家 路易．大衛 最為人知的 《馬拉之死》 等經典藝術作品之外，很多來參加 Social Event 的人是來「 Social 」的：跟一起來的同事聊聊天吃點心、想辦法多認識幾個厲害學者要個名片、或是找幾個陌生人討論彼此的研究。 利用上節説的簡單策略，我將目前流行的 NLP 術語理解了一遍，接著就這樣在皇家美術館裡頭拿著香檳與比利時巧克力，跟完全陌生的研究者、工程師們互相寒暄，大聊 NLP。 皇家美術館裡的 Social Event 我都跟他們說：「我完全不懂 NLP，是劉姥姥到大觀園。」但卻不只一位跟我說：「我覺得你 NLP 概念很不錯啊！我講的內容你都能理解，甚至還能給我的研究一些建議！」 但那只是因為我在前幾天學會了這門「 NLP 語言 」的基礎詞彙，並運用我不受任何限制的想像力，針對他們的研究給出一些自己的想法而已。 在這個夜晚，我學到了第二堂課： 在這個科技變化快速的時代，思考如何用最有效率的方式學習新知非常重要。未來，我們最大的潛力取決於能多快熟悉並掌握新事物。 別誤會，我並沒有說自己去了 EMNLP 就已經掌握了所有 NLP 專業知識，也沒有說學了一門知識的「基礎詞彙」就已經足夠。但對的方式能為你後面的學習奠定非常好的基礎及方向。 這邊的重點在於你要找出最有效率的方式學習，並突破傳統「要掌握一門學門得花數年時間的正統教育」的思考框架。現在網際網路上有數不清的資源等待你的探索，幫助你快速起飛。 開啟全新的學習之旅 我當初努力思考「從這趟 EMNLP 之旅，我究竟學到什麼？」這個問題時，發現會議裡頭的確有不少振奮人心的演說以及構思巧妙的論文，但真正讓我自己獲益最多的是以下 3 個體悟： 閱讀好書是最好的長期投資，能豐富並讓你的人生更好。確保你會實際安排時間與精力去閱讀自己感興趣的書籍。 在這個科技變化快速的時代，思考如何用最有效率的方式學習新知非常重要。未來我們最大的潛力取決於能多快熟悉並掌握新事物。 旅行其中一個好處是能讓你探索自我並改變人生。 薩布隆聖母教堂 前 2 點我們已經在前面花了不少篇幅解釋，在這邊我們花一點點篇幅說明最後一項： 旅行其中一個好處是能讓你探索自我並改變人生。 對我而言，這次的旅行是一個人生的轉捩點。它正式地打開了我「多年」對深度學習以及 NLP 興趣的開關，促使我開始大量學習相關知識。 之後的部落格，除了 資料科學 以及 資料工程 的文章以外，也將會包含自己學習深度學習以及 NLP 時使用到的線上資源和個人心得。如果你也對 NLP 與深度學習有興趣，或許之後可以從這裡學到點東西；而如果你能跟我分享好的 NLP 學習資源，我也會非常感激！ 不管如何，我都希望你能從我的故事裡頭獲得些啟發，重新思考你自己的學習，並做一些好的改變。 布魯塞爾美麗風景 篇幅有限，這邊簡單跟你分享這次旅程中我所看到的一些美麗景色。 EMNLP 會場附近風光 布魯塞爾大廣場 聖彌額爾聖古都勒主教座堂 主教座堂內部 聖心聖殿 聖殿內部 從聖心聖殿眺望布魯塞爾 這趟旅途雖然到此告一段落，但讓我們在下次的 NLP 文章再次碰面吧！：） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/3-lessons-i-learnt-from-emnlp-2018-at-belgium.html","loc":"https://leemeng.tw/3-lessons-i-learnt-from-emnlp-2018-at-belgium.html"},{"title":"資料科學家 L 的奇幻旅程 Vol.2 如何用資料工程當個時間旅人","text":"在 奇幻旅程的第一篇 裡頭你已經看到，為何了解企業內部使用的 KPI 以及熟悉公司內部的「數據流動」能讓一個資料科學家在工作上更如魚得水。 那是一篇稍微正經嚴肅，但我認為對資料科學家來說（ D ata S cientist，後簡稱 DS）很有幫助的一篇文章。不過今天，我想跟你分享一個輕鬆話題：身為 DS 的我，是如何利用資料工程（Data Engineering）在公司裡頭當個「時間旅人」的。 「時間旅人？你在開玩笑嗎？」 「資料工程跟時間旅行八竿子沒關係吧！」 這可能是現在困惑的你的最佳寫照 對對對我知道。 你或許正歪著頭，想著我是不是下了個釣魚標題騙你進來。我得承認自己是個浪漫主義者，常常會將工作上的任務跟看的小說、電影做聯想。但我想，聯想力或許就是人類跟 AI 最大的差距吧！我也不覺得這是件壞事：） 拉回正題。 所以什麼是時間旅人 對我來說，一個理想的「時間旅人」要能掌握兩種超能力： 能夠預測未來，洞察先機 能夠回到過去，修正錯誤 而事實上後面我們會發現要實現這兩個能力，尤其是後者，除了「資料科學」以外，我們還需要「資料工程」。 掌握資料工程讓我們彷彿可以穿越時空 預測未來，洞察先機 如果你也是一名 DS 或是分析人員的話，應該可以猜得到，在資料科學領域裡頭，所謂的「預測未來」是指「建立某些預測模型」。 只不過，光是建立出一個可以做預測的模型並不足夠。 不管是簡單的 Random Forest 還是複雜的 Neural Network ，要讓你的預測模型真正發揮影響力，你需要讓它實際上線做預測，而不只是活在你的 Jupyter Notebook 裡頭。 在資料工程領域裡，讓預測模型實際部署上線，才代表你能真正地開始「預測未來」。 你需要資料工程的知識來將一個 DS 做的預測模型「弄上線」 一些常見的預測模型案例： 使用者在安裝 App 7 天以後會不會繼續使用 顯示給使用者的廣告的點擊率 推薦給使用者的文章會不會被閱讀 預測使用者性別（儘管她/他沒說） 等等。 在目前的公司，我主要使用 Amazon SageMaker 、 Amazon ECS 並搭配 Airflow 來將這些預測模型部署到 Production 環境，以對真實世界做預測。 眼尖的讀者會發現，撇除模型或演算法，上述提到的工具並不實際跟「分析」有關，而比較偏向「工程」。為了發揮這些工具的最大效用，你可能需要了解 ETL 的概念 以及 如何使用 Docker 。 想要有效地預測真實世界，這些工具不可或缺。 在下篇文章，我將說明如何應用上述工具以建立可靠的預測流程。而在本文，我想強調的是另一個你能透過資料工程培養的超能力：「回到過去」。 回到過去，修正錯誤 如果你現在正努力學習資料科學，期待未來能成為一個 DS，你可能會「想像」進了一間新公司以後，前人都已經幫你把所有專案 / 產品分析需要的關鍵績效指標（ K ey P erformance I ndicators，即 KPI）定義完成。 除此之外，所有需要分析的數據也都事先被計算好並存放在 資料倉儲或是資料湖 裡頭供你大展身手。 而你所需要做的，就是開始下 SQL 查詢 並建立分析模型。 如果你的公司規模如 Facebook、Google 或是 Netflix 那麽龐大，裡頭已經有非常專業的 資料平台團隊 ，則或許上述為真。身為一個小小的 DS，你無須擔心什麼 KPI 的定義或是數據品質。 規模非常大的企業讓你看到自己的渺小，但好處是身為一個 DS，你要擔心的東西可能也比較少（數據品質、KPI 定義 etc） 但很多時候，這種抱持著「KPI 永遠是對的！」的假設需要承擔不小風險。 一般企業（尤其是新創）在事後發現，一直以來追蹤、監視的 KPI 計算需要做修正是常有的事情。 最常見的一個例子就是發現當時用來計算 KPI 的 SQL 查詢需要修正，而其原因可能是： 之前產品釋出新功能，但使用者利用該功能的歷史紀錄沒有被反映到 KPI 裡頭 少做了數據品質的檢查，導致表格裡頭有 NULL 的使用者 ID 等問題，無法識別用戶 KPI 裡頭包含了不該被計算在裡頭的雜訊 不管是哪項，我們都需要做修正。 具體來說，是修正該 SQL 查詢的邏輯、更新 KPI 定義，並將改變反映到 Production 環境。 這樣你才能確保在新的一天，該 KPI 能以最正確的方式被計算出來（假設我們一天算一次該 KPI）。 畢竟如同我們在奇幻旅程的第一篇： 新人不得不問的 2 個問題 裡頭看到的，錯誤的 KPI 數字會讓整個數據團隊或是公司策略走錯方向，影響可說是非常深遠，得及早修正。 在你修正該 SQL 查詢並部署到 Production 環境以後，唷呼！明天我們的 KPI 就會用最正確的邏輯被計算了！ 不過別開心得太早。 過去的數個月，甚至數年間持續被顯示在儀表板（Dashboard）上的數字可不會全部「自動地」被以新的邏輯重新計算。 但同時每個 PM 都拉著你，急著向你確認，到底用了最新的定義以後，該 KPI 過去的數字會變得如何、以及其對過去的分析的影響有多大。 這時你需要用新的計算邏輯 / KPI 定義來「更新」過去全部的計算結果，才能讓你公平地比較過去、現在以及未來的數字。 修正 KPI 定義以後，你會想要確保過去的數據也都隨之更新 這時候資料分析能力幫不了你，你需要的是資料工程的知識（或是一個老實的資料工程師， D ata E ngineer，DE）。 好消息是，如果你已經有在使用如 Airflow 等工作流程管理工具來管理你的 ETL，要「回到過去」並利用最新的計算邏輯來修正過去所有「錯誤數字」 並不是一件太難的事情。 事實上，「將過去執行過的 ETL 工作重新執行」這個任務在各個公司屢見不鮮，在資料工程領域裡頭甚至有其專業術語：Backfill。 Backfill：行家才懂的資料工程關鍵字 Backfill 本身直接翻譯成「回填」，在資料工程領域裡頭，代表著「用新的計算邏輯 / SQL 查詢」將過去執行過的 ETL 工作重新執行。 更白話的比喻，你可以想像 Backfill 就是把以前的你或是前人挖的坑、犯的錯「填好填滿」。 在這邊，「計算 KPI 」就是所謂的 ETL 工作。 目前我常常使用 Airflow 以及 Amazon EMR 來重新執行 ETL 工作，並讓實際的計算運行在 EMR 環境上以 scale。 Backfill 常見到 Airbnb 甚至自己建立了一個 Backfill Framework 。而在 一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載 裡頭，我則詳細探討了 Airflow 與資料工程的關係，以及你可以如何利用 Airflow 來「回到過去」，修正一切的錯誤。 利用 Airflow 回到過去，修正錯誤 就算你現在不需自己做資料工程，了解相關概念也會有所幫助。 在尋找數據相關工作或者想了解某個公司的數據環境時，可以詢問該公司的 DS / DE： 所以你們平常是怎麼做 Backfill ？ 這是了解一個公司內部的數據處理流程很好的一個切入點。 驚艷對方的同時，又能讓你實際了解非常多該公司數據平台的細節。 如果你立志成為 DS，且不希望之後操心數據品質或是自己對資料工程沒興趣，那你反而更需要搞清楚，想去的公司的數據環境如何，能否讓你專注在數據分析；如果你是想成為 DE，你能透過這個問題，逐漸了解這家公司適不適合你大展身手。 結語 在這篇文章，我非常輕描淡寫地談了作為一個 DS，我如何利用資料工程當個「時間旅人」： 預測未來，洞察先機 回到過去，修正錯誤 當然這只是我個人的例子。 實際上，每家公司的 DS 以及 DE 的工作內容都會有所不同。了解這個事實並調整期待，將幫助你找到最適合自己的工作環境。 如果你對資料工程多了點興趣，可以參考之前的文章： 資料科學家為何需要了解資料工程 。 就這樣，我們下個蟲洞見！ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/journey-of-data-scientist-L-part-2-time-traveling-with-data-engineering.html","loc":"https://leemeng.tw/journey-of-data-scientist-L-part-2-time-traveling-with-data-engineering.html"},{"title":"資料科學文摘 Vol.7 數據技能、深度學習以及 AI 的倫理道德","text":"今天讓我跟你分享 4 篇與數據以及人工智慧相關的文章。 在第一篇文章，我們將看到如何用一個簡單、有效的方式來決定應該學習什麼「數據技能」；在第二篇文章，我們則會看到如何透過數據，了解網際網路是如何快速地發展成為人們每天不可或缺的一部分。 接著我們會聽聽在計算神經科學領域的先驅之一，泰倫教授解釋何謂「深度學習」以及 AI 與人類智慧如何擦出火花；最後，我們將一窺 AI 的倫理道德議題以及著名的電車難題。 本週閱讀清單： Which Data Skills Do You Actually Need? This 2×2 Matrix Will Tell You. The internet's history has just begun A pioneering scientist explains \"deep learning\" Establishing an AI code of ethics will be harder than people think 廢話不多說，讓我們開始閱讀吧！ Which Data Skills Do You Actually Need? This 2×2 Matrix Will Tell You. 如同我們在 揭開資料科學的神秘面紗 一文提到，在一個數據時代，提升「資料科學力」這件事情不管是對你自己，或者是對公司的資料科學團隊來說都非常重要。畢竟 未來將需要更多跟數據處理相關的人才 ，數據導向的企業也越來越多。 但是要學的東西太多，你可能不知從何下手，或者什麼都想學。 這篇文章提供了一個簡單矩陣，將那些商業分析師、資料科學家以及機器學習工程師等數據相關職業的常見技能，依照 學習、精通該技能所需時間（Time, X 軸） 學習後能為自己及企業帶來的效用（Utility, Y 軸） 兩個要素，劃分出一個有 4 個象限的矩陣： 將常見的數據技能分門別類，以利建立學習的優先順序 從圖中你可以看到每個象限有不同的特色： 左上 Plan：這邊的技能如人工智慧、機器學習，雖然需要花更多時間來精通，但是未來很有用，因此你應該開始規劃長期的學習計畫 左下 Ignore：這裡頭的數據技能要花不少時間學習，但在未來能產生的價值卻不高，你應該盡可能忽略它們 右上 Learn：這邊的技能不需花太多成本精通，但能為你自己及企業帶來不少價值，應該馬上找時間學習 右下 Browse：這邊的技能用處普普，但學習成本也不高，可以瀏覽、儲存相關文章，等有需要的時候拿出來用 事實上，上面的技能擺放位置僅供參考，因為它只是某家公司的資料團隊自己判斷的結果。 你要思考的是，那些你想學的「數據技能」，在你現有的實力下，分別需要花多少時間精通？而它們又能在未來為你帶來多少幫助？ 在你心中或是企業策略裡頭，每個數據技能有了自己的位置以後，你就能非常清楚地知道該開始規劃什麼長期學習目標、該著手學習什麼，而哪些技能可以慢點再點。 以我自己為例，就有一些長期學習「機器學習」的規劃，而在日常工作時就頻繁地學習「資料科學」以及「資料工程」。 這個決定學習優先順序的概念，跟我們在 資料科學文摘 Vol.5 數據科學家面臨的挑戰、儀表板設計以及未來的被駭人生 一文中出現過的 艾森豪矩陣 有異曲同工之妙。 The internet history has just begun 雖然多數的我們早已習慣網際網路（Internet）的存在，但事實上以人類幾百萬年的歷史來看，網際網路的出現也不過短短 20 年，是一個非常年輕的發明（儘管它已經展現巨大影響力） 現在很多人已經無法脫離 Facebook、Google Maps、維基百科甚至是 Github。不過很難想像在我出生的時候（西元 1990 年）這些服務以及網際網路本身都還不存在。 圖中粗線代表各大洲近 3 個月有使用任何裝置上網的人口比例，每一條細線則代表一個國家。頂端粗線為北美（78 %）、最底下的粗線則為撒哈拉以南非洲（20 %） 儘管從上圖我們已經可以了解 20 年來網際網路的蓬勃發展，你會發現在 2016 年，上網人口也只佔全球人口的 46 %。 也就是說，世界上還有一半以上的人類沒有像你閱讀這篇文章般地使用網際網路。南亞以及撒哈拉以南也只有 20 ~ 30 % 的人在上網、東亞平均則為 53 %。 隨著網際網路在這些人口成長迅速的地區快速普及，可以合理相信，網際網路在接下來數年還會持續大幅度地改變人們的生活模式。 或許 Internet 的歷史現在才正式拉開序幕。 對我來說，閱讀 Max Roser 這篇文章給我的最大的啟示是： 人類生活模式的轉變只會越來越快，我們需要加速運轉自己的大腦，以跟上未來的變化。 A pioneering scientist explains \"deep learning\" The Deep Learning Revolution 的作者 Terrence Sejnowski（後簡稱泰瑞） 教授專注在研究神經科學（Neuroscience）以及計算機科學。 在這篇採訪裡頭，他簡單解釋了人工智慧、機器學習及近年備受注目的深度學習之間的關係。一言以蔽之，就如下圖所示： 人工智慧包含了機器學習，而機器學習包含深度學習 （ 圖片來源 ） 我想平常有在閱讀本部落格的讀者應該都十分熟悉這個關係，不須贅述。不過了解深度學習為何變得如此熱門的人就不多了。 一切要從 2012 年，全世界最大的 AI 學術會議 NIPS 說起。當年深度學習裡頭最關鍵的技術 Backpropagation 的發明者 Geoffrey Hinton 教授與他的團隊展示了如何利用深度學習，一口氣將擁有 10,000 個圖片分類以及多達 1,000 萬張照片的 ImageNet 分類挑戰 的錯誤率降低近 20 %。 在這之前，儘管已經有非常多的研究，這個挑戰的錯誤率每年下降不到 1 %。我們可以說，深度學習模型的出現，瞬間縮減了 20 年的研究時間。在那之後，人人爭相學習，開啟「大深度學習」時代。 現在人工智慧發展的背後推手主要即為深度學習，而深度學習的概念則來自於我們對人類大腦的理解。 泰瑞教授表示我們正處於人工智慧以及人類智慧相互匯合的時代： AI 與人類智慧正在匯合。當我們越了解大腦運算的方式，就會越傾向將該知識反映到 AI 上面，讓 AI 變得更強大。但同時，更強大的 AI 也讓我們用全新的方式以及理論來了解人類大腦以及上千萬神經元的運作方式。因此你可以看到在「神經科學」以及「人工智慧」之間有一個不斷互相學習的循環。 ─ Terrence 這個論點跟我們之前在 從彼此學習 - 淺談機器學習以及人類學習 一文中聊到的想法十分類似：到最後，我們及我們的下一代將不在只是從其他人類學習知識，而是向那些我們創造出來的 AI 學習。 未來教育模式的可能改變：從機器 / AI 中學習 （圖片來源： 從彼此學習 - 淺談機器學習以及人類學習 ） 舉個簡單例子，等到語音辨識的技術更為成熟，以後你的小孩可能不再需要一位昂貴的英文老師教他 / 她怎麼唸英文單字，而是透過一個 24 小時不休息的 AI，聆聽由深度學習自動產生的擬人發音來學習英文。 Establishing an AI code of ethics will be harder than people think 人工智慧的進步一日千里，快到我們還無法為其建立一套完善的道德準則。更甚者，完美的準則一開始就不存在。 AI 的快速發展讓我們已經（快要）可以把一些複雜任務如臉部辨識、自動駕駛等工作交給機器處理，從此過著輕鬆快樂的生活。 但你知道事情從來沒有那麼單純。 除了合乎程式邏輯以外，AI 在執行這些複雜任務時，很多時候會牽涉到道德問題。那麼又應該要由誰來決定這些 AI 在執行任務時應該要遵守什麼規定呢？ 「思考」中的 AI （ 圖片來源 ） 美國麻省理工大學 MIT 開發了一個名為「 道德機器（Moral Machine） 」的網站，裡頭重現了著名的 電車難題 ，目的就是為了告訴大家，每個人都有不同的道德標準，要為 AI 建立一套所有人都能滿意的道德準則非常困難。 自動駕駛版本的電車問題：誰該活？誰死了也沒關係？AI 該遵守誰的道德準則？ （圖片來源： 麻省理工大學「道德機器」網頁截圖 ） 你點進去回答完 13 道難題了嗎？ 如果還沒，我強烈建議你點進去 該網站（站內可選中文） ，並利用自己的道德準則決定自動駕駛車的運行方向，再實際看看有多少人以及動物因此受到影響，並了解其他人下的決定。 哪邊的按鈕你點比較多次，左邊還是右邊？ 如果你跟我一樣，花了不少時間掙扎猶豫，你就會了解「奠定 AI 所需要遵守的道德準則」這個課題有多麽困難。 就算你好不容易決定了，你也知道該判斷並不完美，你可能之後會後悔，且也不是所有人都同意你的決定。 紐約大學法學院 的 Philip Alston 教授則認為我們應該以「維護人權」為最高判斷原則，建立不會傷害到人類的 AI。以上面的自動駕駛來說，一個以「人文主義」為原則的自動駕駛車會選擇避開人群，而往一整群貓咪撞下去。 人文主義或許不完美，但或許是一個不錯的基準點。 只是我擔心的是，在這數據主義以及資本主義橫行的年代，人文主義最後是否能站得住腳。 如果自動駕駛還搭配了臉部辨識系統，利用大數據分析以及搜尋 犯罪記錄系統 ，自動車發現前方分別是一隻貓以及一個罪犯，它能否選擇撞人不撞貓呢？ 畢竟以「數據主義」的立場，「人」不再是至高無上的存在，一切由數據說的算。 結語 呼！以上就是本週文摘的內容啦！ 希望這些跟數據、AI 相關的文章以及我個人的想法有刺激到你的思考，讓你的生活變得豐富了一些，並實際思考做點什麼。 歡迎留言跟我說說你自己的想法、分享這篇文章或是點擊下面的訂閱按鈕。 最重要的，記得去 道德機器網站 實際做一下題目，感受一下 AI 時代的倫理難題。 就這樣，我們下次見啦！ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-7.html","loc":"https://leemeng.tw/data-science-digest-volume-7.html"},{"title":"資料科學文摘 Vol.6 人類壽命大進展、GAN、數據工廠以及產品分析","text":"文摘來到第 6 篇，不知道這是你看的第幾篇呢？ 這週我們一樣保持閱讀的「營養均衡」，從全球平均壽命變化的資料視覺化、深度學習最夯的「對抗生成網路」話題、產品分析框架到理解何謂「數據工廠」，我希望能讓閱讀本文摘的你，廣泛地了解各領域跟「資料」相關的議題，並進一步找出自己的興趣，加以深度探索。 本週閱讀清單： Twice as long – life expectancy around the world Interview with Deep Learning Researcher and The GANfather: Dr. Ian Goodfellow Data Factories Engagement Drives Stickiness Drives Retention Drives Growth 讓我們開始閱讀吧！ Twice as long – life expectancy around the world 現在全球健康以及公衛還是存在很多不平等，但別忘了我們已經取得巨大進展。 如同我們上週在 如何用 30 秒了解台灣發展與全球趨勢：用 GapMinder 培養正確世界觀 一文中聊到，好的資料視覺化可以幫助我們快速地了解世界。這週牛津大學的經濟學家 Max Roser 用 3 張橫跨 2 世紀的世界地圖，來告訴我們全球平均壽命（Life Expectancy）的變化： 我們可以看到這 200 年來，生活在世界上的人們經歷了 3 個階段： 在 1800 年以前所有人的平均壽命 < 40 歲，大部分兒童早夭 在 1950 年，部分地區健康大幅改善，歐美及日本的平均壽命為 60 歲，為非洲整體平均的 2 倍，鴻溝顯而易見 在 2015 年，幾乎全球所有地區都能活到 60 歲以上，鴻溝逐漸縮小 我們都希望自己親人及朋友活得長久。就是因為這樣，你更應該感激這 200 年人類取得的進步。 近 2 世紀人類在健康狀況改善的卓越成就，套句 Max Roser 的說法就是： 在人類歷史上，這是我們第一次改善了整個人群的健康狀況。在人類健康狀況停滯千年後，封印終於解除。 ─ Max Roser 值得一提的是，在 1950 年，台灣的平均壽命為 55.5 歲，經過了 65 年，來到了 80 歲。平均每 3 年，台灣人的平均壽命增加 1 歲，成長速度不可小覷。 你也可以用 Our World in Data 提供的圖表來看看全球變化： Interview with Deep Learning Researcher and The GANfather: Dr. Ian Goodfellow 伊恩．古德費洛（Ian Goodfellow）是 Google Brain 的研究科學家 ，最知名的成就是在 2014 年推出 生成對抗網路（Generative Adversarial Network, 簡稱 GAN） 。GAN 最基本的概念是讓兩個神經網路互相對抗，讓模型可以依靠較少的人類介入以及訓練資料，自己學會高度複雜的工作。自從那之後，GAN 領域的研究一日千里，現在 arXiv 上該論文有超過 5,000 次引用 。 GAN 有很多用途，像是自動產生高畫質圖片 （圖片來源： NIPS 2016 Tutorial ） GAN 有非常多「用途」，像是自動產生圖片、創作音樂、寫詩或是製造假新聞。但在這篇文摘裡頭，讓我們先專注於這篇訪問伊恩的內容。 在這篇訪談裡頭，伊恩給想開始研究 ML 的人一些建議： 徹底學好基礎。像是寫程式、除錯、並學習機率及線性代數。很多時候在研究 ML 的時候，幫助你最多的是扎實的基礎，而不是非常前衛的想法（這是他從 Google Brain 創立者 吳恩達 得到的建議） 沒有什麼運算資源時，要選對研究主題。（沒有像是 Google 那樣等級的運算資源的話，就不要想去實現全世界最準的 ImageNet 分類器） 一開始找個人家已經做過的題目來磨練你的 ML 能力。 最後一點需要額外解釋一下。 如果你在練習 ML 的時候，選擇跟隨前人「已經成功」的東西來實作的話，這樣就算自己實作出來的模型表現不好，你也知道只是你的實作、基本功出了問題，而不是這個點子錯了。接著只要回去複習基本概念、加強實作功力即可。 但如果你的 ML 的實作能力沒到一個水平，然後又馬上想要嘗試一個天馬行空的點子／演算法，最後實作出來失敗，你很難知道，到底是點子本身有瑕痴，還是因為你實作能力差而出問題。 另外如果你現在就想開始了解 GAN 的話，可以試試 GAN lab ，在網頁上玩玩生成對抗網路。 GAN lab 讓你可以利用網頁瀏覽器直接探索 GAN 並了解其運作原理 （ 圖片來源 ） Data Factories 身處數據時代，我們應該更關心自己的資料被怎麼利用。 這篇文章想說的是，其實 Facebook、Google 以及其他廣告業者都是所謂的「數據工廠」，而如果政府要立法規範這些工廠，最有效的方法就是請它們允許使用者看到工廠裡頭的情況。 我認為「數據工廠」是對 Google 及 Facebook 這種利用數據來創造價值的公司的一個貼切比喻。因為他們除了使用者的行為數據，也從廣告代理商以及第三方數據收集業者取得大量資料。透過將這些原始資料「加工」並產生衍生價值，據此創造巨大收益。 然而這些「數據工廠」跟一般傳統的「工廠」有一個非常大的差異：誰都無法窺探該「工廠」的內部情況。 記者可以去 Nike 製造足球的工廠裡頭拍拍照，讓世人知道這些工廠內部的運作情況，但在這年代，你無法去 Facebook 裡頭拍拍照，了解他們是怎麼利用各式各樣的演算法，來「活用」所有跟你相關的資料（你按過讚的內容、瀏覽過的網頁，甚至是你為了雙重認證而輸入的電話號碼）。 因此立法者以及那些關心自己數據可能被濫用的使用者要了解的是，要規範 Facebook 這種公司，不能只要求 Facebook 公布他們從使用者手上拿到的原始資料（Raw Data），而是應該公布那些他們利用演算法以及結合多種數據來源所產生出的 user profile，讓使用者自行判斷要不要繼續讓該公司使用自己的 profile。 雖然多數人其實只在乎 Facebook 能不能秀給他們更多的動物影片以及朋友動態，不太在意自己的數據被怎麼拿來獲利。 Engagement Drives Stickiness Drives Retention Drives Growth 在以提供 App 作為服務的公司裡頭，資料科學家大都會需要進行產品分析（Product Analysis）進而改善自家產品。 這篇文章介紹了 App 產業以及我常在使用的一個分析框架，讓你可以感受一下，實際上 DS 在做產品分析的時候，要看些什麼東西。 有做過產品分析的你，應該能很快地理解這個流程圖： 這張圖最重要的核心概念是： 當使用者發現你產品的價值以後，他們會主動回來。 當使用者發現你的產品的價值後，就會進一步參與使用（Engage），而好的參與程度（Engagement Level）會增加他們對此產品的黏著度（Stickiness），進一步讓他們願意回來繼續使用你的產品（Retentaion）。而有了越來越多的忠實用戶，就能進一步帶給你的產品成長（Growth），不斷持續地這個好的循環。 在我們理解每個階段代表的意義以後，我們還需要一些指標（indicators）來實際幫助我們了解產品在每個階段的表現。 像是 Engagement 底下的 TS/DAU 即分別代表「使用時間（ T ime S pent）」以及「每天活躍使用者人數（ D aily A ctive U sers）」。這兩個都很常被拿來衡量使用者參與一個產品的程度。有了好的參與程度，一個使用者就更有可能在安裝 7 天後還回來繼續使用（Retention 階段的 D7）。 這邊沒有篇幅一個個介紹圖中的指標，但要注意的是，在看指標的時候，要去想它是早期指標（Early Indicators）還是延遲指標（Lagging Indicators）。 比方說你的最終目標是提升每月活躍使用者人數（ M onthly A ctive U sers，最右邊 Growth 階段的 MAU）這個延遲指標（延遲在於要過了 1 個月你才知道結果），那你除了看 MAU 以外，還需要去看 TS/DAU 等早期指標。因為 MAU 需要一個月的時間才能計算出來，有時候產品表現差，你從每天使用的人數下降就可以略知一二，可以馬上做調整而不需等到一個月後 MAU 數字難看才大傷腦筋。 及早發現，及早治療。 產品分析領域在網路上的資源不多，有機會再跟你分享我的心得。 結語 呼！這就是本週文摘的內容啦！希望你閱讀後有感覺自己腦中多了點東西，變得聰明了一點。 社會人口、機器學習、產品分析以及數據隱私的議題，你會發現這些文章儘管領域大相徑庭，他們都與「數據」脫離不了關係。 在這個時代，任何人的日常生活中都充斥著大量數據。我們需要重新思考、檢視並理解身邊的數據，甚至活用它們來創造更好的世界。 這也是我寫這系列文章的原因，希望讓更多人（包含我自己）能更輕鬆地用數據理解這個世界。歡迎你點擊下面的訂閱按鈕，未來跟著我一起繼續探索這個世界：） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-6.html","loc":"https://leemeng.tw/data-science-digest-volume-6.html"},{"title":"如何用 30 秒了解台灣發展與全球趨勢：用 GapMinder 培養正確世界觀","text":"再稍微花個 5 秒鐘咀嚼一下你所看到的。 現在問你自己，你看到了什麼？ 在這個資訊爆炸的時代，大腦為了保護你的心智不被大量數據淹沒，可能已經很習慣性地忽視眼前數據其背後所隱含的意義。 但讓我提醒你一下，就在剛剛的 30 秒內，全世界過去 200 年至今的經濟（所得收入）與社會（平均壽命）發展狀況活生生地重現在你眼前！ 這可不是小時候歷史老師會 / 能秀給你看的東西（至少我的老師沒有）我不知道你感受如何，但在我 第一次見識到此圖 的時候，內心可說是激動得不得了！ 瑞典全球公衛教授漢斯・羅斯林 2006 年在 TED 利用上面的泡泡圖向觀眾們解說世界的經濟與社會發展 （ 圖片來源 ） 將專注拉回台灣。看著台灣的發展軌跡，你甚至還可以發現一些值得注意的現象： 1939 至 1945 年，人民所得以及平均壽命走倒車（二戰） 1945 至 1953 年國民平均壽命的大幅提升，所得回歸正常 1960 年代以後，經濟與社會的持續穩定發展 2000 年後成長趨緩 除了本身的發展軌跡以外，還可以發現到了 21 世紀，台灣在右上角，名列前茅。 您的瀏覽器不支援影片標籤，請留言通知我：S 泡泡圖除了可以讓我們觀察世界趨勢，也能同時了解台灣的發展軌跡以及與其他國家的相對位置 剛剛在看圖的時候，你應該還有很多其他發現且迫不及待地想要了解更多。 事實上，如果在看了剛剛的動畫以後，你突然渴望想要知道更多是很正常的。 重要的是不要停止問問題，好奇心有其存在的理由。 ─ 愛因斯坦 如果你現在想要更加地了解台灣或是其他不同的國家在各種社會 / 經濟 / 健康指標的發展（如所得收入、兒童死亡率、二氧化碳排放量等），我鼓勵你先上去改改 X 或 Y 軸、點選不同國家，查看結果以後再繼續往下讀。 畢竟文章跑不掉，你的好奇心則可能在幾秒鐘後消逝：） 你回來了嗎？ 你現在應該已經暸解，透過值得信賴的數據來源（比方說 聯合國 ）以及良好的呈現方式（文章開頭的泡泡圖），能讓你在很短時間內「正確」地掌握全世界趨勢以及台灣的發展狀況。 儘管媒體總是報憂不報喜，你會發現全世界大致上變得越來越好。 你也會發現以「人均收入」以及「平均壽命」的角度來看，台灣的表現在全世界也是數一數二，這點值得我們欣慰及驕傲。 《真確》是 2018 年由漢斯・羅斯林（Hans Rosling）所撰 （圖片來源： 迷誠品 ） 在《真確》這本書裡，羅斯林教授闡述如何利用數據以及正確心態來理解世界，是一本深具啟發性的著作。而「泡泡圖」則是他在傳達知識時，經常使用到的工具。 首先你需要知道，文中的泡泡圖（Bubble Chart）的開發以及各個國家的數據整理，並非由我獨自完成，而是由漢斯・羅斯林教授與他所創辦的 GapMinder 基金會 從多個國際組織（如聯合國、國際衛生組織、世界銀行等） 蒐集、整理 而來。（給他們點掌聲！） 事實上，你可以直接使用 官方的泡泡圖 ，或是像本文一樣，依照 這邊的教學 來將泡泡圖內嵌在你自己的網站裡頭。 我知道你在想什麼。 「既然官方都已經有泡泡圖了，為何你要在這裡再弄一個出來呢？」 非常好的問題，但讓我先賣個關子。 我會在 為何需要本文的泡泡圖？ 章節裡頭仔細說明。（提示：跟台灣有關係） 在這邊想先讓你知道的是，文章接下來會說明泡泡圖裡頭有什麼台灣數據可供你探索，以及提供一些探索台灣以及世界的例子，讓你在了解世界的同時熟悉泡泡圖的使用方式。 等你熟悉泡泡圖以後，可以利用它來更深入瞭解台灣與以及任何你有興趣的國家，並培養正確的世界觀。最重要的是，在有了正確思維以後，你能怎樣讓世界以及台灣變得更加美好。 前言很長，不過接下來才是重頭戲。準備好了就跟上我們的探索之旅吧！ 本文章節 為何需要本文的泡泡圖？ 有什麼台灣數據可供探索？ 用泡泡圖探索世界 媽媽不生寶寶了：生育率大幅下降 怎麼創造乾淨未來：煤炭消耗與環境污染 民主大躍進：我很自由，不過不想參與政治 看到數據背後的故事 你能怎樣讓世界更好？ 為何需要本文的泡泡圖？ 你可能在想，何必要大費周章地弄出自己的泡泡圖。畢竟，只要使用 GapMinder 基金會（以下簡稱 GapMinder） 官方釋出的泡泡圖 就可以透過數據來探索「全世界」與「台灣」了啊？ 這句話只對了前半段。 第一個沒有那麼嚴重但是有點令人困擾的問題是，目前官方的泡泡圖只有英文，沒有繁體中文。 雖然台灣人的英文能力普遍不差，但是要所有人在看到每個國家的英文名字後馬上反應出來，可不是一件簡單的事情。 你還有多少把握可以認出東帝汶或柬埔寨的英文名字？（提示：下圖有其中一個） 更不用說聯合國以及各個國際組織定義的各式各樣社會 / 經濟 / 公衛指標的英文了。（還記得結核病、旱災或是償債出口比怎麼唸嗎？） 我們看泡泡圖的主要目的是為了瞭解世界，而不是學習翻譯各種英文專業術語。 就算這樣講，英翻中或許問問 Google 還是勉強可以解決。但官方泡泡圖存在的第二個問題，則非常致命。 GapMinder 釋出的泡泡圖。在右邊的清單搜尋「 Taiwan 」不會有結果 （ 圖片來源 ） 在我撰寫此文的這個時間點（2018 年 10 月），在 GapMinder 上的泡泡圖 裡頭，你並無法找到「 Taiwan 」的存在。 沒錯，你可以現在 去搜尋看看 。然後你會發現有 2,300 多萬人口的台灣並不存在 GapMinder 的泡泡圖之中。 依據 GapMinder 的說法 ，泡泡圖預設只顯示 聯合國會員國 。因此理所當然地，台灣不會被顯示在上面。 儘管沒有在聯合國裡頭，在台灣努力生活的人們確確實實地存在著 （圖片來源： 寧夏夜市 ） 2018 年 7 月，GapMinder 表示 他們正在想辦法讓非聯合國會員國（如台灣、香港）也能被加到泡泡圖裡頭 ，但自從那之後已經過了數個月。 我真的不怪他們，畢竟他們是非營利機構，人手有限且已經為世界做出很多貢獻了。 聯合國總部，紐約 （ 圖片來源 ） 只是，我無法忍受在閱讀完《真確》並想要開始認真地探索這個世界的時候，發現裡頭竟然沒有熟悉的台灣。 後來的故事你大概猜得到了。我開始研究 GapMinder 製作泡泡圖的程式碼 以及 數據儲存格式 。我寫些程式、閱讀聯合國以及各個國際組織的相關文獻以後，把台灣「駭」進泡泡圖的國家列表裡頭，並將裡頭所有國家以及（幾乎）所有指標翻譯成中文。你在文章開頭看到的泡泡圖就這樣誕生了。（感謝 Google 大神以及咖啡因！） 現在，在了解本文泡泡圖的典故之後，讓我們看一下目前的泡泡圖裡頭有哪些台灣數據可供你探索。 有什麼台灣數據可供探索？ GapMinder 將所有搜集來的資料整理在這個 Github Repo 裡頭，也是本文泡泡圖的數據來源。 理想上，每個指標（如二氧化碳排放量、國民平均壽命、人均收入）都會（或者說都要）包含每個國家及地區每年的資料才能方便我們做比較。但你可以想像，這不太可能實現。 實際上，依照不同國家的數據開放狀況、國際組織蒐集數據的方法差異，都有可能造成指標裡頭沒有某些國家的資料。 以台灣為例，透過分析 GapMinder 數據來源，我們可以知道，截至目前為止，泡泡圖裡頭總共有 500 多個指標，其中約有 40 % （ 200 個 ）指標含有台灣數據。 （跟本文的泡泡圖以及數據來源一樣，此圖的資訊也會定期更新） 以大分類來看的話，「健康」及「工作」分類有較多的資料可供我們檢視台灣的狀況並同時與其他國家做比較；相較之下，「社會」及「人口」涵蓋的台灣指標較少，公共建設分類則只有 1 個（交通死亡人數）。 健康分類中，屬男女的「癌症」相關數據最為完整：大腸癌、胃癌、肝癌、乳癌、攝護腺癌 .. 應有盡有。 工作分類則有各個年齡層的失業 / 就業率及「勞動參與率」等指標，你可以自行稍後在泡泡圖上查看。 不過實際上，你也不需記住哪些分類有多少台灣數據。 利用泡泡圖的選單，我們可以馬上知道每個分類底下有多少指標、有哪些指標有台灣數據、最早的年份為何 如果你剛剛有玩泡泡圖的話，可能會好奇在每個分類後面的數字代表什麼。圖中健康分類後面的 （63/166） 代表在泡泡圖中，健康分類底下總共有 166 個指標，而其中的 63 個有台灣數據。這跟我們上一張長條圖吻合。 現在看到上圖第三欄的「肺癌病例數」：指標名稱後面的 （1990 ~ 則代表在該指標中，台灣數據最早可以追溯到西元 1990 年。有了這些額外資訊，可以讓你更方便地探索台灣與世界的關係。 值得一提的是，以上的結果僅代表 GapMinder 目前有的數據。他們持續努力地在添加新的數據，而我也預計在未來導入更多的台灣數據。但現在，先讓我們從已有的指標裡頭選幾個來探索看看吧！ 用泡泡圖探索世界 在《真確》裡頭，漢斯・羅斯林教授已經向我們展示了很多很棒的泡泡圖範例。而在這個章節裡頭，我會列出一些自己利用泡泡圖探索世界以及了解台灣的例子。 （小提醒：底下的圖幾乎都是動態的。如果你發現它們沒有動靜，請另外使用電腦或是手機上的瀏覽器開啟此頁連結以最佳化閱讀體驗，謝謝！） 媽媽不生寶寶了：生育率大幅下降 很多我們以為是常態的事物，事實上在幾十年前完全不存在。 您的瀏覽器不支援影片標籤，請留言通知我：S 以婦女人均嬰兒數為例，在 1950 年前，跟亞洲大多數國家相同，台灣每個婦女平均有 6 個嬰兒。現代大多數的年輕人應該無法想像這件事情。 但我們可以看到從 1960 年代開始，婦女人均嬰兒數以不可思議的速度溜滑梯下降，直到近年每位婦女平均只有一名嬰兒。 解釋歷史從來不簡單，但我們可以想像在當時，醫療技術以及節育概念還不高，間接造成較高的兒童死亡率。兒童的死亡率高，也就代表平均一位婦女需要生產更多嬰兒來延續後代。要證實這點，我們可以把 X 軸的「人均所得」換成「兒童死亡率」： 您的瀏覽器不支援影片標籤，請留言通知我：S 不只台灣，我們可以發現全世界有一樣的趨勢：兒童死亡率下降，而同時媽媽們也不需再生那麼多寶寶。這現象很大部分是因為醫療進步、女性教育的普及以及家庭觀念的改變。 另外從代表不同洲的顏色可以看到，在 2018 年，所有婦女人均嬰兒數 > 6 的國家都位在非洲。 當然，婦女人均嬰兒數減少，同時也代表 高齡化社會的來臨 。讓我們將 X 軸換成「60 歲以上人口佔總人口比例」以後，看看日本的發展： 您的瀏覽器不支援影片標籤，請留言通知我：S 日本的高齡化人口比例增加 ，也代表青壯年的負擔加重。不只日本，在未來要怎樣建立一個良好的長照制度，在台灣也是一個日漸重要的議題。 怎麼創造乾淨未來：煤炭消耗與環境污染 台灣能源供給高度仰賴進口，其進口量長期維持在 97 到 98 ％，而 煤炭又為台灣第二大主要進口能源 。 因為碳密度高，燃燒煤炭又會產生比其他化石燃料（石油、天然氣）來得更多的二氧化碳，造成更嚴重的氣候暖化以及環境破壞。讓我們看看從以前到現在，一個台灣人平均消耗的煤炭以及產生的二氧化碳的變化趨勢： 您的瀏覽器不支援影片標籤，請留言通知我：S 我們可以看到從 1990 年起，台灣煤炭的人均消耗量（用來發電）快速增加，而同時人均二氧化碳的排放量也逐年增高。儘管近年趨向穩定，我們可以看到作為對照組的美國在 2010 年以後的人均煤炭消耗量已經低於我們。 環境考量以及再生能源的成本下降，讓歐美各國的政府以及能源業者決定投向再生能源懷抱，但台灣似乎還想要 建立燃煤電廠 。 在 2014 年時，只有哈薩克跟澳大利亞的煤炭消耗量超越我們。而作為世界第一煤炭出口國，澳洲自己也因為大量開挖煤炭而導致大堡礁的生態浩劫。 怎麼減少煤炭消耗並維持人民生活水準（如提高再生能源利用率），是台灣的重要議題之一。 民主大躍進：我很自由，不過不想參與政治 對於現在的台灣人來說，「民主」是如吃飯喝水般的基本存在。 但台灣的「民主」一直都存在嗎？要回答這個問題，我們可以看看台灣的 民主指數（Democracy Index） 發展： 您的瀏覽器不支援影片標籤，請留言通知我：S 雖然上頭的數據只到 2011 年，但我想要你看的是，1990 年（也是我出生的那年）之後，比起所得提升速度，我們的民主指數的成長速度讓人驚訝，可以說是三級跳！ 基本上近年台灣的分數變動不大。而在最新的 2017 年全球民主指數 裡頭，台灣則獲得了 7.73 分，全球排名第 33 名。（第一名為挪威，美國 21，日本 23，中國則為 139 名） 民主指數滿分為 10 分，由 5 個評量標準做平均： 選舉過程及多元程度（獲 9.58 分） 政府功能（獲 8.21 分） 政治參與（獲 6.11 分） 政治文化（僅 5.63 分） 公民自由度（ 9.12 分） 可以看到雖然我們的公民自由度很高，但政治參與以及政治文化不足。 我個人認為跟長期無意義的藍綠對抗文化以及年輕一代普遍對政壇上的政治人物冷感有關。 要讓民主成功，我們必須參與，而非只是冷眼旁觀。沒有投票的人沒有權利抱怨。 ─ 路易．路蒙，美國小說家 儘管我們的民主程度已經值得讚賞，在公民參與部分還有很多地方可以改善。 看到數據背後的故事 在上一章節，我們看了一些利用泡泡圖探索台灣以及世界的例子。 相信你也有這種錯覺：搭配著大量數據，泡泡圖彷彿讓你站在上帝的視角綜觀全球。 但我們不能就這樣停止，自我膨脹地以為彷彿透過泡泡圖裡頭的數據，就已經暸解世間萬物。 正如《真確》裡頭漢斯・羅斯林教授跟我們說的： 我要你看到統計數據背後的個別故事，也要你看到個別故事背後的統計數據。不靠數據無法了解世界，但光靠數據也無法了解世界。 舉例來說，在我查看台灣婦女在工業（Industry Sector）的勞動比例時，發現一個有趣的現象： 您的瀏覽器不支援影片標籤，請留言通知我：S 工業一般給人的印象就是包含了很多需要體力的工作，因此看到右邊台灣婦女在工業的勞動比例逐年下降（與之相對，服務業勞動比例上升）完全符合我的期待。但是，看看那個 阿爾及利亞 ！ 光看那條節節上升的曲線無法幫助我們實際了解阿爾及利亞，如同我們無法光靠數據了解世界。 說來慚愧，在觀察到這現象前，儘管小時候從歷史老師的口中聽過它，我完全沒有研究過這個國家。 阿爾及利亞,瓦赫蘭 （ 圖片來源 ） 透過一些閱讀，我現在了解阿爾及利亞（Algeria）是一個位於非洲北部的國家，1962 年從法國殖民統治下獲得獨立。因為 婦女解放 以及女權運動崛起地相對較其他伊斯蘭國家早，該國的女性在各個階級都很活躍。在勞動市場可以看到她們開大卡車、當加油站工人並穿寬大的工作服； 女性議員在議會佔的比例 在阿拉伯世界也是獨占鰲頭，最近甚至 還舉辦比基尼示威 ，來呼籲保守的社會給予女性更多自由。 在這邊不是要推薦你去阿爾及利亞觀光或是 Google 搜尋比基尼照片。 我想強調的是，讓你的好奇心跨越冷冰冰的數字。在透過數據有個宏觀的概念以後，針對你有興趣的問題去實際查查資料，問問人並了解背後的故事。 在你開始這麼做以後，會發現世界變得更遼闊，更多采多姿。 你能怎樣讓世界更好？ 看了那麼多的泡泡圖以及數據，實際上我們可以怎樣讓台灣以及世界變得更好呢？ 我相信每個人都有自己的想法，但這邊讓我給出一些拙見。 如果你是老師或是從事教育業，開始思考要怎麼利用數據來教導學生或是下一代正確的世界觀吧！不要再教他們背誦歷史年表或是生硬數字，而是利用容易理解的資料視覺化工具（如本文的泡泡圖）將過去、現在的世界展示給他們看，刺激他們的好奇心，讓他們自主發問、蒐集資料並想像未來。 如果你是從事經濟 / 社會 / 公衛 / 能源 / 政治 / 國際關係等專業領域的話，重新思考在這個世紀，我們應該要密切關注的人類發展指標吧！ 舉國民平均所得這個指標來說，我們在文章開頭看到近 200 年來全世界每個國家在國民平均所得皆有改善，但在 21 世紀只看這個就夠了嗎？ 21 世紀，我們面臨的新問題是貧富差距。 從「平均所得」這單一數字來看一個國家的經濟發展非常危險，因為這會讓我們忽視一件事情：國家的總所得實際上是怎麼分配到所有人手上的。 在 21 世紀，我們應該更關注如上圖的指標：「最富有的 10 % 人所擁有的收入份額」，來確保我們不只解決貧困，還會記得要對付社會不平等問題。 就算你認為自己不屬於上面兩種人，別擔心！我幫你列了一個自由勾選的行動清單： 分享本文以讓更多人開始探索台灣與世界 閱讀《真確》一書 查看漢斯・羅斯林 在 TED 上的演講 查看 GapMinder 官網，尤其是 Dollar Street 找出泡泡圖的翻譯錯誤並通知我（如果有的話） 加強數據科學力，學習利用數據說故事（尤其適合資料科學家） 你可以用任何方式探索世界，但如果你打算回來玩玩泡泡圖，隨時歡迎！我會持續更新數據來源並將我（和你）的新發現更新到文章裡頭。 你可以： 將 本頁網址 加入書籤，方便隨時回來查看最新的泡泡圖 用下面的按鈕訂閱部落格文章，在新文章出來的時候收到消息 最後，也是最重要的，對世界多點好奇並盡情探索吧！希望你享受我們這趟探索旅程，是時候展開你自己的冒險了：） 致謝 感謝 漢斯・羅斯林 教授，我要謝謝他帶我用更宏觀、積極的態度來理解這個世界並帶給我無數啟發。這篇文章以及文內的泡泡圖是我向他的致敬。 （漢斯・羅斯林教授已於 2017 年 2 月 7 日在瑞典烏普薩拉逝世） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/gapminder.html","loc":"https://leemeng.tw/gapminder.html"},{"title":"資料科學文摘 Vol.5 數據科學家面臨的挑戰、儀表板設計以及未來的被駭人生","text":"真正的數據科學家面臨的 8 個挑戰是什麼？ 何時一個資料科學家可以說他 / 她真正地「完成」了工作？ 10 個儀表板設計的原則是什麼？ 何謂「被駭」人生？ 為了了解這些跟資料科學息息相關的問題以及可能的解答，這週我們一樣會透過閱讀幾篇文章，來分別了解幾位優秀的資料科學家、UI/UX 設計師甚至是歷史學家是怎麼想的。如同以往的 文摘 ，針對每篇英文文章我會附上摘要並穿插自己的心得，供時間寶貴的你做參考。 事不宜遲，讓我們直接開始吧：） 本週閱讀清單 When Your Job Is Done as a Data Scientist 8 Real Challenges Data Scientists Face Data visualisation, from 1987 to today 10 rules for better dashboard design Hackable humans and digital dictators 本週想跟你分享 5 篇文章。如同以往的 文摘 ，你可以點擊任一連結，從有興趣的摘要看起。有時間的話，我則鼓勵你點擊下面各文章的標題 / 圖片來查看英文原文。 When Your Job Is Done as a Data Scientist 在一個企業裡頭，資料科學家（ D ata S cientist, DS ）常常會被各個部門（Product, Marketing, Sales Team etc）要求做各種不同的分析。如果你把每個分析視為一個專案（Project）的話，2 個你常常會需要問自己的問題是： 什麼時候可以說這個專案完成了？ 要做到什麼程度可以說我這個工作做完了？ 在這篇文章裡頭，資料科學家 Conor Dewey 說明了一個簡單的判斷原則： 如果利害關係人無法利用你的成果做出決策，則你的工作就不算完成。 如果專案的利害關係人（Stakeholders）沒有辦法利用你的分析成果做出（好的）決策，則你的工作就還沒結束。反之，當你確定自己的工作結果能夠影響企業決策後，就不需要再去鑽研一些太複雜但沒有 actionable impact 的事情上面。 如同我們在 之前的文摘 中看到的，比起建立複雜的深度學習模型，學會做一個好的簡報，並跟非技術專業的利害關係人溝通結果，進而 影響企業決策 才是對一個 DS 來說更為重要的事情。 為了產生最大的影響力，不管在做什麼分析或者專案的時候，都得要好好控管自己的時間以及專案的優先順序（Priority）。 雖然該作者在文中並沒有著墨於如何管理時間，你可以利用美國總統 艾森豪 的 時間管理準則 來決定專案的優先順序： https://jamesclear.com/eisenhower-box 你會發現，這其實就是我們從小到大在說的「輕重緩急」。 將專案依照重要性（Importance）以及緊急程度（Urgency）分為四個象限以後，你就能很清楚地知道該把自己大部分的工作時間花在那些最重要，且緊急的專案上面（上圖的左上角），藉此最大化自己的影響力。 重要的事情通常不太緊急；緊急的事情大多不太重要 ─ 艾森豪 8 Real Challenges Data Scientists Face 富比士 的這篇文章說明數據科學家在實際工作時會面臨到的 8 個挑戰。以下是我針對這些挑戰，整理出來 5 點 DS 應該時時刻刻放在心上的準則： 你得至少專精一個部門的領域專業。此部門可以是銷售、行銷、廣告或是產品部門，擇你所愛 能向非技術人才、利害關係人簡單明瞭地說明洞見以及可執行的決策，並把技術細節留到 Q&A 不要盲目地想從資料中找出什麼。先利用領域專業或者是直覺來弄出一個假設，然後利用數據驗證結果 明白一個分析的「可信度」只跟你用來做出該分析的原數據「品質」一樣高 不斷地磨練自己處理數據的技能。這通常體現在使用 Python、 R 以及 SQL 的能力 關於第 2 點，此篇文章則是這樣說明的： A data scientist that cannot articulate what their model does and why it's of value to business stakeholders is going to have a difficult path to success. 有固定在追蹤本部落格的你，想必已經非常了解清晰溝通的重要性。你也可閱讀之前的 資料科學文摘 Vol.4 來了解更多相關內容。至於第 4 點，我們則在兩篇文章中有針對資料工程以及數據品質做些著墨： 資料科學家為何需要了解資料工程 資料科學家 L 的奇幻旅程 Vol.1 新人不得不問的 2 個問題 Garbage in, garbage out。 了解企業內的資料處理流程，可以讓你合理地評估利用這些數據產生出來的分析，到底有多少價值以及可信度。 Data visualisation, from 1987 to today 在經濟學人負責資料視覺化的 Graham Douglas 分享他從 1987 年工作到現在，所使用的工具以及製圖歷程。遠在 2, 30年前，在「資料科學」這詞根本還不存在的年代，資料視覺化更像是一門藝術，而不是資料科學： Before computers, creating charts was a lot more like art than data science. 對已經習慣使用 Matplotlib 、 ggplot2 以及 Tableau 等資料視覺化工具的 DS 來說，可能很難想像製作一張折線圖，還需要自己拿尺出來畫等間距格線的時代。 雖然我們現在已經可以利用各種程式語言來輕鬆製圖，讀這篇文章能讓我們重新思考並感謝現代資料視覺化工具帶給我們的方便。我們也看到持續學習新技術以及工具的重要。 對資料視覺化或是 R 語言中的 ggplot2 有興趣的話，可以參考 淺談資料視覺化以及 ggplot2 實踐 。 10 rules for better dashboard design UX/UI 設計師的 Taras Bakusevych 提供了一些很不錯的儀表板（Dashboard）設計建議。 3 點我覺得可以特別提出來： 簡潔，想辦法把精華弄在一頁 不要太依賴互動性，要讓使用者不需什麼操作就能得到重要資訊 選擇對的視覺呈現方式來陳述你想表達的數據關係 針對第 1 點，文章是這樣說的： Don't tell the full story, instead summarize, surface only key info. 大部分儀表板的用意是要讓使用者在「幾秒鐘」之內掌握所有他需要知道的重要資訊。 為了達到這個目的，你應該仔細思考，到底該在儀表板上的有限空間裡頭（一個視窗畫面內）顯示什麼圖表。 不要因為大部分的儀表板可以無限捲動，你就一直往下加新的圖表。什麼圖表都放進去的話，很容易造成資訊過多（Information Overload）而導致使用者抓不到重點。 針對 「選擇對的視覺呈現方式來陳述你想表達的數據關係」 這點，文中則給出一個數據關係跟圖表類型的對照表： 對於一個老練的 DS，這些判斷基準應該都已經很自然地存在你腦海之中的吧！不過我覺得這很適合當做一個 reference 或者 cheatsheet 來使用，提醒自己。 Hackable humans and digital dictators 這篇文章記錄了 人類大歷史 的作者，以色列歷史學家 Yuval Noah Harari 最近在接受新書訪談： 21 世紀的 21 堂課 的內容。 你會說，為何在資料科學文摘裡頭包含了這篇文章？ 在這個一切以數據為本，「數據主義」超越「人文主義」的時代，身為一個 DS，我覺得除了注重數據分析的手法以外，作為一個有血有肉的「人」，也需要去了解數據、機器學習以及 AI 會對未來的我們以及下一代造成什麼樣的影響。這篇訪談中 Harari 用易懂的方式，以歷史學家的角度說明這件事情，值得一讀。以下是我閱讀後整理的摘要。 21 世紀人類面臨的 3 個挑戰： 核子戰爭 氣候變遷 科技破壞（Technological Disruption） 這些挑戰最難的點在於，它們並不能只靠單一一個國家解決，而是要跨國合作。 而前 2 個挑戰幾乎所有人都理解，因此或許不會發生，但最後一項挑戰（科技破壞）的影響卻不太明顯。 未來的人工智慧（ A rtifical I ntelligence, AI ）肯定會自動化掉更多人的「現有」工作。這些 AI 系統也將透過更多的 IoT 裝置來蒐集更多我們的資料（像是搜尋紀錄、身體資訊、情緒變化等），分析這些數據以後來幫我們自動做決策。 這些系統甚至最後可能會告訴我們（現在已經有些系統號稱）： 「透過大數據分析，我比你自己還懂你自己」 這就是所謂的「被駭人生」：這些利用機器學習或是人工智慧的系統能 hack 我們，透過大數據分析，在我們實際行動之前，就已經精準地預測，或者說是大幅度地直接影響我們內心、腦中的決策。你只要想像你現在在做大多數決策的時候，是比較常「聆聽自己內心的聲音」還是去「查看網站、服務、App 給你的個人推薦」就可以稍微了解這點了。 We're becoming Hackable human. 注意的是我們可不是在討論科幻小說，這邊的 AI 不會有情緒感情，只是有著龐大數據、運算能力以及複雜演算法的系統。 如果我們是這些 AI 系統的主人，AI 是為我們每個人自己的利益來服務的話很好。但看看那些大量蒐集你的數據的科技公司：一個比較可能出現的未來是，少數菁英掌握了 AI 力量，而 AI 會為了他們的利益而服務。在這樣的情況下，大多數的人類都會成為不重要的存在，等著被機器取代（如果我們什麼都不做的話）。 The most important fact anybody who is alive today needs to know about the 21 century is that we are becoming hackable animals ... If you can hack something, you can replace it. 這不是在危言聳聽，而是在討論現在的科技發展趨勢之下，可能產生的一個未來。重點是我們在了解現況以後，打算怎麼改變未來。 在找出解決方案之前，你得先了解有什麼問題。 現在還在閱讀 21 世紀的 21 堂課，希望之後能再跟你分享一些我的讀後心得。 結語 在這篇文摘裡頭，我們透過幾篇文章來了解以下幾個議題： 數據科學家的一些工作準則 最大化你的工作影響力並為專案分優先順序 幾個儀表板設計的原則 數據主義時代下的「被駭」人生 因為本文篇幅有限，我只能跟你分享閱讀這些文章以後，自己覺得最精華的一小部分。 閱讀這些文章讓我受益匪淺，因此我分享了自己的摘要，希望能幫助到沒有時間閱讀全部文章的你。儘管如此，我仍建議你從有興趣的議題開始閱讀原文或者相關文章以進一步學習。 同時非常歡迎閱讀後跟我分享你的想法，或是提供一些你覺得有幫助的相關文獻，我會很感激。 Remember we are what we read. Read those books or articles that will make you a better person ：） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-5-challenges-that-data-scientists-facing-dashboard-design-and-hackable-humans.html","loc":"https://leemeng.tw/data-science-digest-volume-5-challenges-that-data-scientists-facing-dashboard-design-and-hackable-humans.html"},{"title":"給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上）","text":"今天我們來聊聊如何將 Docker 應用在資料科學領域裡頭吧！ 全文共分上下 2 篇。在這篇裡頭，我們將透過一些簡單的比喻來直觀地理解何謂 Docker，並讓你能在閱讀本文後馬上利用 Docker 來加速你的開發效率；在下篇的內容當中，我則會分享一個資料科學家（ D ata S cientist：DS）為了解決一些數據問題而時常碰到的 3 種 Docker 使用方式。 不管是哪一篇，我們都不會深入探討 Docker 本身是以什麼技術被實現的。反之，我們將會以 DS 的角度，專注在「應用」層面：如何把 Docker 實際應用在資料科學以及資料工程領域裡頭。 這系列文章適合 2 種讀者： 對 Docker 完全沒有概念，但想讓自己的 Workflow 更有效率的資料科學家 熟悉 Docker，但好奇其在資料科學領域如何被應用的工程師 讓我們開始吧！ 雲端運算 & Docker 在解釋何謂 Docker 之前，讓我把你已經非常熟悉的雲端運算（Cloud Computing）老朋友叫出來。 Amazon Web Service（AWS） 、 Google 雲端平台（GCP） 以及 Microsoft Azure 大概是大家最耳熟能詳的幾家雲端計算 / 服務平台了。隨著時代的演進，這些平台提供越來越多樣的機器學習 API，讓開發人員不需做複雜的開發，透過一個 HTTP 要求就能直接使用各種酷炫的服務，比方說： Amazon Lex 讓你使用 Amazon Alexa 的深度學習技術建立聊天機器人 Google Cloud Vision API 讓你快速建立一個圖像辨識服務 Azure Content Moderate API 讓你自動審核網路上的圖片以及文字 儘管如此，很多時候只使用這些現成的 API 並不能滿足我們這些 DS 以及企業的野心。 比起使用現成 API，如何運用雲端運算來 scale 各種數據處理工作是一個 DS / DE 更常問的問題 除了直接用各家雲端平台提供的 API 以外，一個 DS 可能更常需要利用雲端上的計算資源來完成以下的工作： 部署一些新的分析工具來嘗試提升自己及分析團隊的效率 開發、訓練、部署並規模化（scale）自己的機器學習模型 對大量數據做批次處理，將結果儲存後顯示在儀表板上 事實上，這就是本系列文章最想要跟你分享的 3 件 DS 可以活用 Docker 來最大化產出的案例。 當我們透過這篇文章（上篇）熟悉了 Docker 的基本概念以及操作以後，就能在下篇裡頭深入地探討它們。因此在這篇先讓我們專注在學習 Docker 的基礎知識吧！ 雖然我們現在不會細談，但如果你再看一次上面的 3 個工作的話，會發現裡頭可不只包含資料科學（Data Science）。除了建置儀表板以及設計 ML 演算法以外，這裡頭還包含了不少軟體工程、資料工程甚至 DevOps 成分。當然資料工程師（ D ata E ngineer）很樂意幫助你，但如果你想要快速地自己兜出一些方法呢？你該用什麼工具？ 你可能覺得一個 DS 要在各種 deadlines 內完成以上所有的事情是不可能的。不過後面我們會慢慢發現，活用 Docker 能讓這些工作變得簡單許多。 接著就讓我們以 DS 的角度了解 Docker 到底是什麼技術。我相信閱讀接下來的文章，對你之後開發效率的提升是一個非常好的投資。 Docker：可愛的大鯨魚 首先看看以下這張 Docker 示意圖： 有什麼感覺嗎？注意到上圖包含了 3 個要素： 海洋 鯨魚 貨櫃 現在讓我們發揮點想像力。 如果你把雲端運算的平台想像成一個充滿運算資源的 大海 的話，Docker 就是如圖中在裡頭悠遊的大 鯨魚 。這隻 鯨魚 將上述所有 DS 想要做的數據處理工作、執行的 App，一個個封裝成彼此獨立的 貨櫃 ，並載著它們在這大海上運行。 Docker 提供的抽象化讓我們能輕鬆地運行任何想使用的資料科學工具、軟體而不需花費過多時間在建置底層環境。 我知道你可能還是沒什麼感覺，讓我們看下去。 鯨魚背上的貨櫃：Docker 容器 實際上這一個個假想的貨櫃就代表著 Docker 術語裡頭的容器（Container）。 「容器」顧名思義，是一個「容納」了某些東西的「器具」。 一般而言，一個容器裡通常會包含了一個完整的 App。這邊的 App 不是手機上的 App，而是指廣義的應用程式（ App lication）。 DS 常用的 App 可以是： 一個包含 TensorFlow 函式庫的 Jupyter Notebook 伺服器 一個 ML 產品，如透過已訓練的模型來判斷圖片裡頭有沒有貓咪的 Flask App 一個 SQL 查詢以及資料視覺化的工具，如 Superset 一個簡單的 Python Script，針對輸入的大量數據做處理 要從頭建構這些 App 的環境不是不可能，但除了基本的 pip install 以外你還需要花不少工夫；更令人困擾的是，很多時候你在 Mac、Windows 上安裝環境的步驟，到了雲端上的 Linux 機器上就完全行不通了。 如果這時候有人先幫我們把一個在哪邊都能跑的 App 環境建好，我們不是就能馬上開始使用各種分析工具，進行各種有趣的分析，而不用煩惱底層如不同 OS 的差異了嗎？ Docker 的容器就是這樣的一個概念，幫你事先將一個 App 所需要的所有環境，包含作業系統都「容納」在一起。 Docker 將一個 App 會使用到的程式語言函式庫（JAVA、Python、R）、資料庫、甚至作業系統（OS）都包在一個自給自足的容器（CONTAINER）裡頭。想使用某個 App 的 DS 不用從頭建置環境，只需利用 Docker 啟動該容器即可開始工作 容器裡頭不只包含 App 自己本身的程式碼，也涵蓋了所有能讓這個 App 順利執行的必要環境： App 需要的各種 Python 函式庫，如特定版本的 TensorFlow、Pandas 及 Jupyter Notebook MySQL、MongoDB 等 App 會用到的資料庫 App 會用到的各種 metadata、資料集 各種 OS 限定的驅動程式（drivers）、依賴函式庫 （把所有你想得到的東西填進來） 包羅萬象。 因此只要我們能利用 Docker 把一個 App 需要執行的環境全部包在一個容器裡頭，我們就能在任何有 Docker 的地方啟動並運行該容器。不再需要每次重新建置環境，也不用考慮不同機器上的安裝問題。 而這正是 Docker 最強大的地方： Docker - Build, Ship, and Run Any App, Anywhere 因為連 OS 都被包起來了，實際上每個容器（container）的執行環境都是自給自足的（self-contained）。 你可以把它想像成非常輕量的 虛擬機器 ，其執行結果不會因為啟動該容器的「計算環境」不同而受到影響，在任何地方（Anywhere）都能順利被執行，且執行的結果都是一樣的。 以我們前面的比喻來說的話，每個貨櫃（容器 / App）都是我們想要 Docker 幫我們運送（執行）的東西，而不管 Docker 這隻鯨魚（或大船）現在在哪個海洋（計算環境）裡頭，它都能使命必達。 Docker 就像艘大船，幫我們在任何海洋（計算環境）上運送我們的貨櫃（容器） 有一點值得澄清的是，就算 Docker 幫我們抽象化建置一個 App 環境的工作，在執行一個容器的時候，我們還是需要實際的計算資源來跑這些容器。 因此前面所謂的「計算環境」指的是一個擁有計算資源（CPU、GPU、記憶體 etc）且我們實際運行 Docker 的地方。這計算環境可以是任何一家雲端服務平台上的機器，如 AWS 的某台 EC2 機器 、 GCP 上一個包含數千台機器的群集（Cluster），或是你現在用來看本文的筆電。只要 Docker 能在該計算環境下運行，它就能幫我們在該環境「之上」執行任何容器。 簡單來說： Docker 幫我們抽象化在任何 OS 上建置環境的工作。只要給 Docker 一個容器，它就能在任何地方啟動該容器以供你使用。 現在你對 Docker 以及容器概念有個高層次的理解了，讓我們來看看這些 Docker 容器實際上是怎麼來的吧！ 貨櫃（Docker 容器）從哪來 在了解 Docker 這隻大鯨魚能幫我們運行任意的容器 / App 以後，你腦中浮現的第一個問題應該是： 這些容器（貨櫃）最初是怎麼被產生的？ 非常好的一個問題。 事實上，要產生一個新的 Docker 容器，Docker 需要一份「環境安裝步驟書」來讓它幫我們自動地建置容器內的環境，比方說使用什麼 OS，用什麼版本的 TensorFlow 等等。這份步驟書在 Docker 的世界裡被稱作 Dockerfile 。 舉個例子，以下是 Tensorflow 官方釋出的一個 Dockerfile （截錄重要部分）： FROM ubuntu:16.04 ... RUN pip --no-cache-dir install \\ ipykernel \\ jupyter \\ numpy \\ pandas \\ sklearn \\ && \\ python -m ipykernel.kernelspec ... # Install TensorFlow CPU version from central repo RUN pip --no-cache-dir install \\ http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.0.0-cp27-none-linux_x86_64.whl ... CMD [ \"/run_jupyter.sh\" , \"--allow-root\" ] 除了 RUN 、 CMD 等 Docker 專屬的關鍵字以後，你會發現這份 Dockfile 裡頭的指令其實跟你平常在本地開發時也會使用的指令如 pip install 沒有相差太多。差別在於透過第一行的 FROM ubuntu:16.04 指令，我們要求 Docker 在這個容器裡頭建置一個 Ubuntu OS 後，在其之上安裝這些函式庫。 追求規模性：Docker 映像檔的誕生 聽完以上的解釋，你可能會覺得在我們每次要啟動一個新的容器的時候，Docker 就得拿出 Dockerfile，一步步建置該容器的環境。 這樣的實作也不是不行，但很沒有效率。為什麼？ 其中一個考量是可擴展性（Scalability）。 有時你會想要用同一份 Dockerfile 在短時間內迅速地產生好幾個一模一樣的容器(s)： 用多個相同的機器學習模型，同時對大量的新數據做批次預測 使用多個相同的 Python Script 來處理大量數據 這時候與其在每次要啟動新的容器時才拿出 Dockerfile 建置環境，Docker 可以事先用這個 Dockerfile 把建置環境所需的步驟先做好一遍，然後把該環境「拍張照」，存成一個 Docker 映像檔（image）後等待之後的使用。 等你決定要開始使用容器的時候，因為我們已經有一個環境的快照（Snapshot），Docker 就能利用該映像檔，快速地啟動 1 個（或 100 個）相同的容器給你。 Docker 三元素：Dockerfile、Docker 映像檔以及 Docker 容器 （ 圖片來源 ） 到了這邊，我們已經了解 Docker 最基本也是最重要的概念： Docker 利用 Dockerfile 預先建置好一個 Docker 映像檔。在使用者想要使用容器的時候，以該映像檔為基礎，運行一個對應的 Docker 容器 坐而言不如起而行。 在掌握了這些概念以後，我相信你也迫不及待地想要開始使用 Docker 了，接下來就讓我們實際操作 Docker 來體會一下它的威力。 Docker 映像檔：法式千層酥 不管是 Windows 或是 Mac 用戶，你都可以很輕鬆地在 官方網站 下載 Docker 並安裝。 下載完以後啟動 Docker，大鯨魚就會在你的筆電上開始閒晃，等待你的指示。一般而言，我們會在 terminal 使用各種 docker 指令來跟大鯨魚溝通。 當 Docker 就緒以後，依照我們前面的所學，你會需要一個 Dockerfile 或是 Docker 映像檔來產生一個 Docker 容器。就像 Github 是一個被大家拿來分享程式碼的地方， Dockerhub 則被用來分享 Dockerfile 以及 Docker 映像檔。 假設我們現在要開始一個新的 TensorFlow 專案，並且想透過 Jupyter Notebook 進行開發，最省力的方式就是從 Dockerhub 下載一個 TensorFlow 官方 幫我們弄好的 Docker 映像檔。 讓我們打開一個 terminal 並輸入 docker pull 指令： docker pull tensorflow/tensorflow 第 1 個 tensorflow 代表 Tensorflow 的官方 Dockerhub repository，就跟 Github repository 的概念相同；第 2 個則是容器名稱。 你會看到當 Docker 在下載映像檔的時候，同時也在建置環境，而其環境會分成一層一層（Layer）的： $ docker pull tensorflow/tensorflow Using default tag: latest latest: Pulling from tensorflow/tensorflow 3b37166ec614: Already exists ba077e1ddb3a: Already exists 34c83d2bc656: Already exists 84b69b6e4743: Already exists 0f72e97e1f61: Already exists 6086c6484ab2: Pull complete 25817b9e5842: Pull complete 5252e5633f1c: Pull complete 8de57ae4ad7d: Pull complete 4b7717108c3b: Pull complete b65e9e47e80a: Pull complete 006d31e013ea: Pull complete 700521cc53f3: Pull complete Digest: sha256:f45d87bd473bf999241afe444748a2d3a9be24f8d736a808277b4f3e32159566 Status: Downloaded newer image for tensorflow/tensorflow:latest 我們不會細談 Docker 實作細節，但你可以想像 Docker 映像檔是一個法式千層酥（Mille Feuille）。 這時候的 Docker 是一名蛋糕師傅，利用 Dockerfile 作為食譜，逐行執行裡頭的指令以建立一層層的環境。每做出一層新的環境，就把它加在目前所有環境的上面，最後成為一個 Docker 映像檔。 這樣做有 2 個好處： 當你對 Dockerfile 做變動的時候，Docker 可以只針對被改變的那一層環境做修改，而不用重建每一層，減少建置環境所需要的時間 有利用到一樣環境的不同映像檔可以分享部分結果（如上面的 Already exists ） 一個 Docker 映像檔就像是蛋糕師傅利用 Dockerfile 食譜做出來的法式千層酥（誠摯地希望你不是晚上看本文，餓了） 依照你的網路速度，下載映像檔所需的時間可能有所不同。 在下載完成以後，輸入 docker images 指令可以顯示所有目前本地端擁有的 Docker 映像檔： $ docker images tensorflow/tensorflow REPOSITORY TAG IMAGE ID CREATED SIZE tensorflow/tensorflow latest 76fb62c3cb89 2 weeks ago 1 .23GB 這邊因為我的環境裡已經有一大堆的映像檔，我在 docker images 後面加入額外的篩選器來告訴 Docker 只顯示 tensorflow repository 裡頭的 tensorflow 容器。 有了映像檔以後，最令人期待的時刻終於來臨了！ 我們現在要呼叫 Docker 幫我們從這個映像檔產生並執行（run）一個新的 Docker 容器： docker run -it -p 1234 :8888 tensorflow/tensorflow 短短一行指令，包含了 3 個你不可不知的重要概念： 利用 docker run 來告訴 Docker 我們要利用 tensorflow/tensorflow 映像檔來運行一個容器。實際上 Docker 容器就是在 Docker 映像檔的環境之上再加 1 層可執行的環境供你使用（貫徹千層酥的理念） 利用 -it 參數來告訴 Docker 我們同時要建立一個互動式的 TTY 連線，讓容器內的結果直接顯示在我們的 terminal 裡頭，彷彿我們在本地環境下執行該 App 一樣。我們之後還可以直接在 terminal 使用 Ctrl + C 或 Command + C 來終止容器 利用 -p 1234:8888 告訴 Docker 我們將會透過本地端的 1234 port 來連到容器裡頭的 8888 port 你可以透過 docker run --help 來查看所有 docker run 可以使用的參數。 另外，一個 DS 應該都知道， 8888 是 Jupyter Notebook 預設的 port。因此我們的企圖就跟司馬昭之心一樣，打算透過本地端的 1234 port 連到在容器裡頭跑的 Jupyter Notebook。 現在打開你的瀏覽器並輸入 localhost:1234 ，應該就能連到容器內部的 Jupyter Notebook 伺服器： 容器內的 Juypter Notebook 畫面，所有環境包含 TensorFlow 都已經幫你設置好，只要輸入你在啟動容器的 terminal 裡看到的 token 就能通過認證 對你沒看錯，你已經用 Docker 建置了一個完整的資料科學環境，裡頭有 TensorFlow 以及 Jupyter Notebook。 而你只需要 2 個指令： docker pull tensorflow/tensorflow docker run -it -p 1234 :8888 tensorflow/tensorflow 建置環境什麼的交給 Docker 吧，你已經能馬上開始實作機器學習模型了。 有些 DS 可能會覺得他的 Anaconda 或者是 pip 功能爐火純青，不需要用到 Docker 也能自己在本地建出這樣的環境。其實沒錯，如果你只是開發個人專案，說真的不學 Docker 也沒關係（喂！） 但就如我們在下篇會看到的，當你在開發企業等級的數據處理工作、機器學習模型的時候，你可不能永遠躲在你的本地環境裡頭。當你習慣於在不透過 Docker 的情況下在本機建置環境，等到要在各種雲端平台上的機器重現你的結果的時候，你就會發現不妙了。 利用 Docker 分享你的成果 為了加強你使用 Docker 的動機，讓我再給個例子。 有持續關注我文章的讀者會發現，我在 資料科學文摘 Vol.3 Pandas、Docker 以及數據時代的反思 裡頭有提到，Docker 除了讓我們免除建置環境的痛苦以外，也能讓我們與他人簡單地分享開發結果。 Cat Recognizer 是我用 TensorFlow 以及 Flask 實作的一個非常 naive 的貓咪辨識 App。 如同我們前面所說的，我事先將所有此 App 需要的環境用一個 Dockerfile 定義、全部包在一個 Docker 映像檔後分享在 Docker Hub 上面。 任何想要使用此 App 的人，只需要利用 Docker 輸入兩行指令： docker pull leemeng/cat docker run -it -p 2468 :5000 leemeng/cat 接著他們就能在瀏覽器輸入 localhost:2468 來看到我的 App： Docker 讓你與其他人分享成果，不須額外做一大堆環境設定 當然這個 ML App 在預測能力以及 UI 上都不完美，但這邊重點是你能利用 Docker 與他人快速地分享成果。如果你有想到其他利用 Docker 封裝好的 ML App 例子（或者是你接下來打算做一個自己的），非常歡迎留言讓我知道它們的存在：） 總結 呼！看完本文以後，相信你現在應該對 Docker 有個非常清楚的認識了： Docker 是一個能幫我們在各種不同 OS 上建置開發環境的工具 Docker 三元素包含 Dockerfile、Docker 映像檔（Image）以及 Docker 容器（Container） Docker 利用 Dockerfile 預先建置好一個 Docker 映像檔。在使用者想要使用容器的時候，以該映像檔為基礎，運行一個對應的 Docker 容器 Docker Hub 上有各式各樣可以直接供使用的映像檔 你只需要 docker pull 及 docker run 就能開始一個分析專案 給自己鼓鼓掌！ 現在這張 Docker 的示意圖在你眼裡應該變得平易近人許多 正因為我們是資料科學家，利用 Docker能幫我們抽象化很多不必要的環境建置工作，加速我們的開發效率。 在本系列文章的下篇出爐之前，我鼓勵你先 下載 Docker ，並開始在 Docker Hub 或者 Google 搜尋一些你感興趣的映像檔，甚至自己寫一個 Dockerfile 將你目前的專案打包起來跟別人分享。 雖然我們這篇因為篇幅關係沒有細講，但只要有一個 Dockerfile，你就能使用 docker build 來輕鬆建立一個自給自足的 Docker 映像檔。一個 Dockerfile 也不難寫，像是上面貓咪的 App 的 Dockerfile 也不過就如此幾行： FROM python:3.6.3 MAINTAINER Meng Lee \"b98705001@gmail.com\" COPY ./requirements.txt /app/requirements.txt WORKDIR /app RUN pip install -r requirements.txt COPY . /app ENTRYPOINT [ \"python3\" ] CMD [ \"app.py\" ] 在本篇裡頭我們都是在自己的機器上使用 Docker。在下篇，我們將利用本篇學到的 Docker 知識，將其運用在浩瀚無垠的雲端平台之上，去最大化我們的影響力。 在那之前你可以先熟悉熟悉 Docker，下次遇到你的 DS 同事時，可以問問他/她： 嘿！你的 Docker Image 呢？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/3-ways-you-can-leverage-the-power-of-docker-in-data-science-part-1-learn-the-basic.html","loc":"https://leemeng.tw/3-ways-you-can-leverage-the-power-of-docker-in-data-science-part-1-learn-the-basic.html"},{"title":"資料科學文摘 Vol.4 數據科學 MMORPG 上線！你，選好自己的角色了嗎？","text":"如同以往，這篇文摘會介紹幾篇最近作者閱讀的文章以及其摘要。 不過這次在條列式列出文章以前，我想先跟你分享身為一個資料科學家（ D ata S cientist，DS），我在閱讀這些文章後得到的一些想法。 與其說是想法，應該說是「針對資料科學家這個職業，自己感受到的一些發展趨勢以及對這個職業接下來數年的職涯預測」。對於那些只有 3 分鐘可以閱讀此文的你，這些想法可以歸納成以下幾點： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 如同多人線上角色扮演遊戲，在後數據時代，萬能、什麼數據工作都會的「資料科學家」這個幻想已在式微。取而代之的是各個對相關領域專精的「數據」職業角色們：商業分析師、資料工程師、機器學習工程師、AI 研究者等 （圖片來源： 線上遊戲：暗黑破壞神 3 的角色一覽 ） 接下來我將會列出本週的閱讀清單，並在簡單說明各篇摘要的同時，一一描述它們是如何跟上述幾點概念互相呼應。最重要的，我們將探討在這個什麼職業都跟數據扯上關係的年代，你要如何在「全球數據科學 MMORPG」裡頭，找出自己的定位以及角色。 這篇文章適合對資料科學領域有興趣，或是未來想從事數據相關工作的你。放心，以文章長度來說，保證比上一篇文章：「 一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載 」要來得平易近人許多。 讓我們開始本週的閱讀之旅吧！ 本週閱讀清單 One Data Science Job Doesn't Fit All The Death of the Data Scientist How to be a bad data scientist! Beyond Interactive: Notebook Innovation at Netflix What Data Scientists Really Do, According to 35 Data Scientists 如同以往的 文摘 ，你可以從任意一篇開始看我寫的摘要。不過建議先把所有標題掃過一遍，感受一下我們接下來要談的話題。 另外如果真的很趕時間，可以直接 跳到文章最後 看我給你的建議。 One Data Science Job Doesn't Fit All 在這篇文章中，Airbnb 解釋他們如何在經過多年發展資料科學以後，將資料科學家分為三個路線（Tracks）： 分析路線（Analytics） 演算法路線（Algorithms） 推論路線（Inference） 會這樣做的其中一個很大原因是因為「資料科學」包含的領域太廣，不像這樣細分的話，第一，DS 們不知道自己該注重在什麼方面的知識；第二，公司內部跟某個 DS 合作的團隊也不知道他的專精以及該怎麼期待他的能力。 Airbnb 經過多年經驗，將資料科學家細分為三個路線，主要就是為了讓每個 DS 能專注在對的地方 其實想想很自然。就像是現在我們很習慣將工程師粗淺地分為前端（Frontend）和後端（Backend），未來的資料科學家也有很大機會依照個人的專精以及企業需求來細分路線。要不現在你想知道一家公司對 DS 的定義，還得親自去問裡頭的資料科學家到底在做什麼，且十家公司的 DS 可能會給你 9 種答案。 理想上一個資料科學家是通才（Generalist），三個路線的專業都大致了解。儘管如此，學海無涯。一個建議是至少找出哪個路線你有興趣，去專精它，並尋找渴望你專業的企業。 這呼應到我們最前面提到的第 4 項趨勢（也是最重要的一項）： 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 將這些路線想像成角色扮演遊戲（RPG）中的角色就對了！順帶一提，作者自己想專注在演算法路線，輔修分析路線，你呢？ 另外這篇沒提到跟資料科學密切相關的資料工程（Data Engineering），個人臆測是因為 Airbnb 的資料平台本身建得夠齊全，有很專業的資料工程師在幫 DS 完成這些事情。 The Death of the Data Scientist 「資料科學家的滅亡」。 非常聳動的標題，而且你可以從封面圖片看出作者想要表達 DS 會像恐龍一樣滅絕。 不過基本上我是認同的。 不是說 DS 不再重要，而是再過幾年，就像當年的「大數據」風潮，各企業或許不會再像現在一窩蜂地招聘大量的「資料科學家」，而是各行各業的每個人都能很自然地將資料科學應用在自己的工作裡頭。 如同我們在 揭開資料科學的神秘面紗 一文提到的一樣，在數據驅動的時代之下，培養「資料科學力」將不再只是資料科學家的專利；就算你不是資料科學家，也應該加入這個行業。 這呼應到我們最前面提到的第 3 個趨勢： 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 How to be a bad data scientist! 這篇說明了一般人在學習資料科學時會有的一些錯誤思維，我們應該隨時警惕自己並改善學習態度。 我自己歸納一下新手 DS 常會遇到的迷思或困境： 缺乏持續學習的動力：剛開始你可能因為資料科學很夯，薪水很高決定成為一個 DS。但資料科學領域的最大特色是變動很快。缺乏熱情或是單純跟隨潮流的人，如果沒有持續學習的動力可能會中途開始懷疑人生 誤以為了解全世界：上了幾門線上課程或是參加過 Kaggle 競賽，利用乾淨的資料在 Jupyter Notebook 上建立一個 XGboost 模型就誤以為掌握了所有的資料科學。事實上，業界的 DS 需要做更多事情，如清理資料、建立可靠的資料管道以及與其他部門溝通協調等等。雖然本文沒辦法教你怎麼做良好溝通，想多了解資料工程的話可以參考 資料科學家為何需要了解資料工程 為了學而學，沒有思考如何應用所學：這點甚至稍微資深的 DS 都會遺忘。你最少要嘗試將平常閱讀的文章、學到的分析手法應用在解決工作上的問題。甚至更好的是，改善自己或者周遭人們的問題 這篇並不直接跟本篇主題相關，不過值得 DS 們參考。 Beyond Interactive: Notebook Innovation at Netflix 平常有在關注 Jupyter Notebook 的 DS 們想必都注意到 Netflix 這篇文章了吧。 Netflix 的資料平台（Data Platform）團隊發現，儘管企業內部有各式各樣使用該平台的使用者（如 DS、資料工程師以及分析人員等），並且表面上看來都在使用不同的程式語言，不同的工具，但事實上這些平常在處理數據的人的工作流程（Workflow）大多都可以分為這幾個步驟： 存取資料 資料處理 資料視覺化 排程以及產品化（Productization） Netflix：不同數據專業的人使用很不一樣的工具以及程式語言，但其實宏觀來看，處理數據的工作流程都很類似 在明白這點以後，Netflix 的資料平台團隊展示了他們如何利用 Notebook 的「介面跟計算分離」這個特性，開發出能讓所有分析人員使用的統一介面。 在 Netflix 裡頭，任何一個 DS / DE 都可以利用一個簡單的 Notebook 介面做到： 存取 Netflix 裡頭所有的數據：內部有專門的團隊維護一個可以存取所有資料的 Python 函式庫 參數化 Notebook：一個 Notebook 可以變成一個模板（Template），讓使用者可以每次利用不同參數重新執行類似的數據處理 排程（Scheduling）。當使用者決定為目前 Notebook 規劃排程後，該平台會將當下使用者的 Notebook 存到 AWS S3 變成排程工作的參數設定，並在實際排程時建立輸出用的 Notebook，將所有 Logs 以及輸出都放在該輸出用的 Notebook 裡面，方便之後查看以及除錯。這最小化了一個 DS 建立 ETL 工作的時間以及人力成本。 這篇因為篇幅關係不會進一步解釋，但就算你平常沒在用 Notebook，應該也能感受到 Netflix 的數據平台團隊為了支援每天能在 100 PB 的數據量上跑的 15 萬個處理工作（Job）所做出的努力吧！ 抽象化（Abstraction）是對付複雜性（Complexity）最好的解藥。 這個例子我們看到，Netflix 為了提高他們內部資料科學家的效率以及規模性（Scalability），做了一個這樣的數據平台，將所有基本的資料工程，甚至是對一個正常的 DS 來說需要花不少時間熟悉的數據處理流程都自動化 / 抽象化了。 在全球數據量仍然爆炸性成長的今天，這樣的抽象化只會越來越普遍地出現在各個企業裡頭，而這對一個資料科學家來說當然是好事。再過一陣子，一個一般的 DS 或許也就不用再花所謂的 80 % 時間來做數據清理、建構資料管道等瑣事上，而是能有更多的時間在建構預測模型、進行複雜分析等更高層次的工作。 現在我們已經有各種開源的自動化工具，幫我們快速地將機器學習產品化（如 Amazon 的 SageMaker ）、自動化清理數據的工具（如 Google 的 CLOUD DATAPREP ）等等。一方面 DS 要慶幸這些事情可以被自動化，一方面則要努力學習新知，不能停滯不前。 這呼應到我們前面提到的第 1 點趨勢： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 儘管我們並不都在有這些平台的企業工作，了解自己企業的現有狀況，盡可能將能夠自動化的「數據處理瑣事」抽象化，能讓一個 DS 提高自己的效率以及工作價值。 What Data Scientists Really Do, According to 35 Data Scientists 如果只有閱讀一篇原文的時間的話，我推薦你這篇哈佛商業評論的文章。 這篇透過訪談多位資料科學家的工作經驗，讓我們能好好地思考「資料科學家」這個職業的未來走向。 首先，根據這些資料科學家所說，（事實上我也這麼認為）一個 DS 並不像有些人想像的，整天在研究 AI 演算法。 實際上，這些 DS 在做的是： 資料搜集、資料清理 統計推論（Statistical Inference） 建立儀表板（Dashboard）或是績效報告 實作機器學習以及資料處理管道（Date Pipeline） 跟決策者辯論，影響企業決策 跟專案的利害關係人說明分析結果 從這篇文章，我們也再次觀察到同樣的趨勢：現在的資料科學家的工作範圍以及被期待的技能樹過於廣泛，未來將會再進一步細分。其專業領域的細分的方式則可能依企業不同而異，像是前面提到 Airbnb 的 DS 的三個路線；或是此篇文章內提到的 Type A、Type B 的資料科學家；或是更廣泛地分為資料科學家、資料工程師以及機器學習工程師。 我們同時也從這些專業的資料科學家的口中再度認識到溝通能力的重要。 比起建立複雜的深度學習模型，學會做一個好的簡報，並跟非技術專業的利害關係人溝通結果，進而影響企業決策才是對一個 DS 來說更為重要的事情。 人類大歷史 的作者 哈拉瑞 Yuval Noah Harari 最近也在 訪談 中提到未來 AI 時代裡頭，人類 4 個最重要的技能 4C： 批判性思考（Critical Thinking） 合作能力（Collaboration） 創造能力（Creativity） 溝通能力（Communication） 這呼應到我們最前面的第 2 點的發現： 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 結語 在這篇文摘裡頭，我們透過閱讀不少跟資料科學家相關的文章，了解到了幾個 DS 的職涯趨勢： 資料科學家未來將能花更多時間在從事「更高層次」的工作，但同時也需具備更專業的能力 學習程式語言及分析工具很重要，但是對資料科學家來說，溝通能力以及領域專業順位第一 資料科學家這個職業終將式微或消失，不只 IT 產業，未來（現在）各行各業都會有善用數據的人才 跟資料科學領域相關的工作會依照專業越分越細，最終成為各式各樣的數據職業 這些都是不錯的發現，但如果你只能記住其中一個的話，我希望是最後一個。 如同我們在 Airbnb、Netflix 的例子以及多名專業的 DS 口中可以觀察到這個現象： 在不久的將來，非常有可能各個企業都依照分析領域的不同，再度細分一個 DS 的工作，並將其分為不同的路線，或是直接產生新的職業。 要我打個比方的話，就是像真實世界的 RPG 一樣。 在急著成為一個資料科學家之前，仔細思考數據科學領域裡頭，究竟什麼地方吸引你？ 你是喜歡做統計分析、執行 AB 測試來提供產品改善的洞見嗎？ 還是你熱衷於研究機器學習演算法，想辦法利用龐大數據改善企業的數據產品（Data Product）呢？ 或者你對建構能夠處理大規模資料的數據平台的工作感興趣呢？ 現在就開始思考你想要開的門、走的路線、想要成為的角色是什麼，專精它，並尋找渴望你專業的企業 不管你的答案是什麼，既然我們在玩 MMORPG 遊戲（好吧，可能只有我玩）的時候都會去認真地理解每個職業的優缺點、技能樹等等，為何不將各種數據職業視為一個個的 RPG 角色，了解自己的興趣以及跟這些職業的適合程度呢？ 玩遊戲很嗨，能把規劃數據相關的職涯當做遊戲來玩更嗨。 最後，讓我把文章開頭所問的問題交給你思考並回答： 數據科學 MMORPG 全球玩家齊聚上線。你，選好自己的角色了嗎？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-4-choose-your-own-character-in-data-science-role-play-game.html","loc":"https://leemeng.tw/data-science-digest-volume-4-choose-your-own-character-in-data-science-role-play-game.html"},{"title":"一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載","text":"這是一篇當初我在入門資料工程以及 Airflow 時希望有人能為我寫好的文章。 Airflow 是一個從 Airbnb 誕生並開源，以 Python 寫成的 工作流程管理系統（Workflow Management System） ，也是 各大企業 的資料工程環節中不可或缺的利器之一。 近年不管是資料科學家、資料工程師還是任何需要處理數據的軟體工程師，Airflow 都是他們用來建構可靠的 ETL 以及定期處理批量資料的首選之一。（事實上在 SmartNews ，除了 DS/DE，會使用 Airflow 的軟體工程師也不在少數） 在「資料科學家 L 的奇幻旅程(1)：新人不得不問的 2 個問題」一文提到 SmartNews 如何利用 Airflow 建立資料管道並管理各種 ETL （ 圖片來源 ） 儘管它的方便以及強大，在完全熟悉 Airflow 之前，因為有些專業術語以及資料工程概念的存在，不少初學者（包含當時的我）在剛開始的時候容易四處撞壁。另外如果一開始就以 ETL 當作 Airflow 的入門的話，未免難度過高且缺少共鳴。 追連載：一個 Airflow 的輕鬆使用案例 這篇文章希望以一個簡易的漫畫連載通知 App 作為引子，讓完全沒有資料工程經驗的讀者也能夠透過這個 App 的例子，輕鬆地理解工作流程的概念、自動化排程以及 Airflow 的使用方式。閱讀完本文，你將對 Airflow 以及自動排程工作有更深的理解，並學會如何建立多個能在 Airflow 上穩定運行的工作流程。更重要的，我相信你能利用這些學到的基礎，開始自動化自己生活中以及企業的數據處理 pipeline。 如果你對資料工程有興趣，不太熟悉如 Airflow 這種工作流程管理系統，但有基本的 Python 程式基礎的話（或是純粹對用 Python 寫一個漫畫連載通知 App 有興趣），我相信這篇文章應該會很適合你。 Slack 截圖：追漫畫應該要是件輕鬆的事情。我們將利用 Airflow 來實作一個像這樣會每天從 Slack 推送最新漫畫連載的 App 想重新複習 ETL 概念的讀者可以參考先前的文章： 資料科學家為何需要了解資料工程 。 章節傳送門 了解需求：所以為何要這 App ？ 工作流概念 & Airflow Python 實作 & Airflow 操作 建置 Airflow 環境 Airflow 基本概念 App 版本一：大鍋炒 App 版本二：模組化 App 版本三：填填樂 結語 為讓讀者完整了解開發這個 App 的背景脈絡、此 App 的執行邏輯以及使用 Airflow 來定期執行 App 的原因，在我們實際開始寫 Python 之前有兩小節的解說。 如果你已經有 Airflow 及工作流程的基礎知識，且迫不及待想看 Python 程式碼，可以直接跳到 Python 實作 & Airflow 操作 章節之後再回來查看前面段落。 這篇文章章節不少，你有時可能會需要回到前面章節回顧一些內容。活用左側放大鏡按鈕下面的章節傳送門能讓你更輕鬆地徜徉在本文的 Airflow 世界 所以為何要這 App ？ 平常有在網路上追漫畫連載的讀者們應該都了解，市面上的漫畫網站通常都不是會員制的。更不用說「在新連載出的時候自動通知您！」這種推送功能（Push Notification）了。也因為這樣，導致我常常三不五時上去這些漫畫網站，看每個關注的漫畫到底出了最新一話了沒。可想而知，答案通常是否定的。（一週出一次每天檢查也沒用啊啊啊） 如果你只看海賊王一個漫畫（索隆好帥！），這或許沒什麼負擔。但就像上面 Slack 截圖顯示的，我不只關注海賊王，還看很多其他漫畫。讓事情更糟的是，到最後你會發現： 不記得自己到底在追哪些漫畫 每一部漫畫最後到底是看到第幾話 上一話是什麼時候出的 有幾話是新出而你還沒看的 手動追最新連載經常讓我追到懷疑人生 追漫畫連載應該要是個輕鬆且享受的事情。在一個人人會寫 code 的時代，何不自己做個 App 幫我們自動檢查新連載呢？ 工作流概念 & Airflow 概念上我們可以把此 App 需要做的工作按照「先後順序」由上往下列出來： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 想像上述的工作清單由上往下流動，就形成了一個工作流程（Workflow）：前一個工作如寄 Slack 通知就是下一個工作：更新閱讀紀錄的上游工作（Upstream Task）。 反過來說，更新閱讀紀錄則是寄 Slack 通知的下游工作（Downstream Task）。 定義出工作之間的上下游關係的好處是什麼？ 可以讓我們確保工作之間的相依性（Dependencies）並讓如 Airflow 這種工作流程管理系統幫我們管理工作流程。一般而言，下游工作只能在上游「成功」完成之後被執行；如果上游工作失敗的話，下游工作應該被終止，通常也沒有繼續執行的意義（例：如果 App 在執行上游工作「取得使用者閱讀紀錄」時就失敗的話，不需要也不應該執行下游的「更新閱讀紀錄」工作）。 我們的 App 實際上就是一個完整的工作流程。App 從工作 A 執行到工作 B 就像是水從上游 A 流動到下游 B 一樣。 我知道你在想什麼。 屏除剛剛介紹的工作流程概念，要實作這 App 的邏輯一點都不難。事實上我們只需要寫個 Python script，把每個工作各別用一個函式（Function）實作後再按照順序呼叫它們就好（你甚至可以只用一個函式實現所有邏輯！），為何需要 Airflow？ 在你往下滑前給個提示：我們這個 App 不是每一秒鐘都在執行。 對！顯而易見的，因為這個 App / 工作流程設計的方式不是即時工作（Realtime Job），而是批次工作，執行一次以後就結束它的生命了。 我們可不希望它只在明天早上（比方說早上 9 點）去檢查新連載。我們希望它明天、下個月或是明年的今天早上都在運作。這也是為何我們需要一個像是 Airflow 的工作流程管理系統： 定期執行工作流程 維護相依性，確保工作流程從上游到下游執行，不會在上游沒完成前執行到下游 各個工作失敗時自動重試（ 墨菲定律 ，所有你認為邏輯上萬無一失的工作都會因為各種無法預期的情況給你失敗的驚喜） 簡單易懂的 Web UI 方便管理工作流程 Airflow 非常適合用來管理相依性複雜，且具批次處理性質的工作流程。 小提醒：暗色模式為方便讀者閱讀，會用與真實 AirFlow UI 不同的顏色來呈現，但概念是一模一樣的。 Airflow 的 Web UI 讓我們能更輕鬆地管理及排程工作流程（後面我們會實際利用此 UI 管理並開發 App） 事實上我們也可以透過 Linux 排程工具 Cron 來定期執行我們的 App。但 Cron 本身沒有工作流程的概念，沒辦法管理上下游工作的相依性、失敗時無法自動重跑、當然也沒有易懂的 Web UI。因此以 2, 3, 4 項的角度來看，Airflow 是一個比較好的選擇。 到此為止，我們已經了解 為何要做這個 App 此 App 的工作流程以及工作流程（Workflow）的基本概念 為何要使用 Airflow 來幫我們管理 App 的工作流程 接著只差用 Python 將 App 的邏輯以 Airflow 工作流程的方式實現了，讓我們開始實作吧！ [Warning] 接下來不只給你 Python 程式碼，而是給你大量的 Python 程式碼 Python & Airflow 實作 程式碼都會放在這個 Github Repo 裡頭供你在閱讀完文章後參考。但如果你正在用電腦瀏覽的話且想趕快熟悉 Airflow 開發的話，可以 git clone 下來以後跟著文章走。 開啟一個新的 terminal，移動到你平常放新專案的資料夾，然後輸入： git clone https://github.com/leemengtaiwan/airflow-tutorials.git cd airflow-tutorials 之後沒特別明說的話，指令都會是在 airflow-tutorials 資料夾底下執行。 建置 Airflow 環境 雖然 production 環境需要很多調整，以建構測試環境來說，基本上參考官方的 Quick Start 就可以很輕鬆地完成。因為 Airflow 是以 Python 實作的，我們可以很輕易地用 pip install 來安裝所有需要的東西。用 Anaconda 則是能讓你事後管理不同專案的環境時輕鬆不少： conda create -n airflow-tutorials python = 3 .6 -y source activate airflow-tutorials pip install \"apache-airflow[crypto, slack]\" export AIRFLOW_HOME = \" $( pwd ) \" airflow initdb 以上的指令幫我們： 建立並啟動一個新的 Anaconda 環境 在此環境下安裝 Airflow 以及 支援 Slack 功能的額外函式庫 設定專用路徑以讓 Airflow 之後知道要在哪找檔案、存 log 初始化 Airflow Metadata DB。此 DB 被用來記錄所有工作流程的執行狀況 理想上把 AIRFLOW_HOME 加入到 ~/.bash_profile 裡頭之後會比較輕鬆，不過現在不做也沒關係。 【2018/08/27 加註】如果沒有設定 export AIRFLOW_HOME=\"$(pwd)\" 就執行 airflow initdb 的話，會讓 Airflow 使用作者當初測試時使用的路徑，而不是你 git clone 下來的 repo 的路徑而造成問題，務必記得設定。 在環境都搞定之後，我們可以啟動 Airflow 的網頁伺服器： airflow webserver -p 8080 接著在瀏覽器輸入 localhost:8080 就能看到 Airflow 簡潔的 Web UI 了： Airflow Web UI 首頁：顯示所有已定義的工作流程（DAG）。圖中的 3 個 DAG 就對應到我們接下來逐漸改善 App 時產生的三個 App 版本 Airflow 基本概念 這邊值得注意的是 Airflow 利用 DAG 一詞來代表一種特殊的工作流程（Workflow）。如工作流程一樣，DAG 定義了我們有什麼工作、工作之間的執行順序以及依賴關係。DAG 的最終目標是將所有工作依照上下游關係全部執行，而不是關注個別的工作實際上是怎麼被實作的（這點在後面的 Operator 章節會有詳細解釋）。 另外從它的全名 有向無環圖（ D irected A cyclic G raph） 你可以看出它具備兩個特色：「有向」及「無環」。事實上我們的 App 邏輯就是一個理想的 DAG。首先，裡頭包含多個邏輯上的工作： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 很明顯地， App 是從上而下地執行每個工作，即為「有向」；同時 App 不會在更新閱讀紀錄以後（下游工作），還跑回去漫畫網站看有沒有新的章節（上游工作）：上游會指向下游，但下游不會指回上游，此即「無環」。 有了這個理解以後，我們的目標就很明顯了：將 App 的工作流程轉換成一個能在 Airflow 上執行的 DAG，然後排程它，就能讓它每天去找新連載！ App 版本一：大鍋炒 在 Airflow 世界裡，一個 DAG 是由一個 Python script 所定義的。 以下是我們 App 的第一個版本，也是最簡單的 DAG comic_app_v1 的程式碼（ airflow-tutorials/dags 資料夾底下的 comic_app_v1.py ）： import time from datetime import datetime , timedelta from airflow import DAG from airflow.operators.python_operator import PythonOperator default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } def fn_superman (): print ( \"取得使用者的閱讀紀錄\" ) print ( \"去漫畫網站看有沒有新的章節\" ) print ( \"跟紀錄比較，有沒有新連載？\" ) # Murphy's Law accident_occur = time . time () % 2 > 1 if accident_occur : print ( \" \\n 天有不測風雲,人有旦夕禍福\" ) print ( \"工作遇到預期外狀況被中斷 \\n \" ) return new_comic_available = time . time () % 2 > 1 if new_comic_available : print ( \"寄 Slack 通知\" ) print ( \"更新閱讀紀錄\" ) else : print ( \"什麼都不幹，工作順利結束\" ) with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 為了讓你能專注在 Airflow 及 DAG 最核心的概念，讓我先用 print() 假裝我們已經在一個函式 fn_superman 裡頭實作所有工作的邏輯了。在修改完代表一個 DAG 的 Python script 後，要確保 Airflow 能正確地將其視為一個 DAG，最基本的檢查就是用 Python 直接執行該 script。 你目前的 terminal 應該正被 Airflow 的網頁伺服器所使用。如果你還沒有把 AIRFLOW_HOME 加到 ~/.bash_profile 裡頭的話，開啟一個新的 terminal，重新進入 airflow-tutorials 資料夾以後執行： source activate airflow-tutorials export AIRFLOW_HOME = \" $( pwd ) \" 這邊我們為新的 terminal 啟動 Anaconda 環境，並告訴 Airflow 在 airflow-tutorials 資料夾底下找所有它要的東西。（之後要打開新的 terminal 也要做一樣的事情） 接著我們就可以用 Python 測試 script 的正確性： python dags/comic_app_v1.py 沒有特別設定的話， Airflow 會去 AIRFLOW_HOME 路徑底下的 dags 子資料夾找 DAG，這也是為何我們在上面路徑有個 dags 。（你可以去 Repo 確定檔案的路徑。） 如果沒有任何錯誤跑出來，恭喜！Airflow 能將其視為一個正常的 DAG 並顯示在 Web UI 上。之後只要你有修改 DAG 裡頭的程式碼，都應該做這個檢查。 這個 DAG 的程式碼雖不長，卻隱含了一些非常重要的概念。 輕鬆排程 with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : ... 靠近 Script 尾端的這行實際上就定義了我們的 DAG 並將它命名為 comic_app_v1 。而此 DAG 的排程（Scheduling）設定如 'start_date': datetime(2100, 1, 1, 0, 0) 代表從西元 2100 年開始第一次執行此 DAG 每次執行之間間隔多久。 'schedule_interval': '@daily' 代表每天執行一次 'retries': 2 則允許 Airflow 在 DAG 失敗時重試 2 次 DAG 失敗後等多久後開始重試（ 'retry_delay': timedelta(minutes=1) 代表等一分鐘） 更多更多 ... 乍看之下沒什麼了不起的，就是些設定。 但如果你有自己從頭實作過資料管道的經驗或者使用過 Cron 排程 ETL，就能體會 Airflow 這樣的「Configuration as Code」有多麽的強大：你只做一些設定（Config），Airflow 就幫你自動建立可靠、失敗時會自動重試的工作流程。 按幾個按鈕就能做出可靠的工作流程，將自動化、失敗重試、相依性管理全部交給 Airflow 這些排程設定為了方便管理，一般都另外定義在 default_args 變數並放在 script 的最上面。 Operator：將實作邏輯跟 DAG 排程分離 最有趣的是我們使用 with 關鍵字來定義一個只屬於 comic_app_v1 DAG 的領域。在這裡頭我們則定義了唯一一個工作 superman_task 處理所有事情（你應該能猜到為何它被這樣命名）： with DAG ( 'comic_app_v1' , ... superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 這段程式碼用白話翻譯的話，就是說： 在 DAG comic_app_v1 裡頭，利用 PythonOperator 建立一個名為 superman_task 的工作，而實際執行這個工作的時候，呼叫 fn_superman 函式。 一個非常重要且需要你搞懂的概念是，現在說的工作（Task），是指那些實際透過程式碼宣告，在 DAG 裡頭被定義出來的工作，如 superman_task 。 前面我們提到，App 概念上本身就包含了多個工作（步驟）： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 這些是「邏輯上」的工作，而在 comic_app_v1 DAG 裡頭，為了方便說明，我們將它們全部包起來，定義成唯一一個 Airflow 工作： superman_task 。（在 App 版本二：模組化 章節裡，我們則會分別為這些「邏輯工作」建立他們自己的 Airflow 工作）。 回到 Opeartor 的話題。在 Airflow 裡頭，DAG 只知道有哪些工作以及這些工作之間的執行順序。而實際上這些工作要怎麼被完成，其實作邏輯則是由各種 Operator 負責。 你可以想像 Opeartors 就是幫我們完成特定種類工作的小幫手，像是一些常見的例子： PythonOperator 執行一個 Python 函式 BashOperator 執行 Bash 指令 S3KeySensor 監測 S3 上的檔案存不存在 SlackAPIPostOperator 送訊息給 Slack ... 要建立一個 DAG 裡的工作（Task）就是依照你想要它完成的特定目標，來選擇合適的 Operator。比方說上面的 superman_task 就是透過 PythonOperator 來執行特定的 Python 函式 fn_superman ，而該函式則把 App 裡頭所有的「邏輯工作」實作了。 PythonOperator 可以說是 Airflow 裡最基本也最強大的 Opeartors 之一。學會使用方法以後，你可以將任何你定義的 Python 函式變成一個 Airflow 工作。 基本的使用方法非常簡單，你只要指定一個可呼叫的 Python 函式給 python_callable 參數以及設定一個工作名稱（task_id）即可： superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 在後面的 Airflow 變數以及 Jinja 模板 章節，我們則會看到如何使用其他 Operator 如 SlackAPIPostOperator 來新增一個可以幫我們送 Slack 訊息的工作。 測試開發 Airflow 工作 你現在應該已經理解 DAG 本身關注的是有哪些工作以及他們的相依性，而不是各個工作的實作邏輯。（雖然在 comic_app_v1 DAG 裡頭只有一個工作所以不存在相依性問題） 我們用 python dags/comic_app_v1.py 確保 DAG 本身沒有語法問題以後，接著就是要確保裡頭每個工作（Task）的執行結果如我們預期。 在 comic_app_v1 DAG 裡頭，我們只有一個工作 superman_task （其透過一個函式 fn_superman 幫我們做所有邏輯上的工作）： def fn_superman (): print ( \"取得使用者的閱讀紀錄\" ) print ( \"去漫畫網站看有沒有新的章節\" ) print ( \"跟紀錄比較，有沒有新連載？\" ) # Murphy's Law accident_occur = time . time () % 2 > 1 if accident_occur : print ( \" \\n 天有不測風雲,人有旦夕禍福\" ) print ( \"工作遇到預期外狀況被中斷 \\n \" ) return new_comic_available = time . time () % 2 > 1 if new_comic_available : print ( \"寄 Slack 通知\" ) print ( \"更新閱讀紀錄\" ) else : print ( \"什麼都不幹，工作順利結束\" ) with DAG ( 'comic_app_v1' , default_args = default_args ) as dag : superman_task = PythonOperator ( task_id = 'superman_task' , python_callable = fn_superman ) 這樣的設計有什麼優點？ 一般來說 DAG 跟工作是一對多的關係（一個工作流程裡有多個小工作要做）：要讓一個 DAG 順利跑完，理所當然所有工作都要順利執行完畢。但 comic_app_v1 DAG 是個特例，它裡頭只有一個工作，一人吃全家飽。只要測試且確保 superman_task 工作的執行結果如我們預期，就代表 DAG comic_app_v1 能順利完成，簡單易懂！ 我們可以使用 Airflow 的 test 指令來幫我們測試這個工作： airflow test comic_app_v1 superman_task 2018 -08-18 這行指令是讓 Airflow 幫我們測試 comic_app_v1 DAG 裡頭的 superman_task 工作，並假設這個工作是在 2018-08-18 這個日期被執行。在我們的 App 例子中， superman_task 工作的執行結果基本上不會受到執行日期的影響，可以隨便你改。 但想像一個每天 24 點 0 分準備被啟動，從資料庫撈出數據並計算「當天」使用者數目的工作。其 SQL 查詢可能長這樣： SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '{execute_date}' 因為此工作的結果會受到執行日期的影響，在測試的時候，你就得仔細選擇執行日期（execute_date）。 拉回 superman_task 工作的測試。 從上面 fn_superman 函式的程式碼你可能已經注意到，我埋了個小彩蛋，每次執行都會有不同的執行結果。 幸運的話你會得到下面這種： airflow test comic_app_v1 superman_task 2018 -08-01 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 什麼都不幹，工作順利結束 喔耶！這執行結果如我們預期，可以讓 DAG 上線定期執行了！ 不過別高興得太早。多執行幾次看看。如果墨菲定律發生，你會得到失敗的結果： airflow test comic_app_v1 superman_task 2018 -08-01 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 天有不測風雲,人有旦夕禍福 工作遇到預期外狀況被中斷 假設此執行結果不是我們預期的結果，該怎麼辦？ 如果你反應夠快，會說： 「那又怎麼樣？墨菲定律不會每次發生，而且就算遇到而導致工作失敗的話， Airflow 不是會自己幫我們重試嗎？」 的確，這是我們在前面 輕鬆排程 章節提到 Airflow 的強處。畢竟我們這 App 只是在檢查最新連載，不是做什麼很複雜的運算。基本上就算 DAG 裡頭這唯一一個工作 superman_task 失敗了導致整個 DAG 要重跑，Airflow 也可以應付得來。 但問題在於，企業在運行資料管道的時候，常常需要分成很多步驟，某些步驟可能需要龐大的計算資源跟時間（像是將每天使用者使用 App 的幾億筆紀錄做匯總存入資料庫），有些則很輕量（如存取一個外部 API 取得外匯比例）。 現在假設你無視這些不同步驟的性質差異，將它們全部放在一個 fn_superman 函式裡頭並只建立一個 Airflow 工作，當該 Airflow 工作裡頭任何一個輕量的步驟失敗，Airflow 得重跑整個工作，導致所有龐大計算的步驟也得跟著重新執行，重試的時間/計算成本會大到你哭出來。 雞蛋不要放在同個籃子裡。為邏輯上獨立的工作/步驟分別建立 Airflow 工作，可以讓 Airflow 只從失敗的工作開始重新做起。 因此一個比較好的 Airflow DAG 設計模式是為我們 App 裡頭每個邏輯上獨立的工作： 取得使用者的閱讀紀錄 去漫畫網站看有沒有新的章節 跟紀錄比較，有沒有新連載？ 沒有： 什麼都不幹，結束 有： 寄 Slack 通知 更新閱讀紀錄 都分別建立如同 superman_task 的 Airflow 工作，並定義好它們之間的相依性（Dependencies）。而這將是我們下一節的重點。 題外話：你可能會納悶為何我們只測試 superman_task 工作而沒測試整個 comic_app_v1 DAG。當然「一人吃全家飽」是個理由：只要確定 DAG 裡頭唯一一個工作正確運作，我們就能保證此 DAG 沒問題。 事實上還有一個原因： airflow test 指令實際上只能用來測試單一工作，而不能測試整個 DAG。關於 DAG 的測試我們在後面的 Airflow 排程器 章節會詳細說明。 App 版本二：模組化 所以現在我們要做的改善（Refactoring）很簡單： 將 App 邏輯從 comic_app_v1 DAG 中的函式 fn_superman 中拿出來 為 App 的每個步驟分別定義一個 Python 函式 在 DAG 裡頭利用 PythonOperator 建立多個 Airflow 工作並分別呼叫這些函式 定義這些工作的執行順序 版本二的 App 完整的程式碼如下： import time from datetime import datetime , timedelta from airflow import DAG from airflow.operators.python_operator import PythonOperator , BranchPythonOperator from airflow.operators.dummy_operator import DummyOperator from airflow.operators.slack_operator import SlackAPIPostOperator default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } def process_metadata ( mode , ** context ): if mode == 'read' : print ( \"取得使用者的閱讀紀錄\" ) elif mode == 'write' : print ( \"更新閱讀紀錄\" ) def check_comic_info ( ** context ): all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'get_read_history' ) print ( \"去漫畫網站看有沒有新的章節\" ) anything_new = time . time () % 2 > 1 return anything_new , all_comic_info def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' def generate_message ( ** context ): _ , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"產生要寄給 Slack 的訊息內容並存成檔案\" ) with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : get_read_history = PythonOperator ( task_id = 'get_read_history' , python_callable = process_metadata , op_args = [ 'read' ] ) check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) update_read_history = PythonOperator ( task_id = 'update_read_history' , python_callable = process_metadata , op_args = [ 'write' ], provide_context = True ) generate_notification = PythonOperator ( task_id = 'yes_generate_notification' , python_callable = generate_message , provide_context = True ) send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = \"YOUR_SLACK_TOKEN\" , channel = '#comic-notification' , text = \"[{{ ds }}] 海賊王有新番了!\" , icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) do_nothing = DummyOperator ( task_id = 'no_do_nothing' ) # define workflow get_read_history >> check_comic_info >> decide_what_to_do decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing generate_notification >> send_notification >> update_read_history 天啊這可比 comic_app_v1 DAG 的程式碼長了不少！ 不過在你開始懷疑自己適不適合寫 Airflow DAG 之前讓我提醒你一下。就跟我們剛剛上面提到的，實際上這個 comic_app_v2 DAG 的架構從上到下也就分為三個區塊： 用 def 定義負責實作的 Python 函式（們） 在 DAG 利用各種 Operator 定義 DAG 工作（大部分是 PythonOperator ，並使用 python_callable 指定執行步驟 1 定義的函式） 定義這些 DAG 工作的執行順序（Workflow） 回頭再看一遍，有沒有清楚一點了？ 在細看 comic_app_v2 的程式碼前，先讓我們用 Airflow Web UI 研究一下這個 DAG 在做什麼： Airflow Web UI 裡頭的 Graph View 幫我們視覺化 DAG 的工作流程。 Airflow 工作寫成英文是為了方便使用 airflow test 指令測試每個工作。 儘管工作名稱都是英文，你應該不會覺得陌生。因為這就是我們 App 的邏輯： 取得使用者的閱讀紀錄（get_read_history） 去漫畫網站看有沒有新的章節（check_comic_info） 跟紀錄比較，有沒有新連載？（new_comic_available） 沒有（no_do_nothing） 有（yes_generate_notification） 寄 Slack 通知（send_notification） 更新閱讀紀錄（update_read_history） 看來這應該不是巧合：） Airflow 排程器 如同當初測試 comic_app_v1 DAG 裡頭的 superman_task 工作一樣，在我們放心讓 Airflow 幫我們排程 comic_app_v2 DAG 以前，應該分別測試裡頭所有工作，確保它們的執行結果如我們預期： airflow test comic_app_v2 get_read_history 2018 -01-01 取得使用者的閱讀紀錄 airflow test comic_app_v2 check_comic_info 2018 -01-01 跟紀錄比較，有沒有新連載？ airflow test comic_app_v2 new_comic_available 2018 -01-01 去漫畫網站看有沒有新的章節 ... 假設我們已經做完所有工作的測試，想讓 comic_app_v2 DAG 開始被 Airflow 排程，除了已經被開啟的 Airflow 網頁伺服器以外，我們需要另外開啟 Airflow 排程器（Scheduler）。 因為目前為止一直在運轉的 Airflow 網頁伺服器只負責： 顯示 DAG 資訊，如工作流程圖、各個 DAG 的運行狀況以及 Logs 讓我們輕鬆地終止/開始 DAG 排程（在有排程器的前提） 而實際要執行 DAG、分配每個工作的運算資源則需要 Airflow 排程器。Airflow 的架構圖能幫助我們理解這件事情： Airflow 架構圖：Scheduler 是實際做排程、呼叫 Worker 執行工作的傢伙；我們熟悉的 Webserver 則提供一個 Web UI 讓我們可以輕鬆檢視工作執行時產生的 Logs、DAG 的程式碼以及工作的執行結果；所有資料都被存在 Metadata Database 裡頭。 （ 圖片來源 ） 事不宜遲，讓我們啟動 Airflow 排程器吧！ 現在再打開一個 terminal，進入 airflow-tutorials 資料夾後設定環境： export AIRFLOW_HOME = \" $( pwd ) \" source activate airflow-tutorials 接著啟動排程器： airflow scheduler 到目前為止你應該有 3 個 terminals 各司其職： 用來輸入 airflow 相關指令的 terminal Airflow Webserver Airflow Scheduler 我保証不會再多了。 有了排程器以後，打開 UI，在左邊將 comic_app_v2 DAG 設成「On」後，點擊右邊「Trigger Dag」按鈕可以呼叫排程器馬上開始執行該 DAG。先讓我們按下去以後，再讓我解釋這樣做會發生什麼事情。 在左邊將 DAG 設成「On」後，可以利用右邊「Trigger Dag」按鈕呼叫排程器馬上開始執行該 DAG 為了避免預料之外的排程，Airflow 所有 DAG 的預設狀態都是暫停的（Paused），也就是上圖中如 comic_app_v1 左邊的「Off」。只有在你將 DAG 的狀態設定成如圖中的 comic_app_v2 的「On」，排程器才會開始為其做排程。 手動觸發 DAG 雖說將一個 DAG 取消暫停（Unpause）可以讓它成為 Airflow 的排程對象，實際上 Airflow 的排程又分兩種方式： 手動觸發（Manual） 常用在測試 DAG 或是有意外發生，需要手動重新執行 DAG 的時候 定期執行（Scheduled） 也就是所謂的「正式上線」。 依照 DAG 的 start_date 及 schedule_interval 設定決定何時執行 現在讓我們先專注在手動觸發。 當然你也可以在不透過 Web UI 的情況下，直接利用 terminal 取消暫停一個 DAG 並觸發它： airflow unpause comic_app_v2 airflow trigger_dag comic_app_v2 理論上我們剛剛手動觸發的 comic_app_v2 應該已經跑完了。重新整理你應該會看到 Airflow UI 顯示 DAG 已被成功執行的畫面： 從 Web UI 我們可以清楚地看到剛剛手動觸發的 comic_app_v2 DAG 已經被 Airflow 排程器拿去執行，產生一個新的 DAG Run 並成功執行。DAG 跟 DAG Run 的差異在於前者只是個定義好的工作流程，後者則是該 DAG 在某個時間點實際被排程器拿去執行（Run）過後的結果，會有一個執行日期（execute_date）。 接著點擊右邊 Links 中長得像太陽的 Graph View 按鈕後就可以看到這個 DAG Run 的執行狀況： 將游標放在右邊的「Success」狀態按鈕上可以顯示此 DAG Run 中被成功執行的工作（圖內的工作從左到右被執行）注意圖中 DAG Run 的 ID： manual_2018-08-19... 表示這是一個在 2018-08-19 被手動觸發的 DAG Run。 我們可以清楚地看到這個 DAG Run 完美地模擬了我們 App 在檢查到新連載情報時送 Slack 訊息給我們的情境。我甚至收到一個 Slack 訊息： comic_app_v2 DAG 如果發現有新連載就會寄一個罐頭 Slack 訊息，包含 DAG 執行日期。因為我是在 2018-08-19 當天手動觸發此 DAG，因此日期即為 2018-08-19。後面我們會看到如何客製化 Slack 訊息內容。 定義工作流程 要在 DAG 裡頭定義出如上圖的工作流程也非常的直觀，讓我們參考這兩個工作： yes_generate_notification send_notification 它們在 dags/comic_app_v2.py 裡頭被這樣定義（節錄最重要的部分）： with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... generate_notification = PythonOperator ( task_id = 'yes_generate_notification' , ... ) send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , ... ) # define workflow generate_notification >> send_notification >> ... 你可以發現在 comic_app_v2 DAG 裡，我們分別定義好這兩個工作以後，在最下面用 >> 語法告訴 Airflow 這兩個工作的相依性： yes_generate_notification 工作要在 send_notification 之前執行 另外眼尖的讀者會發現，Python 變數名稱 generate_notification 跟實際上的工作名稱（task_id） yes_generate_notification 並不一致。我們將實際的工作 PythonOperator 命名為 generate_notification ，只是為了後面在定義工作流程的時候好提到它。參考下面的程式碼： with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... task1 = PythonOperator ( task_id = 'yes_generate_notification' , ... ) task2 = SlackAPIPostOperator ( task_id = 'send_notification' , ... ) # define workflow task1 >> task2 >> ... 這段程式碼跟上一段程式碼在定義工作流程上有一模一樣的效果，只是後者的 naming convention 在定義工作流程的時候比較易懂。 雖然要多打幾個字，為了其他 DS/DE 以及未來的自己，一般推薦 Python 變數名稱取跟 task_id 類似的名字。 針對其他工作，我們也是用相同語法將它們串起來： ... decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) ... get_read_history >> check_comic_info >> decide_what_to_do decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing generate_notification >> send_notification >> update_read_history 然後我們就得到前面看過的工作流程圖了： 你也可以回到 App 版本二：模組化 章節，確認 comic_app_v2 完整的程式碼後再利用左邊的傳送門回來，我等你。 Airflow 變數以及 Jinja 模板 現在你應該已經了解如何使用 PythonOperator 建立一個新的工作，並利用 >> 語法定義 Airflow 的工作流程（DAG）了。我們也實際觸發 comic_app_v2 DAG 讓 Airflow 排程器幫我們排程，最後收到一個 Slack 訊息。 現在讓我們仔細研究一下負責寄 Slack 訊息的工作，也就是下圖的 send_notificiation ： 依照 Opeartor 種類不同，工作在 Web UI 上顯示的背景顏色也有所不同，方便區分。 你會發現它的顏色跟其他工作不一樣，這是因為它並不是一個 PythonOperator ，而是一個 SlackAPIPostOperator 。由此 Operator 定義的工作並不會呼叫一個 Python 函式，而是直接呼叫 Slack API 來傳送訊息。下面是我們在當初落落長的 comic_app_v2 DAG 裡頭定義的 send_notificiation ： send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = \"YOUR_SLACK_TOKEN\" , channel = '#comic-notification' , text = \"[{{ ds }}] 海賊王有新番了!\" , icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) 注意 text 參數的值。 {{ ds }} 實際上是 Jinja 語法，它允許我們將 Python 變數渲染（Render）到字串裡頭，動態地產生文本。這就像是我們有個變數 ds ，然後利用 format 語法一樣： text = \"[{ds}] 海賊王有新番了!\".format(ds=ds) 而這邊的重點是 Airflow 在執行一個 DAG 的時候會提供一些預設的 環境變數 供我們使用，像是： ds ：代表 DAG Run 的執行日期（execute_date），以 YYYY-MM-DD 形式表現 yesterday_ds ：DAG Run 的執行日期的前一天，以 YYYY-MM-DD 形式表現 tomorrow_ds ：DAG Run 的執行日期的後一天，以 YYYY-MM-DD 形式表現 ... 而因為我們在 2018-08-19 的時候，利用下面這個指令手動觸發 comic_app_v2 DAG： airflow trigger_dag comic_app_v2 Airflow 會將實際執行該 DAG 的日期設定為執行日期（execute_date）。因此 ds 即為 2018-08-19 ， SlackAPIPostOperator 裡頭的 \"[{{ ds }}] 海賊王有新番了!\" 就會被渲染成 [2018-08-19] 海賊王有新番了! 。 最後我們就得到這個 Slack 訊息： 現在你也了解使用 Jinja 語法可以動態地調整每次 DAG 運行的邏輯以及執行結果。讓我們實際將 comic_app_v2 DAG 丟上線試試看吧！ 執行日期：排程最重要的概念 經過前面的幾個章節，我們已經對 comic_app_v2 DAG 的測試及開發下了不少功夫： 利用 airflow test 指令分別測試每個 Airflow 工作執行如預期 python dags/comic_app_v2.py 確保 DAG 定義無誤 使用 Web UI 點擊「 Trigger Dag 」按鈕或是透過 airflow trigger 來手動觸發 DAG 確認結果 這些都是將一個 DAG 正式上線前必須完成的步驟。在這些測試都完成以後，是時候將我們的 comic_app_v2 DAG 交給 Airflow 排程器，讓 Airflow 幫我們每天執行這個 DAG 了！ 在 手動觸發 DAG 章節我們有看到，要讓 Airflow 排程器開始排程一個 DAG，首先要終止暫停（Unpause）該 DAG。而為何當時 Airflow 沒有在我們 comic_app_v2 一終止暫停 就開始自動排程，而要等到我們手動觸發呢？ 這是因為當時的 comic_app_v2 的排程設定如下： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 'start_date': datetime(2100, 1, 1, 0, 0) 代表我們希望 comic_app_v2 DAG 的第一個執行日期（execute_date）為西元 2100 年 1 月 1 號 0 點。 你可能覺得為何要把話說得那麼複雜，就說： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 不就好了嗎？ 不這麼說的原因，就是因為上面的理解是錯的。事實上這是很多人在利用 Airflow 排程時最容易搞錯的 概念 之一，值得花點篇幅徹底搞清楚。 假如西元 2100 年我們架的 Airflow 排程器還在運作的話，它會在： start_date 2100 年 1 月 1 號 0 點 0 分 + 1 * schedule_interval = 2100 年 1 月 1 號 0 點 0 分 + 1 * @daily = 2100 年 1 月 1 號 0 點 0 分 + 1 * 24 小時 = 2100 年 1 月 2 號 0 點 0 分 也就是 2100 年 1 月 2 號 0 點 0 分的時候，將 comic_app_v2 DAG 拿出來做第一次執行，而該 DAG Run 的執行日期為 2100 年 1 月 1 號 0 點 0 分。 我知道你現在可能滿臉黑人問號，但讓我們好好想一想這到底是怎麼一回事。 要理解為何我們一開始的猜想： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 是非常矛盾的，讓我們做個我最愛的假想實驗。還記得在 測試開發 Airflow 工作 章節提到的 SQL 查詢嗎？ SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '{execute_date}' 現在假設我們給這個工作跟 comic_app_v2 一模一樣的排程設定： 'start_date': datetime(2100, 1, 1, 0, 0) 'schedule_interval': '@daily' 根據本章節一開始的敘述，這個 DAG 的第一個執行日期（execute_date）為 2100-01-01 。而按照我們在 Airflow 變數以及 Jinja 模板 章節所說的，此 SQL 查詢裡頭的 Jinja 語法會被渲染成： SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '2100-01-01' 接著假設我們一開始的猜想： 「 Airflow 排程器會在 西元 2100 年 1 月 1 號 0 點第一次執行此 DAG 」 是對的話，該 SQL 查詢會取回什麼資料？ 答案是什麼都沒有。 因為如果這猜想是對的話，這個 SQL 查詢工作會馬上在西元 2100 年 1 月 1 號的 0 點，想辦法去把西元 2100 年 1 月 1 號整天的使用者資料全部撈出來。而因為此 SQL 查詢執行時， 1 月 1 號才剛開始，這個查詢不會取得任何資料。 很明顯哪裡出了差錯了。 而如果照我剛剛解釋的版本，就會顯得合理許多： 在 2100 年 1 月 2 號 0 點的時候，以下的 SQL 查詢會被執行 SELECT COUNT ( user_id ) AS num_new_users FROM user_activities WHERE dt = '2100-01-01' 這代表我們在 1 月 1 號 23 點 59 分結束以後，也就是 1 月 2 號 0 點的時候，將 1 月 1 號所有的使用者資料做彙總。 一般而言，Airflow 會在 start_date 加上一個 schedule_interval 之後開始 第一次執行某個 DAG ，而該 DAG Run 的 execute_date 為 start_date 。這樣的設計就是為了避免像是上面那個 SQL 查詢在當天才剛開始的時候就想要搜集該天所有資料的窘境。 Airflow 擅長的是管理那些允許「事件發生時間」跟「實際數據處理時間」有落差的批次工作。因此 Airflow 都會在 start_date 加上 schedule_interval 長度的時間過完 以後 ，才開始處理發生在 start_date 到 start_date + schedule_interval 之間的資料。 再換句話說， 一個 DAG Run 中的執行日期，只等於它「負責」的日期，不等於它實際被 Airflow 排程器執行的日期。一個被自動排程且執行日期為 dt 的 DAG Run，實際上是在 dt + schedule_period 後被 Airflow 執行。 我們換了好幾種說法，希望你能百分之百地掌握這個 Airflow 排程的概念，因為這實在太重要了。 有了這章節的排程概念以後，我們可以正式開始排程 comic_app_v2 DAG 了！ 正式排程 經過上一章節排程概念的洗禮，想必你還記得 comic_app_v2 DAG 的開始排程日期（start_date）是遙遠的西元 2100 年 1 月 1 號： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 作者目前撰寫這段落的日期為西元 2018 年 8 月 20 號，所以大概還要再等 82 年，而且我啟動的 Airflow 排程器還活著，這個 DAG 才會被第一次執行。我們可等不了那麼久。 在完全地理解上一章 執行日期：排程最重要的概念 所提到的概念以後，你可能會說： 「那我們可以把 start_date 設為 2018 年 8 月 20 號，並維持 schedule_interval 為一天，這樣等到 8 月 21 號 0 點的時候，這個 DAG 就會被執行，然後我們就知道它 work 不 work 了！」 好傢伙（好姑娘？），我給你 100 分！ 這句話已經抓到 Airflow 排程的精髓中的精髓，只不過別誤會，我趕時間。何不讓我們當個時空旅人，將 start_date 設為 8 月 20 號以前的日期，比方說 8 月 17 號？ 畢竟我們在上一章提到： 一個 DAG Run 中的執行日期，只等於它「負責」的日期，不等於它實際被 Airflow 排程器執行的日期。 將 start_date 設為今天（8 月 20 號） 以前 的日期，並啟動 Airflow 排程器的話，就會讓 Airflow 排程器馬上開始排程執行日期為 start_date 的 DAG Run，並且一直執行到最新的 DAG Run 為止。 Airflow 排程器彷彿就像台時光機器，幫我們排程那些執行日期在過去的 DAG Run，重建過去。 所以現在讓我修改 comic_app_v2 DAG 的程式碼以排程「過去」的 DAG Run： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2018 , 8 , 17 , 0 , 0 ), 'schedule_interval' : '@daily' , ... with DAG ( 'comic_app_v2' , default_args = default_args ) as dag : ... 保持好習慣，修改完程式碼以後用 Python 確認 DAG 沒語法錯誤： python dags/comic_app_v2.py 通常 Airflow 沒多久就會重新載入最新的程式碼。如果你懷疑程式碼沒有被更新，可以點擊 Airflow UI 首頁中 comic_app_v2 DAG 最右邊 Links 裡頭的「Refresh」按鈕。 問題時間。 將 comic_app_v2 DAG 的 start_date 設定成 2018 年 8 月 17 號以後，在作者撰文的 8 月 20 號晚間 10 點為止， Airflow 會排程幾次 DAG Runs？它們分別的執行日期為何？花個幾秒鐘思考，確定你知道答案。（schedule_interval 一樣為 @daily ） 滴答滴答，你能在我們的時光機完成工作之前想出答案嗎？ 答案揭曉，Airflow 排程器總共排程三個 DAG Runs，他們的執行日期分別為： 2018-08-17 2018-08-18 2018-08-19 8 月 20 號的 DAG Run 則要等到 8 月 21 號 0 點才會被執行。 喝杯水重新載入 UI，我們可以從 Airflow UI 裡頭確認 comic_app_v2 DAG 總共有 4 個成功的 DAG Runs： 除了第一個 DAG Run 是我們之前手動觸發以外（你可以從它的 Run Id 以及最右邊的 External Trigger 看出），其他三個都是 Airflow 排程器實際排程並執行的結果（一樣你可以從它們的 Run Id 看出端倪）。 同時我的 Slack 作響。我們可以看到儘管執行日期相異，三個被排程的 DAG Runs 按照順序通知我有新番。 嗯 .. 海賊王一週出一次，想必其中有幾個是 fake news。 不管如何，我們在這章節成功讓 Airflow 排程器從好幾天前開始自動排程 comic_app_v2 DAG 並確認結果成功！ 如果我不將 Airflow 排程器關掉的話，之後每天的 0 點（UTC）它都會幫我執行 comic_app_v2 DAG。沒有意外的話，或許 Airflow 排程器可以幫我們持續排程此 DAG 到西元 2100 年，希望到時海賊王已經完結，不用叫孫子燒給我了。 App 版本三：填填樂 目前為止，本文為了讓你能專注在理解 Airflow 及工作流程的核心概念（而非個別工作的實作細節），以 print() 代替我們 App 的實作邏輯。 在此章節，我們則會一窺實作所有邏輯的 comic_app_v3 DAG，也就是實現本文開頭展示的 App 的程式碼。 但為何說「一窺」呢？ 因為 comic_app_v3 DAG 為了處理 JSON 檔案、利用 Selenium 存取網頁等事情，其程式碼變得比只用 print() 的 comic_app_v2 DAG 要長得多，且其程式碼很大一部份已經不直接跟 Airflow 相關了。 我相信大部分的讀者是為了學習 Airflow 而來，而不是看我東 try 西 try 來實作這個 App。當然，如果你有興趣且想要練習如何建立一個自己的漫畫連載 App，你可以嘗試將實作邏輯填入到 comic_app_v2 DAG 裡頭的各個 Python 函式即可，或者直接執行我已經實作好所有邏輯的 comic_app_v3 DAG，這個我們在後面的 如何建立你自己的連載通知 App（懶人法） 章節會有詳細講解。 填填樂：以 comic_app_v2 建立好的工作流程為基礎，實作每個工作的邏輯就像是填空題一般，將邏輯填入對應的 Python 函式就好。（comic_app_v3 也是從 comic_app_v2 為基礎開發的，工作流程一模一樣） 在這章節，我想跟你分享一些在實作 comic_app_v3 DAG 時用到的 Airflow 知識及技巧。 重複利用 Python 函式 在 App 版本二：模組化 章節我們看到，大部分的 Airflow 工作都是由一個 PythonOperator 所定義，而每個 PythonOperator 分別呼叫不同的 Python 函式。但在 comic_app_v3 DAG 裡頭，我們只利用一個 Python 函式 process_metadata 專門負責讀 / 寫使用者的閱讀紀錄： def process_metadata ( mode , ** context ): if mode == 'read' : ... elif mode == 'write' : ... with DAG ( 'comic_app_v3' , default_args = default_args ) as dag : get_read_history = PythonOperator ( task_id = 'get_read_history' , python_callable = process_metadata , op_args = [ 'read' ], provide_context = True ) ... update_read_history = PythonOperator ( task_id = 'update_read_history' , python_callable = process_metadata , op_args = [ 'write' ], provide_context = True ) 你會發現上面兩個 Airflow 工作的 python_callable 都呼叫 process_metadata ，因為它們做類似的事情： get_read_history 負責讀取閱讀紀錄 update_read_history 負責更新閱讀紀錄 而這兩個工作則利用不同的 op_args 來使用 process_metadata 函式的不同功能。這樣的好處是我們不需要為每個類似的 PythonOperator 都分別建立一個新的 Python 函式，而是利用參數 op_args 來改變同個 Python 函式的執行結果。 當然，傳遞參數給 Python 函式這件事情本身就是很常見，這時候 op_args 就會派上用場。 Xcom：工作之間的訊息交換 Xcom（Cross Communication） 是 Airflow 工作之間交換訊息的方式。一個被 PythonOperator 呼叫的 Python 函式所回傳（return）的值，都可以被其他 Airflow 工作透過 Xcom 存取： def check_comic_info ( ** context ): print ( \"檢查有無新連載\" ) ... return anything_new , all_comic_info def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' ... with DAG ( 'comic_app_v3' , ... ... check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) 你可以看到最底下的 check_comic_info 工作呼叫上方的 check_comic_info 函式，而該函式回傳 anything_new, all_comic_info 。 接著 decide_what_to_do 函式則利用以下語法來取得該結果： anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) 下游工作可以透過這樣的方式取得上游工作的執行結果，來決定接下來要做的任務。 值得注意的是 XCom 的所有資料在 pickle 之後會被存到 Airflow 的 Metadata Database（通常是 MySQL）裡頭，因此不適合交換太大的數據（例：100 萬行的 Pandas DataFrame），而適合用在交換 Metadata。 def check_comic_info ( ** context ): ... 裡頭的 **context 的語法是為了取得 Airflow 在執行工作時產生的 環境變數 ，其中就包含 XCom。除了要在 Python 函式設置 **context 以外，我們還必須將 PythonOperator 的 provide_context 參數設置為 True ，Airflow 才會把環境變數傳給該工作： check_comic_info = PythonOperator ( task_id = 'check_comic_info' , python_callable = check_comic_info , provide_context = True ) 在工作流程內加入條件分支 有時候我們會想要在工作流程裡頭加入分支，當某條件符合的時候執行這個分支，當不符合的時候執行別的分支。 比方說我們的 App 就含有這樣的邏輯： 利用 BranchPythonOperator 實現 Airflow DAG 裡的條件分支 圖中的 check_comic_info 「上游」工作會去漫畫網頁檢查有沒有新的連載，依照結果的不同，我們希望不同分支被執行： 如果有的話，執行上面分支的 yes_generate_notification 「下游」工作 沒有的話，則執行下面分支的 no_do_nothing 「下游」工作 要在 Airflow 裡頭實現這樣的邏輯，可以在上下游工作「之間」新增一個 BranchPythonOperater （如圖中的 new_comic_available 工作）： 砍掉原上游工作跟下游工作之間的 >> 將原上游工作 >> 該 BranchPythonOperator 工作 將該 BranchPythonOperator 工作 >> 原下游工作 資料工程很大一部份的工作就是在建立資料管道/工作流程，接個水管合情合理對吧？ from airflow.operators.python_operator BranchPythonOperator def decide_what_to_do ( ** context ): anything_new , all_comic_info = context [ 'task_instance' ] . xcom_pull ( task_ids = 'check_comic_info' ) print ( \"跟紀錄比較，有沒有新連載？\" ) if anything_new : return 'yes_generate_notification' else : return 'no_do_nothing' ... with DAG ( 'comic_app_v3' , default_args = default_args ) as dag : ... decide_what_to_do = BranchPythonOperator ( task_id = 'new_comic_available' , python_callable = decide_what_to_do , provide_context = True ) generate_notification = PythonOperator ( ... ) do_nothing = DummyOperator ( task_id = 'no_do_nothing' ) decide_what_to_do >> generate_notification decide_what_to_do >> do_nothing 而 BranchPythonOperator 一樣會呼叫一個 Python 函式（上例的 decide_what_to_do 函式），由該函式決定到底最後哪個下游工作會被執行。基本上該函式會依照實際情況決定哪個下游工作被執行，並將該下游工作的 task_id 回傳。 而因為在這個例子中，我們希望依照上游工作 check_comic_info 回傳的一個布林值 anything_new 來決定要執行哪個下游工作，因此可以使用 xcom_pull 取得該結果以後回傳要執行的下游工作 ID task_id 。 好啦，這就是我想跟你分享在實作 comic_app_v3 DAG 時的幾個實用技巧，希望對你上手 Airflow 有所幫助。 接下來我們將針對那些想要建立自己的連載通知 App 的你，提供一個快速起手指南。 不過如果你現在沒有打算做這件事情的話，可以放心跳到最後面的 結語 。 如何建立你自己的連載通知 App（懶人法） 此章節提供一個懶人指南，讓那些想要建立自己的 App 的你，在（幾乎）不需要改變 comic_app_v3 程式碼的前提下完成這件事情。 如同我們在 建置 Airflow 環境 提到的，首先你當然得先把跟這篇文章相關的 Github Repo 複製下來： git clone https://github.com/leemengtaiwan/airflow-tutorials.git cd airflow-tutorials 如果你在之前就有複製 Repo 下來跟著走，你只需要再另外安裝 Selenium 。Selenium 是一個自動化網頁測試的工具，在這個 App 裡頭被我們用來當網路爬蟲，去漫畫網站看連載資訊。 接著啟動目前為止 Airflow 一直在使用的 Anaconda 環境，然後安裝 Selenium： source activate airflow-tutorials conda install -c conda-forge selenium 如果你之前沒有建置任何環境，可以利用 Repo 裡頭的 environment.yaml 從頭安裝 Airflow 以及 Selenium： conda env create -n airflow-tutorials -f environment.yaml source activate airflow-tutorials 在這個 App 裡頭，要讓 Selenium 正常運作，你還需要 Chrome Driver 。下載最新版本以後把它放在你的 $PATH 讀得到的地方。Mac 使用者的話可以放到像是 /usr/local/bin 資料夾下面。如果還是不懂可以查看 這裡的 Chrome Driver 安裝教學 。 環境設定好以後，你會需要一個新的 Slack App 來送訊息到你的 Workspace。建立一個新的 Slack App，給予它寫訊息的權限以後，安裝到你自己的 Workspace。這時候你應該會得到一個開頭為 xoxp- 的 Slack Token。將該 Token 複製下來，打開 airflow-tutorials 資料夾底下的 data/credentials/slack.json 。 將你的 Token 複製貼上如： { \"token\" : \"xoxp-.....\" } 搞定網路爬蟲以及 Slack 認證以後，你需要改變 comic_app_v3.py 裡頭的一行程式碼，以讓 Airflow 送訊息到你 Workspace 底下指定的頻道（channel）： send_notification = SlackAPIPostOperator ( task_id = 'send_notification' , token = get_token (), channel = '#comic-notification' , text = get_message_text (), icon_url = 'http://airbnb.io/img/projects/airflow3.png' ) 將上述的 channel='#comic-notification' 改成你自己的頻道，如 channel='#my-new-channel' 。 接著你會需要一個正常運作的 Airflow 排程器。啟動方法參考 Airflow 排程器 章節。 在 Airflow 排程器、Selenium 以及 Slack 都就緒以後，你可以直接手動觸發 comic_app_v3 DAG 來測試 App 的第一則訊息。如同我們在 手動觸發 DAG 章節提到的，你可以透過 Web UI 或者 terminal 來終止暫停（Unpause）並手動觸發一個 DAG： airflow unpause comic_app_v3 airflow trigger_dag comic_app_v3 一切順利的話，幾秒鐘之後，你會在自己的 Slack Workspace 及 channel 底下收到這個測試訊息： 圖中的 channel 會隨著你實際的設定改變 目前 comic_app_v3 DAG 將使用者的閱讀紀錄儲存在 data/comic.json 裡頭，底下則是為了產生上面這個 Slack 訊息的假閱讀紀錄： { \"1152\" : { \"name\" : \"海賊王\" , \"previous_chapter_num\" : 900 } } 上頭 comic.json 裡頭，海賊王的 \"1152\" 就代表該漫畫主頁在動漫狂的連結中的數字（1152.html） 目前此 App 只能從 動漫狂 （歡迎你丟 PR 改善！）找最新的漫畫連載。為了新增你自己的漫畫，你需要找出該漫畫主頁在動漫狂的連結，將連結中的數字如上述的例子一樣新增在 data/comic.json 裡頭。假設你想開始關注「進擊的巨人」，然後最近看到 100 話的話，可以把 data/comic.json 改成這樣： { \"1221\" : { \"name\" : \"進擊的巨人\" , \"previous_chapter_num\" : 100 , } } 這樣一來， comic_app_v3 DAG 就會用該數字去「進擊的巨人」的頁面，幫你查看有沒有最新的連載。當然你也可以像我一樣，在 comic.json 裡追加多個漫畫： { \"1152\" : { \"name\" : \"海賊王\" , \"previous_chapter_num\" : 911 }, \"1221\" : { \"name\" : \"進擊的巨人\" , \"previous_chapter_num\" : 107 }, \"4485\" : { \"name\" : \"西遊\" , \"previous_chapter_num\" : 152 }, \"1121\" : { \"name\" : \"浪人劍客\" , \"previous_chapter_num\" : 327 }, \"1122\" : { \"name\" : \"王者天下\" , \"previous_chapter_num\" : 565 } ... } 修改完 comic.json ，最後你會想要修改 comic_app_v3.py 裡頭的排程設定： default_args = { 'owner' : 'Meng Lee' , 'start_date' : datetime ( 2100 , 1 , 1 , 0 , 0 ), 'schedule_interval' : '@daily' , 'retries' : 2 , 'retry_delay' : timedelta ( minutes = 1 ) } 將 start_date 改成你想要他它開始的日期，接著 Airflow 排程器就會每天幫你執行 comic_app_v3 DAG 並查看最新連載。搞定收工！ 結語 首先，由衷感謝你花了那麼多寶貴時間與力氣跟隨著我們的 Airflow 冒險。 回顧一下，這一路上你已經學會不少資料工程相關的知識以及 Airflow 的開發技巧： 了解工作流程、上下游工作、相依性的概念以及其與 Airflow DAG 的關係 模組化工作流程的重要性 了解如何利用 PythonOperator 建立一個 Airflow 工作並呼叫自定義 Python 函式 利用 airflow test 指令以及 Web UI 測試 Airflow 工作以及 DAG 了解如何利用 Python 定義一個工作流程以及決定工作間的相依性 利用 Web UI 及 terminal 手動觸發 DAG 並確認執行結果 了解 Airflow 排程概念（如執行日期）並實際讓工作流程上線（ comic_app_v2 ） 了解一些 Airflow 開發時的技巧，如建立條件分支以及使用各種不同的 Operators 建立工作 先給自己鼓個掌吧！ 如同我在文章開頭所述： 這是一篇當初我在入門資料工程以及 Airflow 時希望有人能為我寫好的文章。 當時的我找不到這篇文章，而現在我自己寫了這篇文章。 希望這篇文章能幫助到跟過去的我一樣，正在嘗試學習資料工程以及 Airflow 的你。 雖然使用 Airflow 來實作本篇的漫畫連載 App 可能是一個殺雞用牛刀的例子，但我希望你能參考本文的 App 例子，開始思考如何用本文學到的知識，去實際解決、自動化你自身或是所在企業的數據問題。 儘管這篇的 Airflow 故事即將進入尾聲，你的 Airflow 之旅才剛剛展開。 Keep learning and happy Airflowing！ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/a-story-about-airflow-and-data-engineering-using-how-to-use-python-to-catch-up-with-latest-comics-as-an-example.html","loc":"https://leemeng.tw/a-story-about-airflow-and-data-engineering-using-how-to-use-python-to-catch-up-with-latest-comics-as-an-example.html"},{"title":"資料科學文摘 Vol.3 Pandas、Docker 以及數據時代的反思","text":"不同於上週的 文摘 Vol.2 產品理解以及 DS / DE 之路 ，這週的選文比較技術以及實作導向。本週將導讀 3 篇使用 Python 以及 Pandas 的文章，並鼓勵讀者實際動手學習。我們也會看到如何使用 Docker 來讓資料科學變得更簡單，並提供一個有趣的貓咪圖片辨識 App 給有興趣的讀者參考。最後，讓我們分別看看哈佛商業評論以及美國前首席資料科學家 DJ Patil 談談如何讓資料科學在企業內普及，以及數據時代我們面臨的各種道德議題。 本週閱讀清單 Pandas、Python How to Master Your Skills for Pandas? How to rewrite your SQL queries in Pandas, and more Learn Functional Python in 10 Minutes Docker Docker for Data Scientists Cat Recognizer: A flask app showcasing how to recognize cats using Tensorflow 數據時代的反思 The Democratization of Data Science Data's day of reckoning How to Master Your Skills for Pandas? Python 裡頭最著名的資料處理 library 非 Pandas 莫屬了。 這篇文章 使用互動式的環境，列出挺完整的 Pandas 指令讓讀者可以邊參考 sample code 邊自己動手玩玩看。 其中包含各種利用 Series 以及 Dataframe 兩種 Pandas 常見的資料格式來對數據進行各種操作，適合沒碰過 Pandas 的新手以及想要重新 refresh 語法的人。 How to rewrite your SQL queries in Pandas, and more 提供常見的 SQL 查詢以及其對應的 Pandas 寫法。一個有效率的資料科學家通常需要 SQL 及 pandas 兼具。雖然這篇一開始的目標讀者是那些已經熟悉 SQL 並打算使用 Pandas 的讀者，我認為熟悉 Pandas 但還不了解 SQL 的同學們也能從這篇學到點東西。 這篇適合至少懂 Python 或是 SQL 並想學習另外一個語言的讀者。如果你想要深入了解 SQL 或是其與 Python 之間的差異，你可以看看我之前寫的 為何資料科學家需要學習 SQL 。 Learn Functional Python in 10 Minutes 這篇 Hackernon 文章 則簡單介紹 Functional Programming 在 Python 可以如何被實作，函式（function）是怎麼被視為 Python 的第一公民以及我們能如何活用函式如 Map、Filter 函式。 如果你剛起步，想要有效率地學習 Python 的話，我建議可以從 List comprehension 開始學起。 一個簡單的例子是假設我們想從一個 List 中取得大於 50 的數字： l = [ 5 , - 3 , 100 , 70 , 2 ] larger_than_50 = [ e for e in l if e > 50 ] print ( larger_than_50 ) [100, 70] 文章的後半段則透過 The Zen of Python （Python 的禪學）來說明為何使用 List comprehension 會比使用傳統 Functional Programming 中的 Map、Filter 函式來得簡單。 Python 有一個著名的彩蛋，你可以利用 import this 來顯示 The Zen of Python，它提供使用 Python 的人一個簡單的開發準則，具體如下： import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Docker for Data Scientists 很簡單地說明常見的 Docker 術語以及使用 Docker 可以為資料科學家帶來的好處： 節省建置開發 / 分析環境所需的時間 增加可重現性（Reproducibility） 抽象化作業系統（OS）的概念，再也沒有只能在 Mac 跑而不能在 Windows 跑的問題 這篇提供非常初級的指令來開始在本機環境使用 Docker，可以嘗試看看。 在 Smartnews 我則是使用 Amazon Elastic Container Service 來快速部署一些資料科學家們常會用到的分析工具，如大家的好朋友 Jupyter Hub 、Airbnb 開發的 BI 工具 Superset 。之後有機會會另外撰文分享經驗。 Try It Yourself Docker 讓我們可以快速重現其他人的分析環境或者是有趣的 application。如果你想馬上感受 Docker 的威力，可以看看我之前利用 Tensorflow 以及 Flask 實作的一個貓咪圖片辨識的 Github repo （feat. CNC ）： 利用 Tensorflow, Flask 實作 App 並使用 Docker 快速與他人分享成果 （圖片來源： Cat Recognizer ） 雖然 Github repo 上也有教學指南，想要最快速地在你的電腦上使用這個 App 的話，下載 Docker 並開啟 Daemon 後，使用命令列輸入以下指令： docker pull leemeng/cat docker run -it -p 2468 :5000 leemeng/cat 接著在瀏覽器輸入 localhost:2468 應該就能開始使用了。如果你想多了解點 Docker，可以參考我寫的 給資料科學家的 Docker 指南：3 種活用 Docker 的方式（上） 。 不過現在讓我們繼續看剩下的 2 篇好文章：） The Democratization of Data Science 哈佛商業評論（Harvard Business Review, HBR） 在這篇文章裏頭敘述為何不只是針對資料科學家，提升所有人的「資料素養」對一個企業來說是一件非常重要的事情。 最明顯的優點是可以讓數據團隊專注在： 解決更高層次的企業問題 建立分析工具以加速所有部門的數據分析 而不是處理每個部門的「資料瑣事」。 這個議題並非只跟企業的管理階層相關。對一個資料科學家來說，想辦法利用資料工程（Data Engineering）等方式來自動化如「建立簡單儀表板」的工作，並教導各個部門實際的使用方式，可以讓你一勞永逸，避免永遠在處理非常瑣碎的「資料瑣事」，專著在更大的目標。 !quote -你不會因為自己不是會計師就不遵守專案預算；你也不會因為不是資料科學家就不提升資料素養。 Data's day of reckoning 生活在數據驅動時代的我們或許都能感受到世界變化的快速。 美國前首席資料科學家 DJ Patil 認為不管是資料科學、機器學習還是人工智慧領域，「道德倫理」以及「安全隱私」議題都應該越來越被重視。 電腦科學（Computer Science）時代最著名的安全議題非 SQL 注入 （SQL Injection） 莫屬了。如同這個議題，在數據驅動時代，我們也會面臨類似道德以及數據保護的議題，像是人工智慧模型產生具有偏見的預測、以及最近的 GDPR 等等。 在教育方面，DJ Patil 認為我們應該教育下一代在數據處理時，應該遵守的準則並將其被納入課綱；以數據驅動的公司則需要將這些想法都納入企業文化，在招聘資料科學家的時候，除了考慮他 / 她的分析能力以外，也要評估道德倫理的部分。 身為一個資料科學家，除了技術層面的提升，也應該稍微了解這些議題。 We can build a future we want to live in, or we can build a nightmare. The choice is up to us. 結語 Pandas、SQL、Docker、資料素養的培養以及數據時代的道德倫理問題等等，這週我們也看了不少資料科學相關的文章，希望你有從這篇文章裡頭學到點東西。 雖然因為篇幅關係沒辦法把所有實際的 Python 指令列在這邊，我希望透過摘要的方式能讓沒時間的你也能學習、初步地了解資料科學並進一步發現自己有興趣的地方鑽研。 有時間的話我推薦實際閱讀這些文章（當然也可以閱讀其他你自己收藏的文章，也歡迎分享），也可以試試看我寫的 Cat Recognizer 並留言跟我說說你的想法。 之後一樣會定期更新，希望收到第一手消息的話可以點擊下面的訂閱。另外如果你有其他會對這篇文章有興趣的朋友，也請幫忙分享給他 / 她：） That's it for this week, stay tuned and happy data science! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-3.html","loc":"https://leemeng.tw/data-science-digest-volume-3.html"},{"title":"資料科學文摘 Vol.2 產品理解以及 DS / DE 之路","text":"這週一樣會透過導讀一些優質文章，讓讀者了解 3 個問題： 為何一個專業的資料科學家需要具備「產品理解」？ 何謂「顧客流失分析」？ 我們該如何使用 Python（XGBoost）來建立簡單的預測模型以改善產品？ 此外，我們也將簡單介紹在資料科學領域中逐漸崛起的「資料工程師」，其職責以及專精領域跟「資料科學家」有何不同。 最後也會分享一些與資料科學家/資料工程師相關的文章。 後文為了減少累贅，可能會穿插以下縮寫： 資料科學家 = D ata S cientist = DS 資料工程師 = D ata E ngineer = DE 另外有興趣了解此文摘緣由的讀者可以參考前篇： 資料科學文摘 Vol.1 AutoML、Airflow 及 DAU 。 讓我們開始吧！ 本週閱讀清單 產品理解 Why Data Scientists Must Focus on Developing Product Sense Product Scientist @ Medium Python、客戶流失預測 Introduction to Churn Prediction in Python DS / DE 相關 Data engineering: A quick and simple definition How To Become A Data Scientist in 12 Months Infographic – 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them 就跟以往一樣，儘管下文會依照此順序列出文章與摘要，你仍然點擊上面的任意門，從最有興趣的文章開始閱讀：） Why Data Scientists Must Focus on Developing Product Sense 作者闡述為何產品理解（Product Sense）對一個 DS 很重要，適合新手 DS 閱讀參考。 通常在企業裡頭，一個資料科學家要發揮最大的影響力，就是透過手上的資料，產生可執行的洞見（Actionable Insight），進而影響產品（Product）的發展方向。不管我們做了多少分析、多少層的神經網路模型，或是產生多少特徵值（Features），如果最後這些產物沒有對產品的發展有任何影響，一切都白搭。 我們可以透過深刻地了解自家產品（比方說使用自家的 App）、暸解競爭對手、與使用者直接溝通等方式，來培養「產品理解」。有了產品理解以後，甚至可以反過來幫助我們做特徵工程（Feature Engineering），知道在建立預測模型時什麼特徵會是重要的；還能培養從資料看不出來的敏銳直覺（intuition）。 理解產品的 DS 能同時從資料看出的趨勢以及業界直覺來改善產品並解決人們問題。 在 SmartNews ，資料科學團隊也是在產品部門之下，與此有異曲同工之妙：） Product Scientist @ Medium Medium 的人說明他們在找的 Product Scientist 應該要有什麼特質：簡單來說就是有「產品理解」的 DS，能將資料轉換成更好的產品的人材。 一個好的 DS 需要強大的溝通能力來向其他人說明洞見、了解 A/B 測試的統計顯著性（statistical significance）、以及能合理地解釋 KPI 成長的背後因素。以及最重要的：你渴望改善某個產品。 最後一點大概是所有分析領域的人都不可或缺的： 你要先對某個產品抱持著熱情，才會想方設法地找出洞見並改善它。 不管是什麼領域的 DS，都要想辦法了解自家的產品，以提供可執行的洞見。反過來說，你應該選擇進入真的有興趣的公司 / 產業。老生常談：擇你所愛，愛你所擇。 Introduction to Churn Prediction in Python 這篇適合沒用過 XGBoost 也不熟悉 App 產業的讀者。 此文主要解釋了何謂客戶流失（Customer Churn）、如何利用 Python 來建立一個簡單的 XGBoost 模型，以及如何對一個簡單的資料集做預測。 就跟預測使用一個產品 / 服務的使用者人數相同，能準確預測有多少使用者會在什麼時候放棄使用某產品（Churn）這件事情，對了解一個產品（如手機 App）的發展趨勢是很重要的事情。如果我們把「預測使用者會不會放棄使用產品」這個問題視為一個二元分類問題： 1 = 客戶流失，停止使用某產品 0 = 客戶持續使用某產品 並使用如 XGBoost 等 tree-based 的模型的話，可以直接從模型中得到各個特徵的重要程度（Feature Importance），由此獲得改善產品功能的靈感 / 線索。這同時告訴我們一個重要的事情： 除了準確度，選擇解釋性高的預測模型可以讓 DS 更容易解釋模型給決策者並影響企業決策。 當然，如何定義何謂「客戶流失」就需要資料科學家掌握領域知識。另外值得注意的是，隨著產品功能的進化，客戶流失的定義也有可能跟著改變。 Data engineering: A quick and simple definition 適合想成為資料工程師（DE）並對大數據處理有興趣的讀者。 不同於我們之前以 DS 的角度討論 為何資料科學家需要了解資料工程 ，這篇直接以 DE 角度探討 DE 對企業的重要：處理大數據的能力。 以各別負責的領域來區分的話，DS 通常負責從資料找出可執行的洞見，DE 則是負責資料管道的開發以及保證數據品質。 儘管一個 DS 也需要具備基本的 ETL 素養以及資料清理能力，這些分析專家能提供最大的價值是在找出洞見，而不是清理資料。 因此稍具規模的公司都會尋找具備大數據處理能力（ Spark 、 Flink 、 Kafka 等）的 DE 來處理數據，以讓 DS 能更專注在商業分析。 在資料科學領域越趨成熟的情況下， 我們需要更多 DE 與 DS 分工合作，以從海量數據中創造更大價值。 How To Become A Data Scientist in 12 Months 這篇適合想開始學習資料科學的讀者作為參考。 一個誤打誤撞，闖入資料科學世界的工程師述說他是如何從自學程式語言到成為一個資料科學家。列了一些他個人給新人的建議、不少線上課程以及值得追蹤的資料科學家們。 雖然每個人際遇不同，看完這篇可以確定的是，想要成為一個資料科學家絕對不是學完幾堂線上課程就可以了。關鍵在於持續學習新知。（這道理當然可以套用到各行各業上） Infographic – 13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them 一個簡單的資料圖表說明新手 DS 常犯的 13 個錯誤。 每個項目值得一一查看，不過裡頭有幾點我認為值得特別強調： 資料科學在於應用。總是要想著該怎麼實際應用學到的知識，而不是死記硬背。 專注在能「解決什麼問題」，而不是該學什麼「工具」。 業界想要解決的問題不是你在線上課程學到的那麼單純。要解決一個真正的企業問題，你還需要培養資料工程、領域知識（Domain Knowledge）以及良好的溝通能力。 結語 這週我們學到在分析任何數據之前，為何理解產品對一個資料科學家來說非常重要；我們也看到一個簡單利用 Python 嘗試預測顧客流失的案例；最後我們看到資料工程師的崛起以及閱讀了 2 篇跟資料科學相關的文章。 程式能力以及分析能力固然重要，但對產品的理解、良好的溝通能力都是成為一個專業的 DS 不可不缺的能力。 閱讀完之後如果有任何想法，或者有其他想推薦給其他讀者們閱讀的文章，都歡迎在底下留言跟其他讀者們分享。 目前預計每週會發佈新文摘，不過當有別的系列文章要寫的話可能會順延一週。如果希望在新文摘出爐的時候馬上收到通知的話，可以點擊下面的訂閱：） Stay tuned and happy data science! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-2.html","loc":"https://leemeng.tw/data-science-digest-volume-2.html"},{"title":"資料科學文摘 Vol.1 AutoML、Airflow 及 DAU","text":"作為「資料科學文摘」系列文的第一篇，在開始介紹一些優質的文章之前，請讓我稍微說明一下為何會有這系列文章的誕生。先說結論：我希望透過分享一些優質文章的重點摘要，讓更多人能更快地掌握資料科學領域的知識，找出自己有興趣的領域專研，並激盪出更多的討論。 在開始閱讀之前 或許所有職業皆如此，但個人認為資料科學家是前幾不「佛系」的一個職業，你需要擁有非常多知識來讓工作更為順利： 特定程式語言如 Python、SQL 及 R 的使用方法 統計分析方法 建構資料管道（Data Pipeline） 訓練並部署機器學習模型 業界趨勢 ... 當然，你也可以選擇當個佛系資料科學家（如果你知道有哪間企業在應徵，站內信 500 P）。 不過身為一個（非佛系）資料科學家，我常需要閱讀大量的相關文章、新知，並且嘗試模仿文章內出現的演算法、分析手法，加以實踐並應用在自己的工作上。 愛爾蘭詩人 奧斯卡．王爾德 曾說過一句 名言 ： You are what you read. ─ Oscar Wilde 以資料科學為例，你讀越多相關文章，你就越接近一個資料科學家。不管要精通什麼能力，最快的方式都是透過「模仿」專家怎麼做的。透過閱讀大量的相關文章並從它們學習及模仿，我們可以更快地，且有效率地成為一個稱職的資料科學家。 以往我在閱讀完不錯的文章以後，都會在 Evernote 裡頭寫重點摘要以供自己之後做參考、連結不同領域的知識。在回顧的時候節省了自己大量的時間。有鑑於現在越來越多人對資料科學有興趣，透過分享自己的文摘，希望能讓沒有什麼時間的人也能快速地了解新知，並進一步閱讀自己有興趣的文章。 前言說得夠多了，讓我們來看看這週的文摘吧！ 本週分享：機器學習、資料工程及 App 分析 這週想分享 5 篇文章的文摘，大致上可分為三個主題。這週因為想一次分享 Rachel Thomas 在 fast.ai 談 AutoML 的三篇系列文章，機器學習的文章比例會佔得比較重。 機器學習 What do machine learning practitioners actually do An Opinionated Introduction to AutoML and Neural Architecture Search Google's AutoML: Cutting Through the Hype 資料工程 Airflow: a workflow management platform App 業界分析 DAU/MAU is an important metric to measure engagement, but here's where it fails 除了 AutoML 的系列文以外，閱讀順序不限：） What do machine learning practitioners actually do 這篇介紹機器學習工程師平常在做些什麼，在理解這點以後，我們才知道中間有什麼地方可以自動化，以讓機器學習專案更有效率。 一個完整的機器學習專案通常會包含這些步驟： 理解企業脈絡 清理＆準備資料 訓練模型 實際部署 事後監控模型表現 針對每個步驟，文內都有進一步的項目細分以及解釋，推薦閱讀。儘管一個機器學習工程師不需要自己做所有步驟，了解它們會讓專案更為順利。 就算是專業的研究者，訓練一個深度學習的模型也不是一件非常簡單的事情。而這是 AutoML 以及其子領域，神經結構搜索（neural architecture search）嘗試要解決的。 Google 甚至號稱「只要我們有現在的一百倍計算能力，就可以取代所有機器學習人才」。 An Opinionated Introduction to AutoML and Neural Architecture Search 神經結構搜索或者 AutoML 領域可以幫助我們在「訓練模型」這個步驟的時候，訓練並選擇出最好的超參數（Hyperparameters）。 但如同上篇文所述，這通常只是機器學習專案的其中一小部分，資料科學家或機器學習相關人才並不會因此被全部取代且失業。 現在 AutoML 是非常計算密集（Computation-intensive）的：拿大量的 GPU 計算能力換取研究員的時間。但沒有大量計算能力的人，等 Google 等大公司把最佳化的架構推出來再使用或許是一個比較實際的方案。 DARTS 也是 CMU 與 DeepMind 在嘗試解決「神經結構搜索」這個問題時提出的一個架構，不過他們的假設是所有可行的模型之間是「連續的」，因此可以用常見的「梯度學習」的方式找出最佳模型。這個概念使得他們所需要的計算資源大量減少，值得關注。 Google's AutoML: Cutting Through the Hype 作者認為 Google 在推廣 AutoML 的主張：「我們需要更多計算能力來做神經架構搜尋」值得懷疑，因為就算我們能自動化搜尋出最好的神經模型架構，如何用這些模型解決真正的企業問題、如何實際部署並持續改善機器學習應用等課題，都需要人動腦筋來解決，而這部分還無法自動化。 另外畢竟不是所有做機器學習的人都需要、且有（計算）能力使用神經架構搜尋來訓練自己的模型。但我們可以透過轉換學習（Transfer Learning）來使用已訓練過的模型（pre-trained model）來解決類似問題。與其想著自己也要做最夯的神經架構搜尋，不如多多善用如 Dropout、Batch Normalization 以及 ReLU Linear Unit 來強化模型的預測能力。 不過 Google Colab Notebook 是不錯的免費計算資源，可以善加利用。 Airflow: a workflow management platform 資料科學家在進行各式各樣的分析前，首先需要做的事情通常是蒐集、整理並匯總各式各樣的資料來源以供分析。舉幾個例子： 建立數據倉儲（Data Warehousing） 做 A/B 測試的效果分析 Sessionization：了解使用者在一個 session 裡頭的探訪的網頁、點擊的廣告等活動 為了做這些分析，資料團隊需要建立可靠的資料管道及 ETL，來確保有資料可供分析以及保證資料的品質。 Airflow 是一個由 Airbnb 開發，以 Python 實作的工作流管理系統（Workflow Management System, WMS）。 Airflow 被設計來幫助資料科學家們專注在建構資料管道的邏輯，而不是擔心如果資料管道中間出了什麼差錯時該怎麼維護、重新啟動工作流。（Airflow 有會自己重試失敗工作、當失敗時通知工程師等方便功能）。 Airflow 現在已經進入 Apache 孵化器 ，前景可期。其作者 Maxime Beauchemin 在這篇用淺顯易懂的方式解說 Airflow，值得一看。手癢的朋友可以參考 Quickstart 以及 Tutorial 。 SmartNews 也有在使用 Airflow，我也寫了一篇給新手看的 Airflow 的指南：「 一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載 」，你可以參考看看：） DAU/MAU is an important metric to measure engagement, but here's where it fails 多年前由 Facebook 開始使用，DAU / MAU 是一個 App 產業常用的指標，用來衡量使用者利用自家 App 的程度。 DAU：每天活躍人數（Daily Active Users） MAU：每月活躍人數（Monthly Active Users） DAU / MAU 則是這兩者的比例。可以想像當此比例越高，代表在每月活躍的使用者人數（MAU）中，每天活躍的人數（DAU）越高，可以說明使用者的黏著度越高。 但這篇重點在於說明不同服務、產品因為本身性質的不同，並不都適合用這個指標。像是 Airbnb 這種公司，有些使用者每年可能只使用一次（活躍次數一年才一次），但一次的消費金額很驚人。以 DAU 的角度來看這種顧客的話價值不高，但使用者的生涯價值（Life Time Value）卻很高。 雖然業界很常使用，不盲目使用 DAU / MAU 這個指標，而是依照自己的產品種類，選擇最能代表使用者價值的指標，並將其最大化才是上策。 結語 呼！以上就是這週的文摘內容了！儘管我們在這篇文摘裡頭只包含了 5 篇文章， 3 篇還是系列文，你應該也能感受到不同領域的知識在腦海中互相激盪吧！ 在整理這幾篇文章的重點時我學到不少，希望你也一樣。之後會定期更新，可以隨時回來看看有沒有新文章。如果懶得每天打卡，但希望在新文摘出來的時候馬上收到通知的話，可以點擊下面的訂閱。如果你在閱讀完後有其他感想，也歡迎跟我分享：） Stay tuned and happy data science! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-science-digest-volume-1.html","loc":"https://leemeng.tw/data-science-digest-volume-1.html"},{"title":"資料科學家 L 的奇幻旅程 Vol.1 新人不得不問的 2 個問題","text":"身為一個資料科學家，我平常會寫些相關領域的文章，像是 揭開資料科學的神秘面紗 為何資料科學家需要學習 SQL 淺談資料視覺化以及 ggplot2 實踐 。 它們都獲得不錯的迴響，我也得到不少很棒的回饋。 不過如果只是介紹特定跟資料科學（Data Science，以下簡稱 DS）相關的工具或概念的話，我們可能會陷入「見樹不見林」的窘境：知道很多 DS 的知識，但卻不曉得這些知識是如何實際被運用在解決人們或是企業的問題。實際上我相信大多數企業的資料科學家在做的事情，並不像很多線上課程那麼單純；有時候你需要結合多種領域的知識，如資料工程、分析手法以及領域知識（Domain Knowledge）來解決一個商業問題。 為了讓有志成為資料科學家，或是單純想要了解的讀者們能理解 DS 是如何實際被（企業）應用，以及讓自己多一點反思的機會，趁著最近開始在 SmartNews 的新工作，我打算開始（不定期地）紀錄自己平常的工作內容以及一些經驗分享（當然，在不洩漏隱私資訊的前提下）。 我相信透過寫作，能讓更多人了解資料科學並幫助自己釐清重要概念 這系列文章將以類似說故事（奇幻旅程，喔耶！）的手法，闡述我在 SmartNews 遇到的一些挑戰，以及作為一名資料科學家（Data Scientist），我如何利用手邊各式各樣的工具以及手法來解決這些問題。透過問題導向（Problem-oriented）的方式，我希望能讓更多人理解 DS 是如何實際被應用在企業之中，進而思考自己該如何預先準備，減少進入這個領域的障礙。（歡迎分享你的想法！） 在後面幾篇文章，我們將有機會深入探討一些分析手法、如何建置預測模型，以及建置可靠的資料流（Dataflow）。但在那之前可別忘了：「巧婦難為無米之炊」。我們才剛剛開始資料科學家的工作，就連筆電裡頭也是什麼軟體都還沒被安裝呢！ 因此在大展身手之前，在這篇文章我們將討論作為一個資料科學家，如何在開始第一個分析專案的同時，「有效率」地熟悉新環境。後面你將會發現，這個初始步驟看似瑣碎，卻能讓之後的工作進行地更為順利。 熟悉環境 = 安裝軟體？ 讓我們想像一下剛從 IT 管理部門手上拿到新筆電的情境。 通常拿到公司配的新電腦以後，一個資料科學家會思考的幾個問題是： 「我要在新電腦上面裝什麼軟體？」 「公司的資料科學家們用什麼軟體？」 「我要怎麼存取公司的資料？」 面對一片空白的環境，我們的腦袋可不能也是一片空白 這些問體的確很重要也很實際（practical），也是我當初能馬上想到的問題。但後面我們會看到，該安裝什麼軟體、該怎麼存取資料庫都是「熟悉環境」裏頭最簡單的部分。為什麼？ 因為通常 manager 會準備好一個清單告訴你該裝什麼，只要照著做就好了。這個清單當然會依照公司內部使用的技術而有所差異，但在大 Google 搜尋時代之下，要在自己的筆電安裝任何東西（應該）都不是太困難的事情。 就算公司沒有給你清單，沒問題！事實上，我也不過就安裝了以下軟體： Python & Anaconda R 語言 & RStudio Jupyter Notebook Docker iTerm2 PyCharm SourceTree ... 當然隨著專案的增加，你可能還會需要其他工具，但基本上沒有想像中的那麼多。有了開發/分析工具以後， IT 管理部門也會跟你說明該如何透過加密的方式，存取一些重要的資料庫以及伺服器。等到這些都搞定以後，理論上我們已經可以準備寫落落長的 SQL 查詢來結合多個資料庫的表格，並使用各種酷炫的 Python packages 進行分析了！ 不過在進行分析的同時，有一些問題值得我們花幾天慢慢地思考。這篇我想特別強調 2 個： 公司內有什麼 關鍵績效指標（KPI） ？ 這些 KPI 是怎麼被產生並顯示在儀表板（Dashboard）上的？ 你可能覺得這些事情看起來並不直接跟資料分析相關，但接下來你會看到，為何在進入公司早期就理解它們，對一個資料科學家來說很重要。首先讓我們看看第一個問題。 公司內有什麼關鍵績效指標？ 為什麼了解公司內有什麼關鍵績效指標（Key Performance Indicator, 後簡稱 KPI）很重要？ 因為這些 KPI 代表著一企業或團隊衡量成功的方式，同時也決定了資料科學家們將要努力的方向。沒有這些 KPI，我們將不能評估我們是不是走在對的路上，也不知道前進的速度。講得浮誇點，一個資料科學家能提供的最大價值就是： 分析數據、從中找出洞見讓企業做出更好的決策，以在最短的時間內最大化 KPI 以這樣的角度來看，KPI 的概念就跟機器學習中的 目標函數（Objective Function） 的概念相同，差別只在於我們是用電腦去最佳化目標函數；用人腦去最佳化企業的 KPI。 不少人認為資料科學家的工作就是利用資料以及 DS 的力量，來最大化儀表板上的 KPI 因為 SmartNews 是一個新聞 APP，讓我們舉些 手機 APP 產業中常被使用的 KPI 為例： 安裝次數（#Installs） 每人平均使用時間（Session Time） 瀏覽頁面數（#Page Views） 每天活躍人數（#Daily Active Users） 每月活躍人數（#Monthly Active Users） 重度使用者人數（#Heavy Users） 每日廣告營收（Ad Revenue Per Day） 儘管相同產業可能用類似的 KPI，每家公司給的實際定義（Definition）可能有所出入。不同公司之間的定義有差異是正常的，但該定義合不合理就是另外一回事了。 就跟我們訓練一個機器學習模型的時候會注意目標函數的定義是否合理一樣，在了解有什麼 KPI 以後，我們也應該積極地去詢問相關人員，了解這些 KPI 的定義是否合理。像是： 怎樣的行為可以算是完成安裝？是使用者第一次打開 APP 的瞬間算安裝，還是完成新手教學的時候呢？ 何謂活躍？使用者要做什麼操作才算活躍？打開 APP，更改設定就關掉也算活躍嗎？ 何謂重度使用者？過去一個月使用超過 7 天的人算嗎？ 把握 onboarding 的機會，詢問所有你能質疑的問題 如果 KPI 的定義不合理，糟一點的結果就是你的努力方向對了， KPI 卻沒有上升；更糟的結果則是你往錯的方向最佳化：錯誤的 KPI 提升了，你則沾沾自喜。儘管定義合適的 KPI 需要大量的領域知識，在剛開工的時候，你仍應該對現有的 KPI 做出適當的質疑，嘗試理解它們的合理性。 現在假設 KPI 的定義沒有明顯問題，不管什麼公司都會希望能將這些 KPI 即時地顯示在儀表板上以方便監控自己的營運狀況。但如果一個 APP 的 下載次數超過 2500 萬 ，每天產生上億筆使用者存取紀錄的話，幾個衍生出來的問題就是：KPI 該怎麼從這些原始資料產生出來的？如何保證中間沒有出錯？我們能信賴這些計算出來的值嗎？ 讓我們在下小節討論這個問題。 儀表板上的 KPI 是怎麼產生的？ 實作方式會依照公司有所不同，但讓我們以 SmartNews 為例。 我們的儀表板是使用 CHARTIO ，但基本上 CHARTIO 這種儀表板服務也只是一個 Web UI，它並不會自動幫我們把使用者的存取紀錄轉成 KPI。為了理解每天人們使用 APP 的情況，我們必須自己將所有網路伺服器（Web Servers）上的使用者存取紀錄（Log Data）做一系列的處理以後，轉變成儀表板上的 KPI。 而一個使用者的使用行為大致上會經過以下幾個步驟轉變成 KPI： 使用者打開 SmartNews App，手機客戶端向網路伺服器做出請求（Request） 網路伺服器回傳結果，並將該請求紀錄存在自己的硬碟上 fluentd 搜集所有伺服器上的請求資料，將它們存到 Amazon S3 上 工作流管理系統 Airflow 進行 Batch 處理，定期將被存到 S3 上的使用者存取紀錄轉成 Apache Hive SQL 表格（Tables） CHARTIO 透過分散式 SQL 查詢引擎 Presto 對該表格作查詢，顯示 KPI 從左到右來表示這個流程的話會如下圖： SmartNews 的資料平台：將大量原始日誌資料轉成儀表板上有用的 KPI（當然不只用在顯示 KPI） （ 圖片來源 ） 乍看之下，你可能會想： 「這看起來跟資料科學完全沒相關啊！」 「我只要能存取關聯式資料庫（Relational Database）裡頭的表格不就好了？」 沒錯，嚴格來說這是一個資料工程（Data Engineering）的問題。但正如我們在 資料科學家為何需要了解資料工程 一文裡頭提到的，身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 事實上，了解儀表板上的 KPI 是怎麼產生的，有以下幾個優點： 理解工程師的痛點。能事先以他們的角度思考建立新表格所需的成本的話，他們會更願意幫你建立 通常新的分析會需要新的 ETL，而你可以利用跟產生 KPI 一樣的 ETL 來產生自己的資料管道（Data Pipeline） 確保資料品質。一旦使用的資料有瑕痴，做出來的分析也不會有意義。 一個資料科學家會去了解企業內的資料是怎麼流動的，確保資料的品質並建立自己需要的資料流 最後一點尤其重要。在 SmartNews 的例子裡頭，資料科學家實際上想要分析的是「APP 使用者的存取行為」，而跟使用者行為最直接相關的其實是那些被存在網路伺服器上的 log。只是因為該資料量太大，我們必須建立資料管道做前處理，從大量原始資料中萃取、匯總出我們「可能」有興趣的資料存入關聯式資料庫供之後分析。 以這種角度來看的話，資料彷彿是從網路伺服器（上游）經過一連串的河道（資料管道）流向資料庫（下游）。這也就暗示著兩個可能的風險： 資料在經過河道的時候被污染，資料品質下降 資料在經過河道的時候被限縮，有些有價值的資料沒辦法抵達下游 一個資料科學家如果只專注在下游的資料，就可能冒著以上的風險而不自知。這就是為什麼我們需要了解企業內的資料是如何流動的。 資料的流動當然不限於 KPI 的產生，但我認為用這個問題： 「儀表板上的 KPI 是怎麼產生的？」 來理解一個企業的資料流是一個很好的起始點。畢竟 KPI 是公司最重視的資訊，用來建構其的資料管道也會是最完善且重要的。 結語 在這篇文章裏頭，我們討論了一個資料科學家在進入新公司熟悉環境的時候，除了問該裝什麼工具以外，可以問的兩個問題： 公司內有什麼 KPI？ 儀表板上的 KPI 是怎麼產生的？ 表面上看來是兩個再簡單不過的問題，實際上第一個問題跟業界的專業知識（Domain Knowledge）息息相關；第二個問題則牽涉到大量的資料工程專業。而透過深刻地思考這兩個問題並詢問相關人員，一個資料科學家可以更全面的理解企業並掌握大局觀，做出最有影響力的分析。 當然，除了這兩個問題以外，你還需要問很多其他重要的問題如： 公司的資料文化如何？ 我在 Data Science 團隊裡頭的定位為何？ many more .. 但作為「資料科學家 L 的奇幻旅程」系列文的第一篇文章，為了避免累贅，我把這些問題留給你們（可以留言跟我說你覺得還有什麼問題重要！） 最後的 Bonus 問題：為何是資料科學家「L」？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/journey-of-data-scientist-L-part-1-two-must-ask-questions-when-on-board.html","loc":"https://leemeng.tw/journey-of-data-scientist-L-part-1-two-must-ask-questions-when-on-board.html"},{"title":"從彼此學習 - 淺談機器學習以及人類學習","text":"說到近年最熱門的機器學習（Machine Learning）或者人工智慧（Artificial Intelligence），因為知識背景以及觀點的不同，幾乎每個人都有不一樣的見解。雖然我們有千百種定義、無數的專業術語，這篇文章希望用直觀的方式以及具體的例子，讓讀者能夠在跳入一大堆 ML 的教學文章以及線上課程之前，能以一個更高層次且人性化的角度理解機器學習，並進而思考要如何開啟自己的機器學習旅程。 不僅如此，你將發現機器學習並不是冷冰冰的科學，隨處可見人類的巧思；就算不是資料科學家，你也能從『機器學習』獲得啟發，將一些概念用在改善『自己的學習』。 讓我們開始吧！ 目錄 何謂機器學習 機器學習實例：智慧咖啡機 如何讓機器學得更好 如何改善我們的學習 結語 何謂機器學習 多虧了媒體的大量宣傳，我們現在都知道 機器學習 被應用在各個領域。一些常見的例子包含： 自然語言處理，如 Google 翻譯、iPhone 的 Siri 語音辨識 推薦系統，如 Amazon 的 『買了這個商品的人同時也購買了 ...』功能 垃圾郵件自動判定，如同我們在 《直觀理解貝氏定理及其應用》 一文中談到的 電腦視覺，如 Facebook 的人臉辨識 、Youtube 的影片推薦，影像分類 〈這張照片是貓還是狗？〉 等 例子不勝枚舉。有那麼多應用機器學習的例子，不禁讓人思考，究竟什麼是『機器學習』？ 依照目前機器學習的應用，一個大致上的定義是： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 有了這個定義，讓我們再看一下上面提到的幾個例子： 自然語言處理：將得到的英文字串〈輸入〉，轉成中文文字〈輸出〉 推薦系統：將使用者過去的購買記錄〈輸入〉，轉成使用者可能想要購買的商品列表〈輸出〉 垃圾郵件判斷：將郵件內文〈輸入〉，轉成該郵件為垃圾信的機率〈輸出〉 電腦視覺：將一個 400 x 400 像素的圖片，轉成多個標籤的機率〈輸出〉 Google 教你做影像分類，利用機器學習，將充滿著像素的圖片轉換成一個個標籤 （圖片來源： Google Machine Learning Practica ） 嗯嗯，我想這定義還算合理。 眼尖的讀者會發現，這邊的例子說明了上述定義的一半：將輸入 X 轉換成輸出 y。為了進一步解釋後半段『衡量成功的方式』，下面讓我們以一個虛構的咖啡機舉例。 機器學習實例：智慧咖啡機 假設你是個咖啡愛好者，家裡有好幾台高檔的咖啡機，但每次泡出來的咖啡都不合你胃口。 經過無數的失敗，忍無可忍，你最後決定向月巴克公司買台『智慧』咖啡機。該咖啡機宣稱可以了解你的個人需求，泡出世界上最符合你胃口的咖啡。 拆開咖啡機包裝，你興奮地把咖啡豆、砂糖以及牛奶加到該咖啡機裡頭。幾分鐘過後，號稱世界上最好喝的咖啡完成了！ 外觀看起來不錯，你滿懷期待地啜了一口。 『太甜了吧！砂糖太多了，什麼鳥機器！』 你怒吼著，幾乎馬上萌生退貨的想法。這時候咖啡機感應到你的抱怨，用很委屈的聲調說： 『目前調配咖啡的方式為原廠設定。經過統計分析，要得到一個正常台灣人的最佳評分，平均一杯咖啡裡頭的咖啡豆顆數、砂糖匙數以及牛奶的小杯數的比例應該要是 10 比 2 比 3。』 你只覺得莫名其妙，心想這什麼神奇的比例。而且咖啡機剛剛是在拐彎抹角地說我不正常嗎？ 這時候咖啡機又說話了： 『為了做出最符合您口味的咖啡，滿分 100 的情況下，請按鈕輸入你認為此杯咖啡值幾分。另外請告訴我是哪邊出了問題，如糖份比例太高還是牛奶太多，以讓我能記住您的喜好。』 你翻了個白眼，喝杯咖啡還要教機器怎麼調配？哪裡智慧了？ 但為了喝到最符合自己喜好的咖啡，你決定給咖啡機一個機會，好好地調教它。針對眼前這杯咖啡，你把自己的回饋〈評分、調配比例的建議：砂糖太多〉老老實實地輸入進去。 於是乎就這樣，不知不覺中你已經與月巴克咖啡機一起踏上了調配世界上最好喝咖啡的學習之旅。 為了實際了解咖啡機怎麼學習，你翻開咖啡機使用手冊，看到以下內容： 《月巴克智慧咖啡機說明指南》 基本假設；使用者評分 = 使用者滿意程度 預測使用者給咖啡的評分 y' = w1 * 咖啡豆顆數 + w2 * 砂糖匙數 + w3 * 牛奶小杯數 + 基本分 b 目標：找出一組調配比重 w1, w2, w3, b ，使得咖啡機預測的評分 y' 越接近實際的使用者評分 y 越好 咖啡製作：使用上述比重調配咖啡，使得預測評分 y' 接近 100 各原料調配比重〈原廠設定〉： w1 = 10, w2 = 2, w3 = 3, b = 60 ... 你恍然大悟，原來月巴克公司為了讓咖啡機最大化你給咖啡的評分，在咖啡機裡頭建構了一個簡單的 線性回歸〈Linear Regression〉 模型。 在這模型裡頭，使用者針對一杯咖啡的評分 y 會受到多個原料的量的影響。每個原料量的影響程度則透過個別的 w 來描述。理想上，如果咖啡機可以找出一組比重〈weights〉 w ，使得咖啡機『預測』出來的評分 y' 跟『實際』使用者給的評分 y 非常相近的話，咖啡機就可以利用該模型來合理地選擇咖啡豆、砂糖以及牛奶的量，調配出一杯預期能獲得你最高評分的咖啡。 那咖啡機要如何實際『學習』呢？ 或者換句話說，咖啡機要怎麼樣知道它現在用的參數〈 w1 、 b 等〉夠不夠好呢？如果不夠好的話，要怎麼修正呢？ 在一開始完全沒有任何使用者回饋的時候，咖啡機可以很合理地使用原廠設定來計算使用者評分 y' 。等到你輸入了一些評分 y 以後，將所有從你得到的評分 y 跟咖啡機自己預測的 y' 做比較，看咖啡機做的預測評分跟你的給分差了多少，據此修正原料的比重 w 以及 b 。 y' 跟 y 的差異讓我們暫時稱作 diff_y 。 修正以後，一般來說我們會得到新的比重 w' 以及 b' 。當咖啡機使用 w' 以及 b' 產生新的預測評分 y'' ，其跟你的實際給分 y 也會有一個差距，我們則將其稱作為 diff_y' 當使用新的參數〈 w1 、 b 等〉產生的 diff_y' 比原來的 diff_y 來小的時候，我們就能很開心地表示：『這咖啡機幹得真不錯！學到了點東西，能更準確地找出我的喜好！』。 而在每次獲得你回饋的時候重複上述步驟，咖啡機不斷地修正它用來預估你給咖啡分數的參數，讓預測出來的值 y' 跟你過去所有評分 y 之間的差異都更小。雖然我們這邊不會細講，但在線性回歸裡頭，一個常被用來計算預測值 y' 跟實際值 y 差異的方式是 最小平方法〈Least Squares〉 ： diff_y = 針對每次使用者的評分 y，機器利用當下的參數產生相對應的 y' 以後，用兩者計算 (y' - y) 的平方並加總它們 你可以看到，當 diff_y 越小，代表咖啡機越能準確地依據目前的原料量，來預測你會給咖啡的評分。 咖啡機學得很快。經過幾個怒吼以及失望的夜晚，透過你給的回饋，它現在做出的咖啡已經能很穩定地讓你給出 90 分以上的評價。 透過詢問咖啡機，你現在知道，為了獲得你的高評價，咖啡機學到了以下的模型： 你給咖啡的評分 = 咖啡豆顆數 * 13 + 砂糖匙數 * 1.2 + 牛奶小杯數 * 1.5 + 基本分 40 分 這跟一開始為了滿足所有人的原廠設定相比，還差真不少： 一般使用者評分 = 咖啡豆顆數 * 10 + 砂糖匙數 * 2 + 牛奶小杯數 * 3 + 基本分 60 分 依照你過去的回〈ㄊㄧㄠˊ〉饋〈ㄐㄧㄠˋ〉，咖啡機發現跟一般人相比，咖啡豆量對你來說，是一杯咖啡好不好喝的重要指標〈13 vs 10〉，砂糖跟牛奶量則反而顯得沒那麼重要。而從基本分來看，咖啡機甚至學到你對咖啡的要求程度比一般人要來得嚴格〈40 vs 60〉，實實在在地說明機器了解你是個專業的咖啡愛好者。 現在再讓我們看一次前面定義的機器學習： 讓機器學習如何將輸入的資料 X 透過一系列的運算，轉換成指定的輸出 y。並提供一個衡量成功的方式，讓機器知道怎麼修正學習方向。 經過上面的咖啡機例子，我們能清楚地歸納出以下幾點： 咖啡機是在進行機器學習，學習如何用一連串運算，將原物料的量〈咖啡豆顆數等〉 X 轉換成使用者評分 y 機器學習裡所謂的一系列運算，在咖啡機的例子裡是進行線性回歸，即 y = w * x + b 咖啡機衡量成功的方式是計算『預測評分 y' 跟實際評分 y 之間的差異大小』，此差異越小，代表學得越好 衡量成功的方法很重要，因為咖啡機可以知道『努力/學習的方向』 咖啡機透過反覆地修正參數，進而最小化上述差異，成功地『學習』 機器學習是學習一組最符合目標的『參數』〈如基本分的 40 、咖啡豆顆數的 13 〉 我們可以總結說，咖啡機在你給的回饋以及監督之下，想辦法從三種原料〈咖啡豆、砂糖、牛奶〉中，『學習』出一個最棒的調配比例，以做出一杯能得到你最高評價的咖啡。在機器學習領域裡頭，這實際上被稱作 監督式學習〈Supervised Learning〉 。 太棒了，你跟月巴克咖啡機從此過著幸福美滿的日子‧ 如何讓機器學得更好 如果你閱讀完上面例子，開始思考以下問題： 『除了原物料的量以外，或許還可以搜集其他類型的資料，像是咖啡機主人的性別、年齡甚至泡咖啡的時間，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『與其預測使用者評分，能不能建立新的模型，直接預測使用者喜好？』 『如果咖啡機得到更多我的回饋資料，是不是會更準？』 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我得說聲恭喜，你已經擁有機器學習的思維且準備好進入機器學習的殿堂了。 但在你摩拳擦掌，準備進入殿堂時，有些人可能會跟你說，近年因為機器學習在各領域發展神速，且機器能使用的訓練資料〈Training Data〉也越來越多， 強人工智慧〈Strong AI〉 很快就會出現。不久的未來，我們甚至也不用自己設計演算法以及模型，A.I.會自動幫我們全部做好。也就是：機器會自己讓機器學得更好。 當強人工智慧出現以後，或許人類就不再被需要了。因為機器會自己讓機器學得更好。 真的嗎？沒有人能真正的預測未來，所以我們無從知曉。 但至少在接下來幾年，要讓機器學習或者人工智慧再繼續進步，『人類的思考』是不可或缺的重要因素。主要體現在兩個地方： 機器並沒有意識判斷『為什麼』以及何謂『對的方向』 機器的世界觀是人類教的 機器並沒有意識判斷『為什麼』以及何謂『正確』 電腦因為有著強大的記憶以及運算能力，在很多任務上面都已經可以超越人類的表現。 近年電腦視覺〈Computer Vision〉領域發展快速，機器學習在影像分類〈Image Classification〉的表現已經超越人眼。 （圖片來源： ILSVRC 歷屆的深度學習模型 ） 但能達到這樣的成果的前提，都是因為有人類在設計模型、監督機器學習。 目前機器學習或是 A.I. 的應用其背後的模型，當你去看裡頭一行行的程式碼的時候，裡頭並不會定義『為什麼』要做這些任務。實際上，在機器學習的過程中，機器並沒有意識到為什麼要做這些任務；而如果沒有人類的介入的話，機器也不會自己定義什麼樣的結果叫做『成功』或『正確』，而也就不知道該往什麼方向學習。 該讓機器學習什麼 怎麼定義『正確/成功』，讓機器遵從並往該方向改善 這兩件事情只有依靠人類來做決定。而其決定將大大地影響機器學習的成果以及品質。機器不會跟你說： 『我覺得把影片裡面的貓咪識別出來，比識別出交通號誌燈來得重要。』 『喔... 我覺得我們學的方向怪怪的，讓我們往這個方向學習如何？』 一個定義出錯的目標函式〈Objective Function〉將永遠無法讓機器學出我們想要的結果。 針對這點，我們應該： 找出值得解決的問題，下定我們的目標並明確定義何謂『正確』，以讓機器往該方向學習。 機器的世界觀是人類教的 第二點應該也不難理解。一個機器的世界觀基本上取決於兩點： 人類指定使用的模型〈Model〉 餵給它的資料〈Data〉 如同前面咖啡機的線性回歸，我們透過一個簡單的線性模型，教會咖啡機看世界。在咖啡機所認知的世界裡頭，使用者的評分就只會受到三種原物料量的影響：咖啡豆、砂糖及牛奶。這是一個非常簡單的世界，方便我們理解機器學習，但在真實世界上基本上不會成功運作，你需要考慮更多因素。 如同我們前面有提到，你可能會思考以下問題： 『我的喜好會隨很多因素如時間做改變，要怎麼讓咖啡機模擬這情況？』 『除了原物料的量以外，或許還可以搜集其他類型的資料，然後把它們加到模型裡頭以提高預測評分的準度？』 『除了簡單的線性回歸，我們應該也可以用其他更複雜的模型或演算法來預測使用者的評分？』 『這咖啡機學到最後，是不是只能產生適合我口味的咖啡，而不能產生大家都喜歡的咖啡？』 我們怎麼看世界，將直接影響機器怎麼看世界。 事實上你已經在思考如何擴充機器的世界觀了。你可以使用各式各樣的模型、更多的資料以讓機器能用更全面的方式來理解這個世界。而這個新的世界觀只能由你來定義。 〈現在的〉機器不會突然跟你說： 『嗯... 我覺得我們應該考慮泡咖啡時有沒有下雨，因為這可能會嚴重地影響使用者心情，進而影響評分。』 『我只依照你的評分做最佳化，可能會有 過適 問題喔！』 這些問題都是我們必須自己發現並解決，不能只期待機器自動解決〈至少這幾年〉。在機器學習領域裡頭，最怕的不是模型完全不行，而是上述的 過適〈Overfitting〉 問題：機器所看到的資料本身太過侷限，導致其雖然只看到真實世界的一小部分，就誤以為那是全世界。換句話說，機器裡存在著強烈的偏見〈bias〉。前陣子常聽到的案例是 白人設計出來的臉部辨識模型對黑人有偏見 。 以我們咖啡機的例子來說，如果你家裡只有你一人，咖啡機只需要服務你一人即好；但如果你們是一個家庭，家裡的人都希望咖啡機能為它們弄出好喝的咖啡，則每個人都需要給予咖啡機回饋，以讓咖啡機了解每個人喜好。如果仍然只有你一個人給予咖啡機回饋，其他人不給分，則咖啡機會以為得到的評分來自所有人，誤以為只要最佳化這些評分，就能滿足所有人，其實不然。 為了讓機器看得更遠更全面，我們應該： 想辦法在機器學習的模型內融入更多我們的直觀想法〈intuition〉，並讓機器看到更全面的資料，以拓展機器的『世界觀』。 如何改善我們的學習 閱讀到此，相信你對機器學習已經有個高層次的理解了。 在對機器學習有個基本的了解以後，我們在前面章節提到為了讓機器學得更好，一個可行的方向是將我們的直觀想法、世界觀轉換成機器可以運算的模型或是目標函式，以讓機器能從聰明的我們身上學習。但換個角度思考，在我們教機器『學習』的時候，應該也能從機器『學習』到什麼才對。 事實上，很多我們應用在機器學習領域的想法，緊密地跟我們的個人生活息息相關。 舉個例子，在機器學習中，過適〈Overfitting〉是我們最想要避免的問題。我們不會希望機器只學到事物的表象，或者受到 outliers 的影響，而是希望機器學到更重要的模式〈Pattern〉、趨勢〈Trend〉。所以研究者們透過各種方式來讓機器不要過適： 輸入更多資料 用更簡單的模型 減輕 outliers 的比重 正規化〈Normalization〉 ... 而當機器成功地學到了事物的本質，就能精準地預測未來並且概括所有情況〈Generalize〉： 預測股票漲幅 預測誰最後會當上總統 預測詐騙交易 預測一張照片裡頭有什麼物件 畢竟，一個只看過貓跟狗照片的機器，不管未來看到什麼，就算是汽車或是人類，也只會將視它們為一定程度的貓或者是狗。 知名心理學家 馬斯洛 曾 說過 ： 如果你只有一個槌子，你可能就會把每個問題都視為釘子。 不管學習的是機器還是人類，學會概括〈Generalize〉並避免過適〈Overfitting〉是最重要的課題。手中只有槌子的人，什麼問題都看起來像釘子。 同樣道理可以應用在人類的學習上。 當我們只注重在參加各式各樣的線上深度學習〈Deep Learning〉課程，而不去了解機器學習背後的原理就是一種過適；當我們掙扎著要用 Python 還是 R 畫漂亮的圖，而不去理解為何要這樣畫，才能讓觀眾更容易理解時也是一種過適。更不用說一個只了解 決策樹〈Decision Tree〉 的同學，看到什麼問題都會想要用決策樹來解的案例了。 學習表象比較簡單沒錯，但不能帶你走很遠。了解趨勢或者模式則讓你看到未來： 卓越的歷史學家忽視單一歷史事件，透過了解世界整體的歷史脈絡來預測未來 愛因斯坦觀察到世界的運作原理而推出有名的質能轉換公式 E = mc² 好的學習方式是理解事物背後的運作的趨勢、模式。為何我們要機器學習？為什麼深度學習會崛起？注重在詢問更多的『為什麼』以理解事物本質。 從一些已經被應用在機器學習的概念獲得啟發，我們可以重新思考並改善我們人類自己的學習。 結語 我們在這篇文章前半段以一個虛構的智慧咖啡機為例，深入探討機器學習的一些基本但十分的重要概念以及運作方式。 在掌握機器學習的基本概念以後，我們討論了如何以『人』為本，融入我們人類的智慧以讓機器更聰明地學習、了解這個世界。接著，我們用了一點篇幅，討論了看似不相關的『人類學習』以及『機器學習』之間一個共同且最重要的核心目標：『學習如何去概括〈Generalize〉事物並避免過適〈Overfitting〉』。 現在機器學習〈尤其是深度學習〉跟其他學術領域如統計、電腦科學相比，是一個相對新的領域，大家都還在摸索階段。但正如當年新興的程式設計已經普遍被重視，甚至加入國高中教育一般，我想再過幾年，等機器學習更為成熟後，人們也會開始呼籲將『機器學習』領域的知識納入課綱，成為我們下一代的基本素養之一。 未來教育模式的改變：或許『機器學習』會如同『程式設計』素養一般，成為下一代必備的基本知識素養之一 或許那就是本篇所提到的『從機器學習中學習』。 但在那時代到達之前，讓我們開心機器學習吧！ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/some-thought-on-learning-from-machine-learning.html","loc":"https://leemeng.tw/some-thought-on-learning-from-machine-learning.html"},{"title":"從經驗中學習 - 直觀理解貝氏定理及其應用","text":"貝氏定理（Bayes' theorem） 是機率論中，一個概念簡單卻非常強大的定理。有了機率論的存在，人們才能理性且合理地評估未知事物發生的可能性（例：今天的下雨機率有多少？我中樂透的可能性有多高？），並透過貝氏定理搭配經驗法則來不斷地改善目前的認知，協助我們做更好的決策。 英國數學家 哈羅德·傑弗里斯 甚至 說過 ： 貝氏定理之於機率論，就像是畢氏定理之於幾何學。 因為其簡單且強大的特性，被廣泛應用在醫療診斷以及機器學習等領域。網路並不缺貝氏定理的教學文章，但多數以 機率公式 出發，不夠直觀（至少以我個人來說），就算理解了也不易內化成自己的知識。 https://commons.wikimedia.org/w/index.php?curid=14658489 （ 圖片來源 ） 因此這篇將利用生活上我們（或 人工智慧 ）常需要考慮的事情當作引子，如： 今天的下雨機率是多少？ 這封 email 是垃圾郵件的可能性有多高？ 醫生說我得癌症了，這可靠度有多高？（好吧，或許這沒那麼常發生） 來直觀地了解貝氏定理是怎麼被應用在各式各樣的地方。我們甚至可以效仿貝氏定理的精神，讓自己能更理性地評估未知並從經驗中學習。 廢話不多說，讓我們開始吧！ 今天會下雨嗎？ 在實際說明貝氏定理的公式把你嚇跑之前，讓我們先做個簡單的假想實驗來說明貝氏定理的精神。 假設大雄一早準備出門跟靜香見面，正在考慮要不要帶傘出門。 起床的時候他想： 「這地區不太會下雨，不需要帶傘吧！」 往窗外一看，大雄眉頭一皺，發現烏雲密佈。 「痾有烏雲，感覺下雨機率上升了，但好懶得帶傘 .. 先吃完早餐再說吧。」 走到廚房，發現餐桌上一大堆螞蟻在開趴。 「依據老媽的智慧，螞蟻往屋內跑代表 下雨機率又提升了 。真的不得不帶傘了嗎 .. 不不不！我不要帶好麻煩！」 想著想著，這時候靜香打電話過來了： 「胖虎說他也要去喔！」「蛤你說什麼！？」 胖虎是有名的雨男，每次跟他出遊都會下雨。依照這個經驗以及前面看到的幾個現象，最後大雄放棄掙扎，帶著雨傘出門了。 在上面的例子中，大雄觀察到三個現象： 烏雲密佈 螞蟻開趴 胖虎出沒 依據他過往的經驗，這些現象都會使得降雨的機率提升，讓他逐漸改變剛起床的時候「今天不太會下雨」的想法，最後決定帶傘出門。 這個決策的轉變過程，其實就是貝氏定理的精神： 針對眼前發生的現象以及獲得的新資訊，搭配過往經驗，來修正一開始的想法。 實際上，大雄已經在腦海中進行了多次貝氏定理的運算而不自知（我家大雄哪有那麼聰明）。現在讓我們用比較數學的方式來重現大雄腦海中的運算。 讓我們帶點數字進去 在了解貝氏定理的目的以後，讓我們以 發生比（odds） 的方式來闡述定理。發生比很簡單，就只是列出兩個（或以上）的事件分別（可能）發生的次數。 使用發生比的好處是可以很容易地比較不同事件發生的相對次數。後面會看到，我們也能把發生比轉成機率。 假設依據過往氣象紀錄，大雄住的地區一年 365 天中有 270 天放晴，下雨的天數為 365 - 270 = 95 天。則下雨的發生比為： 雨天數：晴天數 = 95：270 你可以把發生比想像成是一種相對關係，上面這個發生比代表，在大雄所住的地區，每觀測到 95 個雨天的日子，我們同時會觀測到 270 個晴天。晴天約是雨天的三倍之多（270 / 95）。 轉換成機率來看的話，就是把雨天的天數，去除以所有天數： 95 / (95 + 270) = 0.26 = 26% 一年也就只有 26% 的降雨機會，這也是為何大雄一開始在還沒觀察到新現象（烏雲、螞蟻及胖虎）的時候，合理認為今天「應該」不會下雨的原因。 我們再繼續假設，依據大雄的過往經驗，他發現： 雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍 這個 9 倍是怎麼來的呢？ 這其實是所謂的 概度比（likelihood ratio） 。分別計算雨天及晴天發生的情況下，出現「烏雲密佈」現象的機率以後，再將兩者相除： 雨天時烏雲密佈的機率 = P(烏雲|雨天） ------------------------------- 晴天時烏雲密佈的機率 = P(烏雲|晴天） 這兩個機率又被稱為 條件機率（conditional probability） 。一般 P(A|B) 代表在事件 B 發生的情況下，事件 A 發生的機率。 假設平均來看，在 10 個雨天裡頭，早上烏雲密佈的天數為 9 天（也就是說平均有 1 天的雨天是早上沒有烏雲密佈的），則我們可以說，「給定雨天的條件下，烏雲密佈」的機率是： P(烏雲|雨天） = 9 / 10 另外在 10 個晴天裡頭，早上烏雲密佈的天數平均為 1 天（也就是說早上雖然烏雲密佈，但最後並沒有下雨的天數），則「給定晴天的條件下，烏雲密佈」的機率是： P(烏雲|晴天） = 1 / 10 則烏雲密佈的概度比即為： P(烏雲|雨天） 9 / 10 ----------- = --------- = 9 P(烏雲|晴天） 1 / 10 雖然「概度比」一詞很饒舌，但它就是一個比例，也就是「幾分之幾」的概念。 9 就是「一分之九」＝ 9 倍，而因為分母是「晴天」，你可以解讀這個 9 為 「在烏雲密佈發生的情況下，每觀測到 1 個晴天，就會同時觀測到 9 個雨天」。 也可以像是大雄觀察到的： 「雨天時，早上烏雲密佈的頻率是晴天時出現烏雲的 9 倍」。 經驗告訴我們，早上烏雲密佈的情況下，該天是雨天的機率就隨著上升 貝氏定理初顯鋒芒 所以有了這個倍數可以做什麼？直覺及經驗告訴我們，在觀測到烏雲密佈的前提下，下雨機率理論上會有所提升。 換句話說，在烏雲密佈，且觀測到的晴天數不變的情況下，觀測到的雨天數應該要有所上升，這樣下雨的天數在所有天數裡頭的比例才會上升。 而其上升的倍數就是前面的概度比（ 9 倍）。因此在烏雲密佈發生的情況下，新的下雨發生比（odds）可以寫成： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 95 * 9 ： 270 = 855 ： 270 新的下雨發生比是我們利用觀測到的現象重新計算的，因此一般稱為「事後發生比」（posterior odds）；而一開始的發生比則被稱為「事前發生比」（prior odds）。 事後發生比告訴我們，在烏雲密佈的情況下，每觀測 855 個雨天，就會同時觀測到 270 個晴天。跟事前發生比相反，現在雨天數反而超過晴天的三倍（855 / 270）。 要計算新的下雨機率，我們一樣把雨天數去除以所有天數： 855 / (855 + 270) = 0.76 = 76% 跟一開始的 26% 相比，在觀測到烏雲密佈這個現象以後，下雨的機率足足上升了 50 個百分點，現在我們有更充分的理由請大雄帶把傘了。 實際上，透過上面的計算，我們已經套用貝氏定理的公式了（ 發生比版本 ）： 事後發生比 = 概度比 * 事前發生比 如同大雄的例子，一般應用貝氏定理的情境如下： 對一件未知事物有初步的猜測（事前發生比） 觀測到跟該事物相關的現象 利用先前跟該現象有關的經驗計算出概度比 利用概度比修正該猜測，得到修正後結果（事後發生比） 重新評估、做決策 又觀察到新現象，重複步驟 3 到 5 透過貝氏定理，我們可以很快速地利用過去的經驗改善自己的想法，並產生更好的決策。 大雄不死心：單純貝式 雖然觀察到了烏雲密佈，且利用過往經驗修正下雨的機率到了 76%，懶惰的大雄一開始還是不想帶傘出去。但為何最後還是帶傘出門了呢？那是因為除了烏雲密佈以外，他還觀察到了其他兩個影響下雨機率的現象： 螞蟻開趴 胖虎出沒 貝氏定理本身雖然強大，但其中一個使它被廣泛利用的是 單純貝式（Naive Bayes） 的概念：假設不同現象之間出現的機率為 獨立 )。 設成獨立有什麼好處？事情變得很簡單，我們不用考慮現象 A 跟現象 B 之間的關聯性，能針對每個現象，分別去計算概度比，修正從「前面」的現象得到的結果，持續改善我們的認知。也就是上一節提到的貝氏定理的應用步驟 6。 如法炮製，讓我們假設大雄針對其他兩個現象的經驗是： 雨天時，螞蟻出現在室內的天數是晴天的 2 倍 雨天時，胖虎出遊的次數是晴天的 3 倍 讓我們再次套用貝氏定理，但這次不是套用在一開始什麼都不知道的事前發生比： 雨天數：晴天數 = 95：270 而是在觀察到烏雲密佈後的事後發生比： 烏雲密佈下的雨天數：晴天數 = 855：270 首先，讓我們套用跟螞蟻相關的經驗： 雨天時螞蟻出現在室內的天數是晴天的 2 倍 概度比已經算好，所以依照貝氏定理的公式： 事後發生比 = 概度比 * 事前發生比 新的（螞蟻）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 855 * 2 ： 270 = 1710 ： 270 下雨的機率則提升為： 1710 / (1710 + 270) = 0.86 = 86% 比起只有烏雲密佈，在螞蟻也出現的情況下，降雨機率又提升了接近 10%。大雄是一個降雨機率不大於 90% 就不帶傘的傢伙，讓我們看看胖虎出沒能不能使他改變心意。 同樣，再次套用定理到上一個（螞蟻的）發生比，則新的（胖虎）事後發生比為： 新雨天數：晴天數 = 原雨天數 * 概度比：晴天數 = 1710 * 3 ： 270 = 5130 ： 270 在轉換成機率之前，我們發現新的雨天數是晴天數的 10 倍以上，因此可以想像新的機率至少是 90% 以上。而實際計算下雨的機率： 5130 / (5130 + 270) = 0.95 = 95% 在觀察到烏雲密佈、螞蟻以及胖虎出沒以後，大雄預估降雨機率上升至 95%，這下不得不帶傘出門了。 重複套用貝氏定理以修正想法的過程就像是在創作：把眼前所有所見（顏料）一個一個納入考量，做出最後的判斷（作品） 動動腦時間 到了這邊，我相信你現在應該已經可以在腦中直觀地運用貝氏定理：針對眼前發生的現象，運用過去相關的經驗（計算概度比），來理性地評估某事件可能發生的機率。 事實上在你繼續讀下去之前，我建議先停一停，思考幾個可以實際在生活中運用（或者已經在用）此定理的現象，以幫助你內化（internalize）這些概念。 如果你一時想不到點子，這邊提供幾個例子： email 內文裡頭出現「週年慶」時，該郵件為垃圾信的機率 新聞內文出現「柯文哲」時，文章主題為政治的機率 醫生說你得胰腺癌 時，你真的得病的機率 玩 英雄聯盟 時， KDA 超過 4 的對手排位是鑽石以上的機率 在東京藥妝店血拼，旁邊講中文的人是台灣人的機率 如果你有其他有趣的例子，歡迎留言跟大家分享。（現在留言不用登入了！） 如同上述，貝氏定理有非常多應用。不過這邊想深入探討第一個 email 的案例：給定一封電子郵件的內文，你要怎麼判斷該信是不是垃圾信件？ 從人腦到電腦：讓機器幫我們做判斷 一個典型的垃圾郵件內文 你說沒有什麼事情難得了我們人腦。依照過往經驗： 垃圾信件裡頭出現「週年慶」一詞的機會是一般信件的 20 倍 垃圾信件裡頭出現「折扣」一詞的機會是一般信件的 10 倍 在假設所有信件裡頭一半是垃圾信件（發生比 1：1）的前提下，依照單純貝氏的公式，這封信是垃圾信件的可能性上升 200 倍（20 * 10），我們可以放心把這封信丟入垃圾信分類。 但是沒有人會想要在腦中對每封信做這個運算。人類是懶惰的，能自動化的東西就請電腦幫我們解決就好了。 另外你也不可能記得每一個詞的倍數，實際上也沒有必要。只要讓電腦幫我們記住每個詞分別在垃圾郵件以及一般信件出現的次數，就能計算所有詞彙的概度比（odds）。 等到一封新的信件來以後，找出裡頭的字對應的倍數做相乘以後，電腦就能自動分類郵件了。事實上這就是 機器學習 中 單純貝氏分類器（Naive Bayes Classifier） 在做的事情。 讓機器取代人腦自動判斷，有幾個顯而易見的好處： 判斷速度倍增 記憶能力超強 （可以把分類郵件空出的時間拿去看貓咪影片） 唷呼！垃圾郵件自動變不見！小鎮村又變得更美好了。 當然你想自己實作單純貝氏分類器的話，Python 可以使用 scikit-learn 來實作。 小心！你的經驗可靠嗎？ 我們花了很長的篇幅講了幾個貝氏定理/單純貝氏的應用，也看到它既簡單又強大的特性。但在你摩拳擦掌並實際應用此定理的時候，有幾點需要注意： 不同現象/事件真的獨立嗎？ 一開始的猜測以及經驗可靠嗎？ 很多現象不一定是完全獨立而是相關的。不過一個常見的解決方法是想辦法增加更多的現象/事件/特徵值（features）來讓做出來的貝氏分類器比較可靠。貝氏定理當然不完美，但正如統計學家 喬治·E·P·博克斯 所說 ： 所有模型（models）都是錯的；但有些是有用的。 儘管「獨立」這個假設在某些情況下不合常理，但在如垃圾郵件分類等問題上，貝氏分類器有不錯的表現。而且重點是它實作簡單，可以拿來當作 baseline。 而一開始的猜測跟經驗可不可靠這個問題，英國數學家 卡爾·皮爾森 ，針對貝氏定理則給出一個我很愛的 名言 ： 一個信奉貝氏定理的人常常做這樣的事情：模糊地期待著馬的出現，瞥見驢子的蹤影，強烈地相信他是見到了一匹駝子。 「先入為主」大概是應用貝氏定理最忌諱的點了。下次再套用定理時，記得先思考自己一開始的假設以及經驗是否值得信任或者有什麼盲點。需不需要搜集更多資料來修正一開始的想法。 我真的是驢子不是駝子啊！（豆知識：駝是馬跟驢生下的動物） 總結 我們在這篇開頭首先用「大雄評估下雨」的例子來直觀地理解貝氏定理背後的精神，接著透過簡單的數學概念、發生比（odds）以及概度比（likelihood ratio）來推出基本的貝氏定理公式。 接著進一步延伸至單純貝氏（naive bayes）的概念，讓機器透過過去累積的資訊，為我們自動分類垃圾郵件。 最後我們提到一些應用貝氏定理需要注意的事情。 即使基本的貝氏定理不難，延伸的領域非常的廣。這篇沒辦法包含所有範圍，但希望透過這篇基礎介紹，能讓讀者能利用貝氏定理的概念，更理性地評估未知並從經驗中學習（或者是建立自己的貝氏分類器）。 另外如果你有其他有趣的例子可以應用在貝氏定理，歡迎留言跟大家分享。（現在留言不用登入了！） if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/intuitive-understandind-of-bayes-rules-and-learn-from-experience.html","loc":"https://leemeng.tw/intuitive-understandind-of-bayes-rules-and-learn-from-experience.html"},{"title":"揭開資料科學的神秘面紗","text":"幾乎每天我們都能看到跟資料科學（Data Science）相關的新聞與文章，像是最近 Google 利用遞迴神經網路建立可以跟真人對話而不被發現的語音助理 、 成為 Apple 等公司的資料科學家前必讀的面試題目 等等。 市面上有大量資料科學相關課程、書籍供我們自由學習，事實上，多到一個人不可能看完。你有想過為何我們需要學習資料科學嗎？為什麼資料科學現在那麼夯？我們應該拿資料科學來做什麼？ 抽離技術實作或者分析手法的討論，這篇文章試著用簡單的經濟學原理回答這幾個問題。 希望閱讀完本文的讀者能了解為何資料科學在資訊時代扮演重要角色，以及我們要怎麼有效率地把握「資料科學力」以創造更大的價值。 目錄 本文大致上會分成以下段落： 聽說你想當資料科學家？ 資料科學到底在夯什麼？ 啊所以那個資料科學勒？ 充實你的資料科學力 結語 讓我們開始吧！ 聽說你想當資料科學家？ 資料科學大概是近年最夯的流行語之一了。不管在哪邊，你都可以聽到媒體相關的報導： 食農教育科研成果農業大數據結合資料科學 想成為資料科學家？來挑戰 Google、FB、Apple 等六間公司人工智慧最新面試題 台灣產業AI化 最大問題人才不足 成為搶手資料科學家應具備什麼技能？先學Python準沒錯 ... 族繁不及備載。 而因為企業對擁有資料科學能力的人才需求大，想成為資料科學家（Data Scientist）的同學們也不少，相關的教學文章、線上課程如雨後春筍般湧現。這邊我沒辦法把它們一一列出，但你可以前往一些知名的線上課程平台如 Coursera 、 Udemy 、 DataCamp 並搜尋「資料科學」（或者 Data Science）就知道我的意思了。 DataCamp 上基本上全部都是資料科學相關課程，寫程式寫到飽 （ 圖片來源 ） 如果我們把這些新聞報導或者教學課程，依照主題/領域做個粗略分類的話，還可以得到一些關鍵字： 大數據（Big Data） 人工智慧（Artificial Intelligence） 資料視覺化（Data Visualization） 機器學習（Machine Learning） 深度學習（Deep Learning） 統計分析（Statistical Analytics） 雲端運算 （Cloud Computing） Python、R、SQL ... 資料科學涵蓋大量領域，各領域的專業知識就像一棟棟大樓將你包圍吞噬 想學習資料科學的同學這時候就頭疼了： 「全部都要學嗎？從哪邊開始 .. 」 「選 Python 或是 R 語言 ？還是先學 SQL ？」 「資料視覺化要學 Python 的 Matplotlib 還是 R 的 ggplot2 ？」 現在有些網站很用心，為了解決你的煩惱，還將相關的課程集結起來成一個 專業課程（Specialization） 讓你一步一步跟著學。 勤學如你，上了幾門課以後學會如何利用 Python 做簡單的機器學習模型 、 使用 R 做資料視覺化 ，甚至也懂得 使用 SQL 存取資料庫 。 恭喜！你是個資料科學家了！ ... 痾.. 這麼簡單？好像哪裡怪怪的？ ... 你會不會開始思考： 所以到底啥是資料科學？資料科學到底在夯什麼？為什麼我要學資料科學？ 實際上會這樣想的不止你一人。在仔細思考並給上述問題一個合理的解釋之前，就算學了再多門課，充其量只是在不斷擴充自己的「資料科學工具盒」，但卻不知道「為何要買這些工具」、「要拿這些工具做什麼」。 資料科學工具箱：琳瑯滿目，酷！但你要用這些工具創造或是改善什麼？ 因為你學的是方便實踐資料科學的程式語言、工具、方法論（Methodology），而不是「為什麼資料科學重要」。我會用剩下的篇幅試著對此問題給出一套解釋。解釋方法有很多種，所以非常歡迎在底下留言分享你的看法。 不過現在，且聽我娓娓道來。 資料科學到底在夯什麼？ 除了耳熟能詳的「技術發展快速」、「資料量龐大」的理由以外，資料科學之所以那麼夯，背後還有一個可想而知的巨大推手：「商業利益」。 要進一步解釋這個概念，我們可以從 Google 首席經濟學家 哈爾·范里安 在 2009 年接受麥肯錫的訪問，探討 網際網路對企業的挑戰 中看出一些端倪。（真知灼見，建議作課外閱讀） 近年網際網路快速發展。要在網路上發表內容，對任何人或者任何企業來說都是輕而易舉的事情。這邊說的內容（Content）可以是任意資訊，比如説： 一則 Facebook 粉絲團貼文 一則銷售青島啤酒的網頁 一個教你學習資料科學的線上課程網頁 一篇部落格文章（像你正在看的這篇） 因為傳播媒介以及科技的進步，要在網路上發布這些資訊並讓他人注意到的成本趨近於零，而其導致的結果就是 全球的資訊量急速成長 。被稱為人工智慧之父之一的經濟學家 赫伯特·西蒙 針對這種現象就曾說過一句 名言 ： 在一個資訊豐富的世界裡頭，資訊量的富裕導致人們注意力的貧窮。 以個人的角度來看，在時間以及精力有限的情況下，我們每天能接受資訊的時間以及注意力都是有限的。如何分配這些寶貴的注意力以接收對的資訊，變成現代人的課題。 痛點即商機。很多企業透過解決這個 資料超載（Information Overload） 的問題來提供使用者價值： 漫畫網站把所有知名漫畫整理在一起供你閱讀 價值：統整、數位化、自動更新散落各地的漫畫資訊 Google 提供搜尋功能給你 價值：讓你快速找到存在地球上的任何相關資訊 Youtube 讓你免費看到飽 價值：讓你隨時看全世界最新的貓咪影片 只要喊「+1」Facebook 粉絲團就免費把「珍貴」的內容給你 價值：給你數位內容如新產品資訊、整理過後的旅遊資訊等 天下沒有白吃的午餐，企業願意這麼做必定有得到什麼。你的確取得了免費的數位內容（文章、影片、漫畫），但又付出了什麼？ 資訊時代最珍貴的資源是人們（與喵）的關注 實際上，不管是閱讀文章、觀看影片、瀏覽漫畫，你都是在拿了你最寶貴的「注意力」跟企業交換這些價值。而在成功獲得你目光的同時，這些企業則透過秀廣告給你來獲利（例 1 - 3，暫不考慮 AdBlock）。 註：在這邊，「注意力」跟「時間」有些微秒差異。不過你只要回想昨天晚上跟朋友或是家人吃飯的時候，各自滑手機的景象就可以了：你把「時間」花在跟身旁的人吃飯，卻把「注意力」（或者說是關注）放在手機裡頭的數位資訊。（如果你沒用手機，我很抱歉。） 例 4 很有趣，你是拿「你自己以及你朋友圈的人的注意力」來做價值交換（你的留言讓 Facebook 的演算法自動推播該貼文到你朋友的動態牆上，粉絲團賺到他們的關注），但基本上是同樣的道理。 資訊時代最常見的價值交換：給我你的關注，我就給你免費資訊（外加廣告） 以經濟學的角度來重述前面的觀點，現在的資訊時代最不缺的資源就是「資料」；稀有、價值高且需要小心分配的稀有財是「人們的注意力」。在這個資訊爆炸的時代，企業透過加工處理大量的原始資料，產生新產品、服務及價值來換取該稀有財： 誰能善用資料科學的力量、從現有數據創造新價值、服務或產品，並以此吸引人們珍貴的關注，就能獲得商機。 這就是為何資料科學那麼夯的其中一個原因：從資料中創造新價值，進而產生商業利益。 啊所以那個資料科學勒？ 聽到上面的例子，有些人的想法可能是： 「哇這些企業好狡猾把我的注意力都偷走了！」 「這樣回覆 +1 好有罪惡感喔嗚嗚」 「好險我用 AdBlock 嘻嘻」 但這邊重點是要說明，這種依靠廣告的商業模式已經行之多年。Facebook、Google 等企業為了抓住我們的目光，持續不斷地在精進，以求能有效率地儲存、處理以及分析由我們產生的大量數據。 而他們用來處理、分析、視覺化以及理解數據的這些程式語言、工具、方法論的總集合就構成所謂的資料科學。 資料科學的本質是搜集、理解、分析、處理以及視覺化數據，並從中萃取有用價值。 讓我們以一個簡單的 Google 搜尋做更進一步的解釋。 想像你在 Google 上搜尋「 data science courses 」後可能跑出以下結果： Google 日常：搜尋結果之上有幾個相關廣告 沒什麼特別的，Google 日常不是嗎？ 現在試著做以下步驟： 開一個新的分頁/視窗 隨便搜尋一個你有興趣的商品/產品，記下出現的幾個廣告還有它們的順序。 隨便點幾個連結或者什麼都不做 重複步驟 2 跟 3 幾次以後，你應該可以觀察到顯示的廣告消失或者順序改變了：而這是因為背後有 Google 的廣告競價系統在運作。下面是這系統的超級簡化示意圖： Google 廣告競價：運用使用者的行為資料，即時地推算出該使用者點擊各廣告的機率。搭配業主的出價，選出適當的廣告顯示。 要完成此系統需要強大的資料科學技術支持。只有一個人搜尋的時候事情還好辦，但你得知道，在本文撰寫當下，Google 平均 1 秒鐘處理 67, 000 筆 搜尋。試著想像一下，為了實現這個系統，Google 可能需要完成以下幾件事情： 使用 深度學習 進行 自然語言處理 ，判斷使用者輸入的語言以及想要表達什麼 即時處理所有使用者查詢的串流數據 利用使用者過往的瀏覽紀錄來預測點擊某廣告的機率 在公司內部監控目前台灣使用者的搜尋趨勢（類似 Google Trend ) 機器學習、統計分析、大數據 ... 這些工作運用到的技術，不就是那些我們在 聽說你想當資料科學家 章節裡頭看到的關鍵字嗎？ 我們這篇只以 Google 的廣告系統為例，但實際上現在幾乎可以說是全世界都在想辦法利用資料科學的力量來處理資料並創造新的價值、服務、公司。看看現在的新創，有哪些沒有用到資料科學？ 所以你現在知道為何資料科學那麼重要了。 全世界都在想辦法活用資料科學，以從龐大數據中為潛在使用者創造更多價值。 充實你的資料科學力 綜觀資料科學一詞萌芽到最近的過程，全世界的資料量 持續成長 ，而人們也不斷地在想辦法追趕這些資料： 用最有效率的方式儲存這些資料 用最快的速度處理及分析這些資料 對這些資料做實驗，重複再重複測試不同的假說及演算法 快速地從資料萃取出新的洞見（Insight） 以這些洞見創造新的價值、產品、服務 加速以上步驟所需要的循環時間 如同前面 Google 的例子，這些都是資料科學。 你會發現，所謂的資料科學（Data Science）就是對資料（Data）做科學、有系統地（Scientific）的處理罷了。資料科學一詞或許誕生沒多久，但對資料做科學這概念老早就存在了。只是近年因為 數據量的快速成長（如 物聯網裝置的火紅 ） 運算能力的進步 人工智慧的突破 等等原因，讓我們更急迫地想辦法用以往做不到的方式來理解這個世界的龐大數據。 Youtube 現在能夠分析出你喜歡看貓咪影片 ， Google 可以建立跟真人對話而不被發現的語音助理 。這些都是他們利用資料科學，從現有的大量數據創造額外價值的例子。如同 這篇 所說的： 未來是屬於那些能從大量複雜數據創造價值的企業以及人才的。 一個好消息是： 一企業擁有的資料量 一企業裡能夠處理、分析此資料量的資料科學人才數量 這兩者在多數企業都是不成比例的（後者短缺），因此擁有資料科學能力的人才薪水可以說是水漲船高。而這當然也變成為何近年那麼多人想成為資料科學家的動機（儘管有些人可能不知道背後原因）。 了解資料科學相關知識的人才 : 是大多數的企業積極尋找的對象 在了解這點以後，你可以先想想自己的興趣在哪裡、想用資料科學創造什麼價值。這邊想強調的是，先思考你能透過資料科學，創造什麼新的「價值」，而不是什麼「商業利益」。 如同我們前面看到的，資料科學是現行廣告經濟的背後推手，但為何我們願意看 Google、Facebook 丟給我們的廣告？那是因為他們「先」從資料創造了價值（方便的搜尋功能、社群網路功能）從而取得我們的關注。 實際上，在取得關注以後，你的商業模式不是一定要秀廣告給使用者看。訂閱制（Subscription）或會員制是一個替代方案： NetFlix 和 Amazon 都是這樣。甚至，你可以 不像 Google 一樣思考 ，使用新的商業模型。 但「商業模式」不是這篇想討論的議題。重點是「價值」： 在資訊爆炸的時代，各行各業的每個人都需要學習如何善用資料科學，從數據中創造新的價值。 事實上，與其想著要成為一個資料科學家，不如先好好想想，在自己目前所在的業界、公司、職位能怎麼利用手邊的資料數據搭配資料科學來創造新的價值。 結語 如果你耐心地看到這邊，代表我得到你最珍貴的關注了，賺賺賺！ 稍微複習一下，我們在這篇文章開頭假想了一個有志學習資料科學的同學。在他/她學習資料科學的過程產生了幾個疑問：「為何資料科學那麼夯？」「為何我們需要資料科學？」 而本篇則以非常簡單的經濟學供給概念，加上 Google 以及 Facebook 的運作方式來說明現在的企業是怎麽利用資料科學來創造新的使用者價值來交換人們的關注。 我們接著說著為何今後各行各業都需要「資料科學力」來處理日益增加的資料數據並為人們建立新的價值。事實上很多職稱不是「資料科學家」的人現在都已經在做著資料科學： 搜集、理解、分析、處理、視覺化資料數據並從中萃取有用的價值 當年網際網路開始蓬勃發展，軟體工程師是最夯最潮的行業。儘管現在工程師的重要性並沒有下降，隨著人們的程式能力穩定上升，軟體工程師回歸平凡，甚至還有人戲稱為「碼農」、「程式猿」。 歷史總是不斷重演。 或許再過幾年，等人們的資料科學力上升到一定階段，資料科學變成呼吸喝水般的知識以後，資料科學家們也會被人戲稱為「資料農」。 或許當你幾年後遇到我，我可能這樣回你： 嘿！我就只是個資料農！你也是嗎？ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/demystify-the-hype-of-data-science-and-its-value.html","loc":"https://leemeng.tw/demystify-the-hype-of-data-science-and-its-value.html"},{"title":"為何資料科學家需要學習 SQL","text":"這篇簡單討論 結構化查詢語言（SQL） 在概念上跟命令式程式語言如 Python 有什麼不同之處，以及在什麼樣的情況下我們會想要利用 SQL 做資料分析。 這篇注重在為何你會想要使用 SQL 做資料分析，而非 SQL 本身功能的教學。如果要學習 SQL 本身，可以參考最後面的 推薦閱讀 。 使用 SQL 與數據對話 身為資料科學家或者是分析人員，我們都知道 SQL 基本上是必備的分析工具。 簡單來說， SQL 是一種程式語言 ，我們可以透過它對被儲存在 關聯式資料庫 裡頭的資料進行查詢或操作。 SQL 是資料科學家與資料庫（Database）溝通的語言 在沒接觸過 SQL 之前，你可能會想 「做為一個程式語言，為何 SQL 有那麼多人在使用？ 」 「我們有 Python、R，不學 SQL 應該也沒關係吧？」 「又要學一個程式語言好麻煩。」 為了釐清這些疑問，讓我們做一個假想實驗。比方說我們現在想要知道某個特定顧客過去的所有購買記錄。 如果你熟悉 SQL 的話，可以對資料庫下一個簡單的查詢（Query）： SELECT c . name AS customer , o . totalprice , o . orderdate FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey WHERE c . name = 'Customer#000000001' ORDER BY o . orderdate ; 上面這個查詢翻為白話就是： 從顧客清單 customer 還有購賣紀錄 orders 裡頭 FROM customer AS c INNER JOIN orders AS o ON c.custkey = o.custkey 找出名為 Customer#000000001 的顧客的所有購買紀錄 WHERE c.name = 'Customer#000000001' 並把那些紀錄依照購買日期排序 ORDER BY o.orderdate 最後只回傳顧客名稱、總購買金額、購買日期幾個項目 SELECT c.name AS customer, o.totalprice, o.orderdate 這個查詢對第一次寫 SQL 的人可能會覺得很複雜，但注意，我們並沒有告訴資料庫「如何」取得這些資料，比方說： 怎麼合併顧客跟購買紀錄？ 怎麼過濾特定顧客？ 怎麼排序？ 我們只告訴它該給我們「什麼資料」。而得到的結果是： customer | totalprice | orderdate --------------------+------------+------------ Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 如同我們預期，只有該顧客的購買紀錄被回傳，且依照購買日期 orderdate 從早排到晚。 實際上，資料庫可能需要做以下運算來取得資料： 將顧客表格 customer 以及購買紀錄的表格 orders 分別命名為 c 及 o 依照共通的鍵值 custkey 合併（ JOIN ）兩表格 找出特定顧客 Customer#000000001 的購買記錄 將該紀錄依照購買日期 orderdate 排序 選出要顯示的欄位 這些運算最後都得依照「某個」順序執行，但是我們不需要考慮這些事情，完全依靠資料庫的 查詢最佳化器（Query Optimizer） 來幫我們決定。 寫 SQL 敘述時，你可以理解成我們是指定「要的資料」，而查詢最佳化器會依照此需求，找出一個最佳路徑來取得必要的資料。 SQL 查詢：專注在你的目標，查詢最佳化器會負責找到達成目標的最佳路徑 換句話說，當我們在寫 SQL 的時候，是在進行 宣告式程式設計（Declarative Programming） ：我們只告訴資料庫，我們想要什麼資料（What），而不是怎麼取得（How）它們。 這跟一般常見的 命令式程式語言（Imperative Programming） 如 Python、Java 有所不同。在寫 SQL 時，我們告訴資料庫它該達成的目標 - 取得什麼資料（What）；在寫 Python 時，我們得告訴程式該怎麼達成該目標（How）。 為了進一步闡述這個概念，接著讓我們試著使用 Python 來取得跟上面的 SQL 查詢一樣的結果。 用 Python 達到 SQL 查詢效果 首先先假設所有顧客資料是透過一個 list 儲存，裡頭包含多個 dict 。每個 dict 則代表一個顧客的資料： customers = [ { \"name\" : \"Customer#000000001\" , \"custkey\" : \"1\" }, { \"name\" : \"Customer#000000002\" , \"custkey\" : \"2\" } ] 而購買記錄則是一個 dict ， dict 的鍵值為所有顧客的 custkey ；鍵值對應的值則是包含該顧客所有購買紀錄的 list ： orders = { \"1\" : [{ \"totalprice\" : 152411.41 , \"orderdate\" : \"1993-06-05\" }, { \"totalprice\" : 270087.44 , \"orderdate\" : \"1997-03-04\" }, { \"totalprice\" : 165928.33 , \"orderdate\" : \"1995-10-29\" } ] } 所以 orders[\"1\"] 就代表 custkey = 1 的顧客的購買紀錄。 了解背後的資料結構以後，我們可以寫一段 Python 程式碼來取得資料： print ( \"customer | totalprice| orderdate \" ) print ( \"------------------ | ----------| --------- \" ) # 從所有顧客找符合條件的人 for c in customers : # 跳過我們沒興趣的顧客 if c [ 'name' ] != 'Customer#000000001' : continue # 利用 custkey 取德該顧客的購買紀錄 c_orders = orders [ c [ 'custkey' ]] # 依照 orderdate 排序購買紀錄 c_orders_sorted = sorted ( c_orders , key = lambda x : x [ 'orderdate' ]) # 將所有排序後的記錄回傳 for o in c_orders_sorted : values = [ c [ 'name' ], str ( o [ 'totalprice' ]), str ( o [ 'orderdate' ])] print ( \" | \" . join ( values )) # 已經找到該顧客，提早結束迴圈以減少處理時間 break customer | totalprice| orderdate ------------------ | ----------| --------- Customer#000000001 | 152411.41 | 1993-06-05 Customer#000000001 | 165928.33 | 1995-10-29 Customer#000000001 | 270087.44 | 1997-03-04 所以我們使用 Python 達到跟上面的 SQL 查詢一樣的結果了。但兩者在執行上有什麼差異？ 使用命令式程式語言來處理資料時，我們需要： 了解資料結構以操作資料（顧客是存在 list 還是 dict ？） 明確地定義執行步驟（先排序購買記錄 orders 還是先把顧客 customers 跟購買紀錄合併？） 最佳化（如最後的 break ） 再看一次先前的 SQL 查詢（+註解）： -- 給我以下幾個欄位：顧客名稱、總購買金額、購買日期 SELECT c . name AS customer , o . totalprice , o . orderdate -- 將有相同 custkey 的顧客跟購買紀錄合併 FROM customer AS c INNER JOIN orders AS o ON c . custkey = o . custkey -- 只需要此顧客的購買紀錄 WHERE c . name = 'Customer#000000001' -- 依照購買日期排序 ORDER BY o . orderdate ; 這裡頭我們不需要了解資料被以什麼形式儲存，也不需要定義要以什麼順序執行查詢，更不用做最佳化。這些事情全部交給背後的資料庫處理，使得資料科學家可以專注在更高層次的問題：「我們需要什麼資料？」 而這正是 SQL 最強大的地方： SQL 讓資料科學家可以專注在需要「什麼」資料而非要「怎麼」取得。 結語 雖然我們這篇只舉了一個十分簡單的例子，但一般來說 SQL 非常適合以下的使用情境： 將多個資料來源（例：表格）合併起來並依照一些條件篩選結果 依照取得的資料做一些簡易的 aggregation （如：加總、平均、最大值） 簡單的資料轉換（例：把 datetime 欄位取出年份） 如果需要十分複雜的資料轉換或者計算時，一般我還是推薦使用 Python 或 R。但是下次當你有機會使用 SQL 取得想要的資料時，不妨試著專注在「想要什麼資料」而不是「怎麼取得資料」。說不定一個 SQL 查詢就能幫你省下一些花在搜集資料的時間。 推薦閱讀 DataCamp - Intro to SQL for Data Science DataCamp - Joining Data in PostgreSQL LinkedIn Learning - Advanced SQL for Data Scientists if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/why-you-need-to-learn-sql-as-a-data-scientist.html","loc":"https://leemeng.tw/why-you-need-to-learn-sql-as-a-data-scientist.html"},{"title":"資料科學家為何需要了解資料工程","text":"透過描述資料科學家的一天日常，本文將簡單介紹資料工程（Data Engineering）的概念、其如何跟資料科學相關。以及最重要的，作為一個資料科學家（Data Scientist）應該如何學習並善用這些知識來創造最大價值。 身為一個資料科學家，擁有資料工程的知識可以提升工作效率，點亮你的方向並加速專案前進。 目錄 資料科學家的一天 資料準備 第一挑戰：資料量大增 第二挑戰：非結構化資料 資料為本 資料管道 資料倉儲 資料湖 如何實際應用資料工程？ 結語 資料科學家的一天 一說到資料科學，在你腦海中浮現的幾個關鍵字可能是： 資料分析 資料視覺化 A.I. / 機器學習 等為人津津樂道的面向。 的確，這些都在資料科學的範疇裡頭，但實際上佔用多數資料科學家大部分時間，卻常被忽略的部分是資料準備： 依據 Forbes 調查，多數資料科學家花 80 % 的時候在準備資料 （ 圖片來源 ） 資料準備 說到資料準備，你可能會聯想到我們在前一篇 淺談資料視覺化以及 ggplot2 實踐 裡頭，使用 R 語言做的簡單資料清理： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read.csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as.numeric ( ramen_all $ Stars ) # Subset 資料 ramen <- ramen_all %>% filter ( Country %in% count ( ramen_all , Country , sort = TRUE ) [1 : 6 , 1 , drop = TRUE ] ) %>% filter ( Style %in% count ( ramen_all , Style , sort = TRUE ) [1 : 4 , 1 , drop = TRUE ] ) 在做分析之前，我們做了以下的步驟來準備資料： 讀進 ramen-ratings.csv 轉變某些欄位的資料型態 依照一些條件取出想要分析的資料 雖然資料量不大，你仍然可以試著想像我們實際上建立了一個 ETL 工作： 將資料從來源（硬碟）擷取出來（ E xtract） 做了一些轉換（ T ransform） 載入（ L oad）目的地（記憶體） 假設我們把一般的資料分析專案分為以下兩階段： 資料準備：將資料轉換成適合分析的格式 資料分析：探索資料、建構預測模型 上面的 ETL 就屬於第一個步驟。又因為此資料集大概只包含 5,000 筆資料，步驟 1 所花的時間跟步驟 2 的所需時間相比，可以說微乎其微，它不會是你做資料科學的一個 bottleneck。 但如果你要處理的資料量是這個的 1,000 倍大呢？你還能馬上進入分析階段嗎？ 第一挑戰：資料量大增 實際上一個資料科學家每天需要分析的資料量可能要乘上幾個級數。現在假設你從銷售部門拿到一個包含數百萬筆銷售紀錄，大小為 60G 的 CSV 檔案，我想你應該不會想要直接打開它，即使它在某些人眼裡還不夠資格稱為大數據 (´；ω；｀) 你殫精竭慮，最後決定去問公司內一位資深的 資料工程師（Data Engineer） 該怎麼解決這問題。 該仁兄施了點你不曉得的魔法，過了幾分鐘從 Slack 丟來個神秘的 URL。連到上面，你發現熟悉的 Jupyter Nook 介面，而且 CSV 還幫你載好了 Σ(ﾟдﾟ; Bonus：Jupyter Lab 是 Jupyter Notebook 的改善版，大推 （ 圖片來源 ） 你開心地在資料工程師幫你搞定的機器上做出分析，最後在大家面前做口頭報告。大家針對報告的反應不錯，但坐在底下的廣告部門的人這時候提問了： 「可以把這些銷售紀錄跟廣告點擊的串流日誌（log）合在一起分析嗎？這樣我們會有更多有趣的結果！」 你的頭又痛了起來。 第二挑戰：非結構化資料 除了資料量級的差異，一個資料科學家在企業裡頭會遇到的另外一個挑戰是非結構化資料（Unstructured Data）的快速增加。你如何將各種不同格式（JSON、存取日誌、CSV 等）的資料以有效率的方式跟平常熟悉的關聯式資料庫如 PostgreSQL 裡頭的資料結合以供分析？ AWS Reinvent：非結構化資料快速增加，但因為不存在關聯式資料庫裡，無法直接被拿來分析 （ 圖片來源 ） 如果我們能寫一個簡單的 SQL 查詢，把銷售資料（sales）跟廣告點擊（clicks）資料依照共有的鍵值 sale_id 合起來該有多好： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 你想著想著就到下班時間了。 「算了，還是先回家睡個覺，明天再厚著臉皮問資料工程師吧！反正之前他也幫我在 資料倉儲（Data Warehouse） 加了新的表格。」 資料為本 從上面這個資料科學家的一天，我們得到什麼啟示？ 資料（的基礎設施）為資料科學之基礎 - 巧婦難為無米之炊 老實說這個例子裡頭的資料科學家已經非常幸運：公司裡有資料工程師能幫他把大量、複雜格式的資料做 ETL 並以資料倉儲中的一個新表格（Table）的方式呈現轉換過後的資料以供他使用。硬要說稍微不方便的地方，頂多就是該資料科學家得等資料工程師搞定好資料就是了。 然而因為資料工程師是一個很新的職位，多數的企業現在並沒有這樣的人存在。大多數的資料科學家只能自己下海，想辦法生出可以用的資料。實際上， Monica Rogati 在 The AI Hierarchy of Needs 提到，一些常見的資料科學專案像是 建置 AI 建置簡單的機器學習模型 資料分析 都得建立在「有完善且可靠的資料」這個基礎之下： 資料科學的金字塔層級要求：你需要建立好資料科學的基礎設施才有本錢往「上」發展 （ 圖片來源 ） 以金字塔最下三層為例，要讓資料科學的專案順利進行，你最少要（由下而上）： 持續搜集（COLLECT）原始資料 將該資料轉移（MOVE / STORE）到適合分析的地方如資料倉儲、 資料湖 轉換（TRANSFORM）被轉移的資料，進行前處理以方便分析 我認爲資料工程的重頭戲在上面的 2, 3 點：將資料以「轉換好」的形式「送」到可供分析的地方。（當然也可以先送再轉換，或者不轉換，詳見下面章節的 資料湖 ） 身為資料科學家，如果你夠幸運，公司內部有專業的資料工程師幫你把上面這件事情做好，恭喜！你可以多專注在分析以及建置預測模型上面； 但假設公司裡頭只有資料科學家，而企業又想要處理大數據的話，抱歉，你得擔起這個攤子，想辦法把資料的基本設施搞定： 每個成功的資料科學家背後都有個偉大的資料工程師。或者該資料科學家就是那個資料工程師。 身為資料科學家，如果我們也能了解資料工程相關的知識的話，不就能更快地、更有效率地進行資料分析了嗎？ 這個想法即是所謂的 從鄰近專業（Adjacent Disciplines）學習 ：透過學習跟本業息息相關的資料工程，資料科學家可以加速資料科學的專案進行，並為個人以及團隊創造更大價值。想閱讀更多，可以看看 在 Airbnb 工作的資料科學家怎麼說 。 接著讓我們稍微聊聊到底什麼是資料工程以及一些相關例子。 資料管道 依照前面的論述，資料工程最主要的目的就是建構資料科學的基本設施（Infrastructure）。而這些基礎設施裡頭一個很重要的部分是 資料管道（Data Pipeline） 的建置：將資料從來源 S ource 導向目的地 T arget 以供之後的利用。有必要的話，對資料進行一些轉換。 一些簡單的例子像是我們之前部落格提到的： 將 NoSQL（MongoDB） 資料導向資料倉儲（Redshift） 將串流資料（Kinesis）導向資料湖（AWS S3） 從上面的例子也可以看到，實際上資料管道是一個涵蓋範圍很廣的詞彙，包含 即時的串流資料處理 Batch 處理（如：每 12 小時作一次） ETL 做的事情跟資料管道類似，但偏重在 Batch 處理，這篇文章將 ETL 視為資料管道裡頭的一個子集。 從資料來源擷取、轉換資料並將其導入目的地 （ 圖片來源 ） 資料的來源或目的地可以是： 分散式檔案儲存系統（如 HDFS 、 AWS S3 ） 一般的資料庫 / 資料倉儲（如 AWS Redshift ） ... ETL 最重要的是轉換步驟，一些常見的轉換包含： 改變欄位名稱 去除空值（Missing Value） 套用商業邏輯，事先做資料整合（Aggregate） 轉變資料格式（例：從 JSON 到適合資料倉儲的格式如 Parquet ） 資料工程師建構資料管道以讓大量的資料可供分析 這些轉換都是為了讓之後使用資料的資料科學家們能更輕鬆地分析資料。為了建置可靠的資料管道 / ETL 流程，我們常會需要使用一些管理工具像是 Airflow 、 AWS Glue 以確保資料的處理如同我們預期。 一些關鍵技術 Hadoop 生態環境 分散式系統上的 ETL 設計 SQL-on-Hadoop 的專案了解（如 Apache Hive, Spark SQL, Fackbook Presto） 資料流程管理（如 Airflow、AWS Glue） 那經過資料管道處理後的資料要怎麼存取/分析？依照存取方式的不同，資料管道的架構方式也會有所不同。 而存取資料的方式大概可以分為兩種： 資料倉儲（Data Warehousing） 資料湖（Data Lake） 資料倉儲 資料倉儲的概念就跟實際的倉儲相同：你在這邊將原料（原始資料）轉化成可以消化的產品（資料庫裡頭的經過整理的一筆筆紀錄）並存起來方便之後分析。 這邊最重要的概念是：為了方便商業智慧的萃取，在將資料放入資料倉儲前，資料科學家 / 資料工程師需要花很多的心力決定資料庫綱目（Database Schema）要長什麼樣子。 也就是說資料庫的綱要（Schema）在建立資料管道的時候就已經被決定了：這種模式稱之為 Schema-on-Write。這是為了確保資料在被放進資料倉儲的時候就已經是可以分析的形式，方便資料科學家分析。 你可以想像資料工程師在建構資料管道 / ETL 的時候，得對原始資料做大量的轉換以讓資料在被 寫 入資料倉儲時就已經符合一開始定義的 Schema。而資料倉儲最常被拿來使用的一個資料模型（Data Model）是所謂的 Dimensional Modeling （Stars / Snowflaks Schema）。 資料倉儲最被廣泛使用的 Data Model：Stars Schema （ 圖片來源 ） 資料工程師將企業最重要的事件（如：使用者下了訂單、發了一個 Facebook 動態）放到最中間的 Fact Table，並且為了可以使用所有想像得到的維度來分析這些事件，會把事件本身的維度（Dimensions）再分別存在外圍的多個 Dimension Tables。常見的維度有： 時間（此事件什麼時候產生、年月份、星期幾等） 商品的製造商的資料、其他細節 ... 因為看起來就像是一個星星，因此被命名為 Stars Schema。Snowflakes 則是其變形。 一些關鍵技術 在資料倉儲的部分，關鍵的技術與概念有： 了解正規化（Normalization）的好處 分散式 SQL 查詢引擎的原理（如 Presto ） 分析專用的資料模型的設計原理（如 Stars / Snowflakes schema） 了解分散式系統背後各種 JOIN 的原理（Sort-Merge JOINs、Broadcast Hash JOINs、Paritioned Hash JOINs 等） 資料湖 「每天新增的資料量太多，要把所有想分析的資料都做詳細的 Schema 轉換再存入資料倉儲太花人力成本。總之先把這些資料原封不動地存到分散式檔案儲存系統上，之後利用如 AWS Glue 等服務將資料的 schema 爬出來並分析。」這就是以資料湖為核心的資料管道架構想法。一般這種存取資料的方式我們稱之為 Schema-on-Read，因為 Schema 是在實際載入原始資料的時候才被使用。 AWS Athena 就是一個 AWS 依照這樣的想法打造的服務。 舉個簡單的例子，假設我們現在想把 資料科學家的一天 提到的銷售資料以及廣告資料合併起來做分析，在 AWS 上我們可以實作一個這樣的資料管道： 利用 AWS Athena 及 AWS Glue 實作以資料湖為基礎的分析架構，即時合併並分析不同格式的資料 我們將存在關聯式資料庫的銷售資料透過 ETL 存到資料湖（AWS S3）裡頭以後，利用 AWS Glue 將資料的中繼資料（Meta Data）存在資料目錄（Data Catalogue）底頭。常見的中繼資料有 表格定義（有哪些欄位，如： sale_id ） 各個欄位的資料型態 各個欄位實際在原始資料（如 CSV ）裡頭的排列順序 接著我們就可以利用提到的 SQL 查詢把銷售資料跟廣告資料合併： SELECT * FROM sales AS s INNER JOIN clicks AS c ON s.sale_id = c.sale_id 收到以上的 SQL 查詢，Athena 會分別把銷售資料以及廣告資料依照對應的資料目錄解析資料後合併再回傳結果給我們。 我認為今後這種以資料湖為基礎的分析架構會越來越熱門，原因如下： 非結構化資料量越來越大，花費人力在事前為資料倉儲建立完整的 schema 越來越不實際 分散式 SQL 查詢服務像是 Athena 抽象化複雜的資料格式，允許資料科學家下 SQL 查詢做 ad-hoc 分析 透過 Parquet / ORC 等資料格式來自動減少資料湖沒有做正規化而導致的效能損失 一些關鍵技術 雖然再過幾年，等到資料工程的人才增加，資料科學家或許可以完全不用介意背後的資料基礎設施的建置，但近幾年這部分可能還是要靠資料科學家自己實作。 資料湖的概念 AWS Glue + AWS Athena 的運用（Bonus: Serverless 分析架構，不需管理機器） Hive MetaData Store 在資料湖的例子我主要都用 AWS 的服務來舉例，但你可以自由使用其他雲端服務或者 Hadoop。 如何實際應用資料工程？ 首先你得先了解目前環境的資料基礎設施。而為了釐清這點，你可以問自己或者相關人員以下問題： 資料科學的金字塔，我們建到哪一層了？ 我們過去有哪些專案是在取得、準備資料階段就陷入瓶頸？ 我們有專業的資料工程師或相關人員在做資料倉儲或者是資料湖的準備嗎？ 我們的資料是儲存在什麼分散式檔案儲存系統上面？ HDFS 還是 S3？ 我們是怎麼管理/監管 ETL 工作的？ 要考慮用 Airflow 嗎？ 要建構一個新的資料管道的話，要自己架 Hadoop 群集還是使用雲端服務？ ... 在你思考過以上幾個問題以後，你就會發現為何過往有些資料科學的專案進展緩慢了。這時候與其一直在等待資料的到來，你可以把你想到的幾個問題拿去跟相關的工程師討論。相信我，從你開口跟他們討論如何解決資料基礎設施的瓶頸這點開始，他們將不再視你為「那個只想要拿到資料」的敵人，而是同伴。 Hadoop 的分散式基礎設施。要學的東西太多，不如就用雲端服務吧 （ 圖片來源 ） 假如很不幸，你們公司沒有專業的工程師，而你得自己想辦法兜出一個可以處理這些大量資料的方法，我會建議先從現存的全受管（Full-Managed）雲端服務找能解決痛點的方案。 使用現成的雲端服務來建置資料基礎有幾個好處： Pay-as-you-go，通常是用多少花多少 Proof-of-concept，你可以直接開始嘗試建立最重要的商業邏輯而非架機器 Serverless 架構，不需管理機器（如 AWS Glue + Athena） 導入成本降低（相較於自己架 Hadoop Cluster） 結語 我嘗試在這篇文章說明資料工程對資料科學家的重要，以及你可以如何開始學習資料工程。 在這個大數據時代，資料科學家的價值在於找出「大量」資料背後的潛在價值，不要反而讓「資料量太多」這邊成了你最大的限制。 從雲端服務開始，多學一點資料工程，讓你的資料科學專案前進地更快吧！ 如果你有任何想法想要提出或分享，都歡迎在底下留言或者透過社群網站聯絡我 B-) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/why-you-need-to-learn-data-engineering-as-a-data-scientist.html","loc":"https://leemeng.tw/why-you-need-to-learn-data-engineering-as-a-data-scientist.html"},{"title":"淺談資料視覺化以及 ggplot2 實踐","text":"這篇主要描述自己以往在利用 Python 做資料視覺化 (data visualization) 時常犯的思維瑕疵，而該思維如何在接觸 R 的 ggplot2 以後得到改善。 本文會試著說明資料視覺化的本質為何，以及在設計視覺化時，概念上應該包含什麼要素以及步驟。最後展示如何透過 ggplot2 活用前述的概念，來實際做資料視覺化。 目錄 文章內容大致上會分為以下幾個小節： 資料視覺化是資料與圖的直接映射？ 資料視覺化應該是 .. ggplot2 實踐 結語 References 資料視覺化是資料與圖的直接映射？ 身為一個 Python 起家的資料科學家，在做資料視覺化的時候，我很自然地使用 Python ecosystem 裡像是 matplotlib 以及 seaborn 等繪圖 packages。針對手中的資料，我會想辦法找到一個「對應」的圖然後把資料塞進去。簡單無腦 (:3 」∠) 舉例來說，當我們手上有三個變數 x, y, z 且其各自的資料型態為： x: 定量變數 (quantitative) y: 定量變數 z: 定性變數（categorical） 則我們想要進行資料視覺化的時候有幾種選擇： 想分析 x, y -> 都是定量資料 -> 散佈圖 (scatter plot) 想分析 x, z -> 一定量一定性 -> 長條圖 (bar chart) 在這，「資料視覺化」的定義是一種映射關係 (mapping)：也就是如何將資料直接對應到某個「特定」圖表形式（折線圖、散佈圖 etc.）。基本上這種映射關係在做簡單的分析的時候沒有什麼問題，但是當想要同時分析/呈現的變數超過兩個 （例： x & y & z ）的時候就不容易找到適合的圖。一個折衷的方法是我們把變數兩兩畫圖做比較，但這樣會侷限我們能分析的資料維度數目，錯過一些有趣的洞見。 資料視覺化應該是 .. 先確認觀眾及目的 在完成一些 ggplot2 的 tutorials 後，可以發現資料視覺化一般依用途可以分為兩種： 探索、了解資料特性 說故事：將探索過後得到的洞見 (insight) 傳達給其他人 搞清楚資料視覺化的目的以及觀眾是重要的第一步 （ 圖片來源 ） 依照目的以及觀眾的不同，資料視覺化的方式會有所不同。一個常見的例子是當我們第一次接觸某個資料集。這時候資料視覺化的觀眾是自己，目的是在最短的時間了解資料特性。則這時我們在做圖的時候的要求就可以很寬鬆，像是不加上標題，或是只要能做出自己能理解的視覺化即可。 正式定義 在確認觀眾及目的以後，我們終於可以開始進行資料視覺化了！資料視覺化的定義因人而異，而這邊我想給出一個非常直觀的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 一些常見且肉眼容易識別的視覺變數 / 刻度（visual variables / scales）包含： 位置（x / y axis） 顏色（color） 大小（size） 透明程度（alpha） 填滿（fill） 形狀（shape） 用更口語的方式來解釋：在做資料視覺化的時候，我們希望能將 肉眼難以分析的資料 對應到： 肉眼容易解讀的視覺元素 透過這個映射關係，我們可以將原本的變數的數值變化也映射到視覺變數的變化。而因為我們人類容易區別視覺變數的變化（位置差異、大小長度變化 etc），我們能更容易地理解原始資料的樣貌、變化以及模式。 舉例來說，我們可以： 把不同捷運路線（文湖線、板南線）對應到不同顏色 把各國的 GDP 對應到點的大小 把某個資料的年份對應到 Ｘ 軸，越右邊代表越接近現代 一個簡單例子 事實上，我們可能平常每天都在做資料視覺化而不自知。比方說我們有一個數列 y ： y = [ - 2.055 , - 1.132 , - 0.522 , - 1.229 , 0.013 .. ] 光是看這個數字，肉眼無法看出什麼模式，但我們可以簡單畫個圖： 這邊我們利用視覺變數「Y軸位置」來呈現數值的變化，可以馬上看出數列裡頭的值都落在 -3 到 3 之間，而這是因為我們肉眼很容易辨別「位置」這個視覺變數的變化。 圖像的分層文法 在 A Layered Grammar of Graphics 裡頭， Hadley Wickham 闡述所謂的圖像（包含由資料視覺化產生的圖像）實際上如同我們平常使用的語言，是有文法的。而其文法可以拆成 7 個部分（層）。前述的 原始資料 = 資料層（Data） 視覺變數層（Visual variables = Aesthetics） 則恰好是這個架構裡頭最底下的兩層。視覺變數是我為了方便理解建立的名詞，在原文以及 ggplot2 裡頭被稱作 Aesthetics 。（中文翻作「美學」，當初看好久也無法理解啊 (╯°Д°)╯ ┻━┻） 圖像的分層文法 （ 圖片來源 ） 看到這你一定會「哇靠那我每次畫個圖都要實作七層？」。實際上不需要，上面幾層像是主題（Theme）比較像是裝飾品，給我們更大的自由與彈性來訂製（customize）視覺化結果。在下一節我們會看到，ggplot2 會自動幫我們設定合適的主題或座標。（如果沒特別指定的話） 但一般而言，一個圖像最基本的組成是底下三層。也就是除了前述的兩層（資料、視覺變數）以外還需要加上 幾何圖形層（Geometries） 為何還要這層？假如我們有了資料，決定了視覺變數（第二層，例：把資料中的變數 A 對應到 X 軸；變數 B 對應到 Y 軸）後，實際上就可以畫一個充滿點（point）的散佈圖了不是嗎？ 這樣的思維如同 資料視覺化是資料與圖的直接映射？ 部分所提到的，有所瑕疵。如果變數 A 是分類型變數（Categorical）的話，單純以 點 為圖形的散佈圖就會變得十分難以理解（下圖左）；這時候以 長條 為圖形（下圖右）的方式會比較清楚： 獨立幾何圖形層，讓資料視覺化不再侷限於「我要畫什麼圖」，而是「我想要怎麼畫」 將「幾何圖形」這個選擇獨立出來一層讓我們在資料視覺化的時候有更大的彈性。有了這些基本概念以後，我們可以開始嘗試使用 ggplot2 來實際做一些資料視覺化。 ggplot2 實踐 在這個章節裡頭我們將使用 Kaggle 的 Ramen Ratings 來做資料視覺化。這資料集紀錄了各國泡麵所得到的星星數。首先我們要先載入這次的主角：R 語言裡頭最著名的視覺化 package ggplot2。 dplyr 則是 R 語言用來處理資料的 package。 載入 packages library ( ggplot2 ) library ( dplyr ) 值得一提的是它們都是同屬於 TidyVerse 的一員。TidyVerse 是 R 裡頭常被用來做資料科學的 packages 的集合，以 Python 來說大概就像是 Pandas + Matplotlib + Numpy 的感覺吧。 載入資料 + 簡單資料處理 如下註解所示，這邊將資料集讀入，做一些簡單的資料型態轉變後選擇一部分的資料集（subset）來做之後的視覺化： # 將 CSV 檔案載入成資料框架（dataframe） ramen_all <- read . csv ( \"datasets//ramen-ratings.csv\" ) # 將「星星數」轉成定量資料 ramen_all $ Stars <- as . numeric ( ramen_all $ Stars ) # Subset 資料，選擇拉麵數量前幾多的國家方便 demo ramen <- ramen_all %>% filter ( Country % in % count(ramen_all, Country, sort = TRUE)[1:6, 1, drop=TRUE]) %>% filter ( Style % in % count(ramen_all, Style, sort = TRUE)[1:4, 1 , drop=TRUE]) 除了我們使用 dplyr 的 filter 依照條件 subset 資料集以外，值得一提的是 pipe 運算子 %>% 。它是前面提到的 TidyVerse 裡頭的 packages 共享的介面（interface），將前一個函示的輸出當作下一個函式的輸入，讓我們可以把運算全部串（chain）在一起。在 Linux 裡頭就是如同 | 的存在。 而實際我們的資料長這樣： head ( ramen ) Review.. Brand Variety Style Country Stars Top.Ten 2580 New Touch T's Restaurant Tantanmen Cup Japan 37 2579 Just Way Noodles Spicy Hot Sesame Spicy Hot Sesame Guan-miao Noodles Pack Taiwan 7 2578 Nissin Cup Noodles Chicken Vegetable Cup USA 16 2577 Wei Lih GGE Ramen Snack Tomato Flavor Pack Taiwan 19 2575 Samyang Foods Kimchi song Song Ramen Pack South Korea 47 2574 Acecook Spice Deli Tantan Men With Cilantro Cup Japan 39 簡單資料視覺化 有了資料，讓我們再確定一下資料視覺化的目的及觀眾： 目的：探索資料 觀眾：我們自己 這樣的條件讓我們知道視覺化的條件是快速做出結果，不需調整如標題、主題的設定。 現在讓我們問一些簡單的問題。像是 泡麵的包裝（碗裝、袋裝等）各佔多少比例？ 不同國家各有多少泡麵在資料集裡頭？ 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 其中一種能解決第一個問題的資料視覺化是： ggplot ( ramen , aes ( x = Style )) + geom_bar () 在 ggplot ( ramen , aes ( x = Style )) + geom_bar () 裡頭，我們實際上已經建構了圖表最基礎的三層元素： 資料層： ramen 告訴 ggplot2 使用此資料框架 視覺變數層： aes(x = Style) 告訴 ggplot2 我們將使用「 X 軸位置」這個視覺變數來反映泡麵包裝 Style 這個變數的變化 因為包裝的值有四種可能，你可以想像 ggplot2 已經準備好要幫你在 X 軸上的四個位置畫圖 aes 是我們前面提到 aesthetics 的縮寫 幾何圖形層： geom_bar() 告訴 ggplot 去計算對應到 x 視覺變數的變數裡頭，所有值的出現次數後將結果以 長條 來呈現 我們通常透過 + 來疊加不同層的結果。 基本層數缺一不可 上面的例子很簡單，但假如我們沒有指定幾何圖形層的話，圖會長什麼樣子呢？ ggplot ( ramen , aes ( x = Style )) 就像我們剛剛所說的，雖然 ggplot2 已經知道要用什麼資料框架、要用什麼視覺變數，不知道要用什麼圖形表示的話就會是空白一張圖。 另個簡單例子 讓我們依樣畫葫蘆，來解決第二個問題： 不同國家各有多少泡麵在資料集裡頭？ ggplot ( ramen , aes ( x = Country , fill = Style )) + geom_bar () + coord_flip () 這邊有兩個值得注意的地方： 除了基本的三層以外，我們透過 + coord_flip() 額外對座標層（Coordinates）做操作，請 ggplot2 把 x, y 軸互換。 透過 aes(..., fill = Style) 裡頭的 fill = Style ，我們告訴 ggplot2 將長條圖裡頭的填滿空間（fill）這個視覺變數，依照泡麵包裝（Style）做變化 第二點是在做資料視覺化的時候，想辦法增加 資料墨水量（Data Ink Ratio） 的例子。透過增加顯示在同張圖上的變數數目，進而提高該圖能傳達的訊息量。 舉例而言，我們可以很明顯地看到，在這資料集裡頭，台灣的杯裝泡麵（Cup）沒有被記錄到多少；而日本被記錄到的泡麵量最多，且袋裝（Pack）數目最多。這些是在我們沒有用「填滿」這個視覺變數時無法察覺的。而在 ggplot2 裡，要實現這種視覺化非常容易。 複雜例子 讓我們解決最後一個問題： 不同包裝的泡麵所得到的星星總數，在不同國家有什麼差異嗎？ 資料視覺化一個有趣的地方就是：同個問題不同的人會有不同的做法。而針對這問題其中一種做法是： 將包裝 Style 對應到 X 軸、星星數 Stars 對應到 Y 軸，然後使用長條 geom_bar 顯示數值 依照每個國家重複步驟一 而 ggplot2 的實作為： ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) + facet_wrap ( ~ Country ) 實際上在上面的程式碼裡頭，我們多操作了額外兩層： 統計層（Statistics）：專門負責匯總資料 小平面層（Facets）：依照選定的變數分別畫圖，如上述的步驟二 首先 ggplot2 的 geom_bar 預設只需要 x 視覺變數，因為匯總資料的統計層會把 x 依照不同的值分別計數（也就是各個包裝的數量），然後讓 geom_bar 顯示。但我們並不希望 geom_bar 使用這個數值，因此使用 geom_bar 裡頭的 stat = \"identity\" 是告訴統計層不要分別計數，而是使用我們給定的星星數 y 。 而 facet_wrap( ~ Country) 則是告訴小平面層依照 Country 這個變數重複畫 ggplot ( ramen , aes ( x = Style , y = Stars )) + geom_bar ( stat = \"identity\" ) 注意所有的圖的 x, y 軸都是一致的，方便我們做比較。 結語 資料視覺化需要統計知識以及設計美感，涵蓋範圍非常廣大。這篇雖然打了落落長，但真的只有碰到皮毛（淚）。資料視覺化感覺都可以打個系列文了。但最後再次重申資料視覺化的定義： 資料視覺化是將資料中的變數映射到視覺變數上，進而有效且有意義地呈現資料的樣貌 總之先確認你的觀眾與目的，選好你想要觀察的變數，選擇適當的視覺變數做可視化吧！ References DataCamp - Data Visualization with ggplot2 (Part 1) r-statistics.co - ggplot2 tutorial Safari - Data Visualization in R With ggplot2 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/data-visualization-from-matplotlib-to-ggplot2.html","loc":"https://leemeng.tw/data-visualization-from-matplotlib-to-ggplot2.html"},{"title":"利用 Kinesis 處理串流資料並建立資料湖","text":"所謂的 資料湖 (data lake) 指的是一企業裡頭所有形式的資料的集合。這些資料包含原始資料 (raw data)，以及經過轉換的衍生資料 (derived data)。 資料湖的核心概念是將所有可用的資料全部整合在一個邏輯上相近的地方以供企業自由結合並做各式各樣的運用。資料湖可以用很多方式建立，這裏我們主要介紹如何利用 Amazon Kinesis 將串流資料 (streaming data) 載入資料湖。 概觀 資料湖概念上可以說是企業的所有資料的最終目的地。現在假設我們打算以 Amazon S3 中作為我們的資料湖，問題就變成：要如何將串流資料穩定地傳到 S3。這部分我們將透過 Amazon Kinesis 來達成。 Kinesis 本質上是跟 Apache Kafka 類似的 message broker ，將訊息依照 message producers 產生的順序傳遞給 message consumers。實際上資料的流動會如下圖所示： Simple Dataflow：將 streaming data 透過 Kinesis 保存在 S3 上圖有幾點值得說明： 作為一個簡易的 demo，這邊我們的串流資料產生者 (streaming data producer) 是一個簡易 python script Streams 指的是 Amazon Kinesis Data Streams 。在 Kinesis 架構裡頭，一個 data stream 通常代表一個主題 (topic)， 跟這個主題相關的 producers 會把資料傳入該 stream 以讓該主題的 consumers 之後能接受訊息。 Firehose 指的是 Amazon Kinesis Data Firehose ，是專門把接受到的串流資料寫入 AWS 上的資料存放區（如 S3、Redshift、ElasticSearch）以供後續分析的服務。 建構流程 要完成上述的資料傳輸 pipeline，我們會 follow 以下步驟： 建立一個 Kinesis data stream 建立一個 Firehose delivery stream 用 Python 傳串流資料 確認 S3 上的資料 在每個步驟裡頭會稍微澄清一些概念。 建立一個 Kinesis data stream 現在假設有一個名為 naive-app 的應用程式，我們想要把使用者在上面做的操作紀錄下來。這時候我們可以建立一個新的 Kinesis Data Stream 來接受 app 的 streaming data。這邊指的 streaming data 是使用者存取應用程式時產生的 access log。 Scalability 這邊最重要的是 Number of shards 的設定。Kinesis 將接收到的資料以 log 的方式儲存在硬碟上，而為了提高 scalability，Kinesis 利用 Partitioning 的概念將 log 切割成多個部分並分配到不同的 shards 上，再將這些 shards 分別存在不同機器上面以提高 read/write capacity。因此我們可以理解一個 Kinesis Stream (Topic) 的資料吞吐量 (throughput) 直接受到 shard 的數目影響： shard 數目越多，同時能處理 read/write 的機器越多，資料吞吐量越高。 How to scale 理想上是一開始就掌握該 Stream/Topic 需要的資料吞吐量，進而決定最佳的 Number of shards ，但有時候事與願違。事後想要改變 shard 數目時需要透過 AWS Streams API 做 Resharding。Resharding 實際上就是在改變 shard 數目：增加 shard 會讓已存在的 shard 再度被切割；減少 shard 則會合併已存在的 shard。 在這邊我們就只直接使用一個 shard for demo。 Availability 另外值得一提的是 Kinesis 為了避免資料損失，會在三個不同的 availability zones 進行資料的 replication。因為這個額外的 overhead 可能使得在同樣設定下， Kinesis 比 Kafka 慢 的情況。因為是 log-based message broker，資料會被暫時存在硬碟上，預設保留 24 小時，而最多可以付費提升到維持 7 天以用來 replay data。 建立一個 Firehose delivery stream 有了接受 naive-app 串流資料的 Kinesis stream 以後，我們要建立一個 Firehose delivery stream 來接收 Kinesis stream 的資料。 Firehouse delivery stream 簡單來說是一個將串流資料存到 AWS 資料存放區的服務（如 S3、Redshift、ElasticSearch）。因此除了 Kinesis stream 的串流資料 以外，當然也可以接其他的串流資料： CloudWatch 的 log AWS IoT 使用者自定義的串流資料 在這篇裡頭我們的串流資料是 Kinesis stream，因此 Source 選擇 Kinesis stream 並填入我們剛剛建立的 stream 名稱： naive-app-access-log 。 值得一提的是 Firehose delivery stream 會 auto-scale，並不像 Kinesis stream 要手動調整 shard 數目。不過當然傳越多花越多。 如上張圖所示，實際上 Firehose 還允許我們在 delivery stream 接受到串流資料以後把原始資料傳到指定的 Lambda function 做進一步的轉換。 但因為我們想要資料湖儲存原始的串流資料，這邊我們省略這步驟。 Configuration 實際上 Firehose 不會一接收到資料就進行資料轉移。我們可以設定 Buffer size 以及 Buffer interval 讓 Firehose 在達到其中一個條件的時候把接收到的訊息統整起來一次做資料的轉移 (batch processing)。這邊為了能讓 Firehose 盡快把收到的資料轉移到 S3，設定 Buffer interval 為 60 秒。 選擇 delivery stream 目的地 在設定好 Firehose delivery stream 的串流資料來源（e.g., Kinesis stream）以及基本設定以後，我們要決定串流資料的目的地。這邊基本上很直覺， Destination 選擇 Amazon S3 以及想要放資料的 bucket 即可。 比較需要注意的是我們可以指定此 Firehose delivery stream 在放資料進入 bucket 時要為檔案加什麼前綴。 假設未來其他的串流資料我們也想要統一放在 me-data-lake 這個 bucket 裡頭。為了方便管理，我們可以為每個 delivery stream 設定一個識別用的 Prefix。以 naive-app 來說，我們指定 Prefix 為 naive-app-access-log/ 。加上 Firehose 預設的 YYYY/MM/DD/HH/ ，該 stream 的每個檔案的路徑就會變成如下圖的 naive-app-access-log/YYYY/MM/DD/HH/file_name 。 加入 Prefix 後實際將串流資料存入 S3 時的檔案路徑 用 Python 傳串流資料 確保 Kinesis stream -> Firehose delivery stream -> S3 的資料流設定以後，我們可以寫一個簡單的 Python script 實際傳資料進 Kinesis stream 做測試。但首先先讓我們使用 AWS SDK for Python 實作一個寄訊息給 Kinesis stream 的 function write_to_stream ： import boto3 import json def write_to_stream ( event_id , event , region_name , stream_name ): \"\"\"Write streaming event to specified Kinesis Stream within specified region. Parameters ---------- event_id: str The unique identifer for the event which will be needed in partitioning. event: dict The actual payload including all the details of the event. region_name: str AWS region identifier, e.g., \"ap-northeast-1\". stream_name: str Kinesis Stream name to write. Returns ------- res: Response returned by `put_record` func defined in boto3.client('kinesis') \"\"\" client = boto3 . client ( 'kinesis' , region_name = region_name ) res = client . put_record ( StreamName = stream_name , Data = json . dumps ( event ) + ' \\n ' , PartitionKey = event_id ) return res write_to_stream 基本上是把一個 Python dict event 利用 json.dumps 轉成字串後傳到指定的 region 的 Kinesis stream 裡的函式。（完整的 Gist ） 這邊值得注意的是 Data=json.dumps(event) + '\\n' 裡頭的 '\\n' 。如果之後想要利用 AWS Glue 或者 Athena 來進一步分析此串流資料的話，推薦在代表一個 event 的字串後面加上換行符號以維持「一行一事件」的資料形式，方便 schema 的自動產生。 範例日誌檔案內容會像是這樣： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 另外值得一提的是因為 Kinesis 背後是使用 Hash partitioning 來分配資料到 shard，基本上 PartitionKey=event_id 裡頭的 event_id 只要每個訊息都是獨一無二的，就能保證資料能「平均地」分配到各個 shard 上。 有了此函式以後，我們可以實際傳一些訊息進 Kinesis stream： while True : event = { \"event_id\" : str ( random . randint ( 1 , 100000 )), \"event_type\" : random . choice ([ 'read_post' , 'write_post' , 'make_comments' ]), \"timestamp\" : calendar . timegm ( datetime . utcnow () . timetuple ()) } # send to Kinesis Stream event_id = event [ 'event_id' ] write_to_stream ( event_id , event , REGION_NAME , KINESIS_STREAM_NAME ) time . sleep ( 5 ) 假設我們的 naive-app 可以讓使用者讀文章、寫文章以及寫評論，則上面的程式碼是模擬使用者使用 naive-app 時產生的事件，並將該事件的內容傳到 Kinesis stream naive-app-access-log 。60 秒內幾筆產生的事件如下： {'event_id': '56262', 'event_type': 'write_post', 'timestamp': 1522740951} {'event_id': '35672', 'event_type': 'make_comments', 'timestamp': 1522740956} {'event_id': '71613', 'event_type': 'read_post', 'timestamp': 1522740962} {'event_id': '48160', 'event_type': 'make_comments', 'timestamp': 1522740967} {'event_id': '96093', 'event_type': 'write_post', 'timestamp': 1522740972} 確認 S3 上的資料 注意因為上面的 5 個事件在 $5 * 5 = 25$ 秒內就產生了。且因為我們前面設定 Firehose delivery stream 的 Buffer interval 為 60 秒，Firehose 會把以上的事件的訊息全部串接起來，放到一個檔案裡頭，而不是分成五個檔案： 加入 Prefix 後實際將串流資料存入 S3 時的檔案路徑 而實際檔案的內容如下： {\"event_id\": \"56262\", \"timestamp\": 1522740951, \"event_type\": \"write_post\"} {\"event_id\": \"35672\", \"timestamp\": 1522740956 ... 結語 到這邊為止成功把（偽）串流資料透過 Kinesis 存到 S3 了！為了方便之後的應用，輸出的檔案的內容格式或許還可以再改進，但資料湖的其中一個想法是 Command Query Responsibility Segregation (CQRS) ，也就是在存放資料的時候就只專心丟資料，不去在意之後資料會被以什麼方式、schema 使用，可以保證之後實際應用資料時有最大的彈性。 另外在確保資料好好地儲存在資料湖以後，我們通常會實際針對串流資料再進行一些處理 / 分析像是： 放到 Elasticsearch 並用 Kibana 做 Visualization 觸發 Lambda function 做進一步處理 使用 Athena 做 ad-hoc 分析 ... 加入 Prefix 後實際將串流資料存入 S3 時的檔案路徑 但這邊時間有限，之後有機會再來記錄資料湖之後的分析筆記。 References Youtube: Introduction to Amazon Kinesis Firehose sumologic - Kinesis Stream vs Firehose A Cloud Guru - difference betwwen Kinesis Streams and Kinesis Firehose Getting started with AWS Kinesis using Python opsclarity - Evaluating Message Brokers: Kafka vs. Kinesis vs. SQS if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/use-kinesis-streams-and-firehose-to-build-a-data-lake.html","loc":"https://leemeng.tw/use-kinesis-streams-and-firehose-to-build-a-data-lake.html"},{"title":"AWS Data Migration Service - 從 MongoDB 遷移到 Redshift","text":"同樣一份資料因應不同的使用案例，可能需要使用不同的存取方式。而針對這些不同的存取方式，我們通常需要選擇最適合的資料庫來最佳化使用者體驗。 這篇文章將簡單介紹如何使用 AWS Database Migration Service (以下簡稱 AWS DMS )來快速地達到我們的目標：將 MongoDB 資料遷移到 Redshift 上。 使用案例 舉例來說，一個電子商務網站的後端可以使用一個具有高度彈性的 NoSQL 資料庫如 MongoDB 來應對變化快速的使用者需求；而公司內部的資料科學家可以利用資料倉儲如 Redshift 來找出 business insight 。但這時候一個問題產生了：資料科學家用的資料倉儲 (例：Redshift) 的資料哪裡來？ 常見的方式是對 MongoDB 裡頭的資料定期做 ETL 以後將轉換過後的資料載入 Redshift 供分析需求。理論上在做 ETL 時要依照資料倉儲的 Data Model 重新設計 Tables (例： Star Schema )，但為了能在最短的時間將 MongoDB 上的資料轉到 Redshift 進行一些 Query，這篇文章將簡單介紹 AWS DMS 的運作方式，以及如何運用它來實際進行資料遷移所需要的步驟。 AWS DMS : 遷移（並轉換） AWS 上的資料庫 AWS DMS 基本介紹 DMS 基本上運作方式就是幫我們啟動一台 EC2 機器 (稱之為 replication instance) ，然後在上面跑 replication task(s) 。 一個 instance 上可以有多個 tasks 進行資料遷移。 instance 則分別透過 Source Endpoint / Target Endpoint 連結來源 / 目標資料庫。在後面我們會看到， endpoints 實際上就只是告訴 AWS DMS 的 replication instance 如何連結到實際的資料庫的設定罷了。在我們的例子裡頭，來源 / 目標資料庫分別對應到 MongoDB / Redshift 。 DMS 基本運作方式 : 資料遷移是由在 Replication Instance 上執行的 Replication Task 透過 endpoints 連結來源/目標資料庫完成的 基本遷移步驟 在假設來源 / 目標資料庫已經在運作的情況下，如同 AWS DMS 的 Get started, 一般會進行以下步驟來遷移資料： 建立 replication instance 確保 replication instance 能連結到來源 / 目標資料庫 定義 replication task Debugging：確保一切運作正常 以下針對每個步驟，我會紀錄一些需要注意的地方。 建立 replication instance 點擊 AWS DMS 介面的 Get started 選項會請我們建立新的 instance: 建立 Replication Instance : 注意 VPC /設定 這步驟基本上沒什麼問題， replication task 會佔用大量的 CPU 以及記憶體資源，理想上是依據需求選擇 Instance class ，不過第一次測試功能的話用預設的 t2.medium 即可。這邊值得注意的是 VPC 以及下面進階選項的 VPC Security Group(s) 設定。 如果來源 / 目標資料庫都可供公開存取的話，基本上不需要 VPC 。但一般來說我們都會有安全考量，也就是要求所有在 AWS 上的資源都要套用安全設定，則最簡單的架構是將來源資料庫、目標資料庫以及 Replication Instance 都放入同一個 VPC ，並利用 security group 設定來允許該 Instance 存取兩個資料庫。概念上此 VPC 的架構會如下 ( sg 為 Security Group 之縮寫 )： 來源 / 目標資料庫所在的 Security Group 要允許 Replication Instance 所在的 Security Group 存取 以上圖為例， Security Group sg_mongodb 以及 sg_redshift 的 Inbound Rule 要允許 sg_replicate 存取。而允許存取的 Port 則依照資料庫實際使用的 Port 設定即可 (例： MongoDB 慣用 27017； Redshift 則是 5439)。最後別忘了在建立 replication instance 的進階設定的 VPC Security Group(s) 選擇 sg_replicate 。 另外你可能已經注意到上圖的 S3 bucket 。就 replication tasks 的 log 來看， AWS DMS 在遷移資料的時候實際上會再細分為兩步驟： Replication Task 將來源資料庫的資料載出、轉換並暫存到 S3 Task 將存在 S3 的資料載入目標資料庫 雖然 ClodWatch 需要額外收費，但為了方便除錯，建議使用。在文章後面的 Debugging 我們會實際看一些例子。 確保 replication instance 能連結到來源 / 目標資料庫 上一步驟設定好以後， AWS DMS 會馬上幫我們建立一個新的 replication instance。在等待的同時我們可以開始設定資料庫的 endpoints。 設定來源 / 目標 enpoints : 在此步驟確保 Replication Instance 可以連到兩個資料庫可以減少除錯時間 這步驟基本上依照資料庫的不同，需要的輸入的項目可能不一樣。不過值得一提的是，在 建立 replication instance 的時候我們已經讓來源 / 目標資料庫以及 replication instance 都待在同個 VPC 裡頭。假設我們的 MongoDB 是運行在該 VPC 裡頭的某個 EC2 instance 之上，要允許在同個 VPC 的 replication instance 存取該 EC2 instance，我們要在 Server name 選項輸入運行 MongoDB 的 EC2 的 Private IP (上圖第一個紅框)。 MongoDB as Source Database 當 MongoDB 為來源資料庫時有一些值得注意的事情可以參考 官方文件 。以下會說明一些值得特別注意的地方。 Metadata mode Metadata mode 預設為 document （上圖第二個紅框），也就是把 MongoDB 裡頭的 json-formated 文件放到 Redshift 裡頭對應 Table 的一個 _doc 欄位。假設 MongoDB 裡有一個 users collection ，裡頭存了以下文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 將會被以下的格式載入 Redshift： _doc | --------------------------- {\"user\": \"leemeng\", \"fav .. 而這通常不是我們要的。將 metadate mode 設定為 table 模式能讓 AWS DMS 把文件裡頭的欄位扁平化後放入對應的欄位(column)： user | favorite | a.b | unnecessary_field ------------------------------------------------- leemeng | chocolate | For fun!| Don't include me! 注意到這邊有一個我們不需要遷移到 Redshift 的 unnecessary_field 。在後面的 Transformation Rules 我們會了解怎麼辦該欄位去除。 Numbers of documents to scan 而 Numbers of documents to scan 選項則讓我們決定要讓 AWS DMS 拿多少文件來決定要建立哪些欄位。如果要遷移的 MongoDB collection 的文件 schema 很常被更動（常有新鍵值）的話，建議可以讓 AWS DMS 掃描多一點文件來建立足夠的欄位。 Redshift as Target Database 如果按照 AWS DMS 的 Get started 一步一步走的話基本上沒有問題。要注意的是 Redshift 要有允許 DMS 存取的 AMI Rule，否則會出錯。 定義 replication task 在確定 replication instance 可以連線到兩個資料庫後，可以開始建立我們的 replication task： 這邊我們可以看到有三種資料遷移方式 (圖中的 Migration type)： 遷移目前 MongoDB 的資料 同上，但在遷移後之後繼續同步 MongoDB & Redshift (前提是 MongoDB 要以 Replica Set 模式執行) 只把 Task 啟動後 MongoDB 的資料變動遷移到 Redshift 這邊選擇自己的想要的遷移方式即可。接著我們要告訴 AWS DMS 想要進行遷移的 MongoDB Collections 以及在想要做的簡單轉換。兩者分別透過 Selection Rules 還有 Transformation Rules 定義。 Selection Rules Selection Rules 的用途是告訴 AWS DMS 該遷移以及不要遷移的 (MongoDB) collections 。我們可以定義一個 general rule 讓一個 task 處理某個 db 的所有 collections ；也能讓一個 task 只負責一個 collection 的遷移。後者的設定比較花時間但是彈性比較高，可以依照不同 collection 特性決定遷移的方式。 下圖是定義一個 rule 告訴 AWS DMS 遷移所有在 MongoDB 的 Collections。另外如果想要排除哪個 collection 的話就新增一個 rule 並在 Action 選擇 Exclude 。基本上想要加幾個 Selection Rules 都可以。而 Exclude Rules 的效果是在所有 Include Rules 後套用。 Selection Rules : 選擇要遷移到目標資料庫的 Tables / Collections Transformation Rules 這邊所謂的轉換並不是對欄位的實際值 (value) 進行轉換，而是針對 Table / Column 層級做 排除（不遷移該 Table / Column） 幫 Table / Column 更名、大小寫轉換或是名稱加上 prefix / postfix 這種操作。下圖是將 users collection 裡頭不需要遷移的鍵值 uncessary_field 從 Redshift 排除的 rules: 透過這個 Transformation rule，我們上面 users collection 的範例文件： { \"user\" : \"leemeng\" , \"favorite\" : \"chocolate\" , \"a\" : { \"b\" : \"For fun!\" }, \"unnecessary_field\" : \"Don't include me!\" } 就會被轉成： user | favorite | a.b ------------------------------ leemeng | chocolate | For fun! 注意 uncessary_field 不會被存到 Redshift 裡頭。 Debugging 當建立並執行一個新的 replication task 後，我們可以從 Load State 看到每個 Table 載入的狀況。 Load State 有幾種可能的值： Before Loading Full Load Table completed Table error 當出現 Table error 時，我們可以先看 log 瞭解情況： 2018-03-28T01:23:30 [TARGET_LOAD ]E: RetCode: SQL_ERROR SqlState: XX000 NativeError: 30 Message: [Amazon][Amazon Redshift] (30) Error occurred while trying to execute a query: [SQLState XX000] ERROR: Load into table 'users' failed. Check 'stl_load_errors' system table for details. [1022502] (ar_odbc_stmt.c:4406) 依照不同的錯誤、不同的目標資料庫，實際的 log 內容會有所不同。以我們目標資料庫 = Redshift 的情況下，上面的 log 告訴我們 replication task 在載入 users Table 時出錯，詳情可以參考 Redshift 的 stl_load_error Table ( 官方文件 )： SELECT * FROM pg_catalog . stl_load_errors ORDER BY starttime DESC LIMIT 1 ; 查看 Redshift 裡頭 stl_load_error Table 來除錯 就這個錯誤例子來看， err_reason 的內容告訴我們有個 memo 欄位 ( colname ) 的值太長導致沒辦法載入 Redshift。這時候可以把正在運行的 replication task 暫停，用前面提到的 Transformation Rules 來去除該欄位。而基本上其他錯誤也能用類似的方式解決。 到這邊為止大致上應該可以順利把 MongoDB 的資料載入 Redshift 了。之後想到什麼再補充。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/replicate-data-from-mongodb-to-redshift-using-aws-data-migration-service.html","loc":"https://leemeng.tw/replicate-data-from-mongodb-to-redshift-using-aws-data-migration-service.html"},{"title":"Designing Data-Intensive Applications (1) - 序言","text":"最近在拜讀 Martin Kleppmann 的 Designing Data-Intensive Applications ， 覺得受益匪淺，且我也相信透過 Feynman Technique 將學到的東西用最淺顯易懂的方式表達能幫助自己內化這些知識，遂嘗試把閱讀後的心得記錄在此。 另外在提到書內內容時都會盡量使用英文原文，不另做名詞的翻譯，以方便對照書內內容。 何謂 data-intensive applications 所謂的 data-intensive applications 如同名稱所示，專注在如何有效率地處理、儲存 密集資料 。通常一個這樣的系統的後端要用多種方式處理資料，而不是只用一個資料庫就結束了。（雖然對 end users 來說可能看起來像這樣） 舉個簡單例子，一個電子商務網頁的後端除了做為 OLTP 的 NoSQL 資料庫 (e.g., MongoDB) 以外，可能還有： 一個專門存放網頁快取的資料庫 (e.g. Redis) 給資料科學家分析用的資料倉儲 (e.g., Redshift) 處理 streaming events 的 messaging queue (e.g., Kafka) 定期將 NoSQL 資料庫的資料做 ETL 存到 資料倉儲的批次處理 (e.g., Hive jobs) 光是要把以上所列的資料庫 / 分散式系統 / 資料流 以有系統的方式組合起來就需要大量經驗，更遑論還要達到以下三個要求了： 可靠性 (reliable): 像是 zero-down time, 很短的回應時間 etc 規模性 (scalable): 即使之後資料量增加，系統也能很好地運作 維護性 (maintainable): 容易改善、新增功能的系統設計 Image Credit : 如何了解各個 data system 的優缺點並予以組合 儘管我們不可能熟悉所有資料庫以及分散式系統的細節，了解他們背後設計的核心理念、演算法以及大致上的運作方式能讓我們了解每個 data system 的特性以及優缺點，依照不同的使用案例選擇最適合的 data system 並予以組合。 何謂資料密集 書中所指的「密集」資料有以下所列的特徵（一個以上）： 大量資料 資料的（格式、 schema etc）變動速度很快 資料有複雜結構 針對「資料有複雜結構」以及「資料變動很快」這點，最為人所知的 solution 就是 NoSQL 等允許彈性 schema 的資料庫的崛起； 而針對「資料量很大」這點，則端看使用案例有各式各樣的資料庫、分散式系統。舉幾個例子： 能有效儲存大量資料的 Google BigTable 以欄 (column) 為單位儲存以壓縮大量資料的資料倉儲 Redshift Amazon 的 Single-leader Replication - DynamoDB 專門處理 realtime streaming data 的 Kafka, RabbitMQ etc. 如同前述，以上提到的系統依照它們想要解決的問題的特性，背後都會有一些假設以及 trade-off 。了解這些背後的原理可以讓我們了解哪些工具在什麼時候最 powerful 。 這本書主要分成三部分來闡述，抓到大方向會比較容易閱讀： 針對單一機器上的資料，有哪些常用的資料儲存/處理方法 類似前一部份，闡述針對分散式系統的資料儲存/處理方法 資料密集型應用：如何將多個 data systems 組合起來 一句話總結 在資料密集的時代，我們的最終目標在於如何將各式各樣的 data systems 以有系統的方式「組合」起來，以建立一個可靠、具規模性以及維護性的系統。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/designing-data-intensive-applications-1-preface.html","loc":"https://leemeng.tw/designing-data-intensive-applications-1-preface.html"},{"title":"Google Data Studio 基礎","text":"Google Data Studio 是 Google 推出的一個 Dashboard / Reporting 的服務，讓我們可以利用多種 連結器 將儲存在如 Google Analytics、 Google 試算表及 Google BigQuery 等特定資料來源的資料做出漂亮的 visualization ，用資料講故事而不用自己設計 UI。公司內部雖然有自己的 dashboards 不過想說多試一些方案沒有壞處，而且現在 Data Studio 還是 Beta 版本，雖然介面是中文，說明文件還只有英文，想說把學到的一些技巧以及使用心得記錄下來。 將 Google 試算表的資料可視化 為了快速展示 Data Studio 的功能，我們將使用 政府資料開放平臺 上由交通部觀光局提供的 105年來台旅客性別統計 資料。將 CSV 檔案下載下來，稍微簡化格式後上傳到 Google 試算表以當作報表的資料來源。下圖是簡化後的資料： 資料來源 : 2016年來台旅客性別統計 每一列代表某地區 / 國家的訪台人數以及男女比 條件欄位應用 條件欄位 讓我們可以針對試算表裡頭每一列做 IF ELSE 判斷，依照判斷結果給予不同的值。現在假設我們想知道有多少國家的男性遊客過半數，可以使用簡易的評量表來計算： 訪台男性遊客過半國家佔全部國家的比例 我們發現高達八成的國家（有些是區域）的訪台男性遊客較女性為多。我們可以調查其他國家的訪客性別比，看是不是只有台灣有此現象。要產生分母的「國家數」很直覺，我們只要新增一個欄位並計算有幾個國家即可： 新增一個名為「國家數」的欄位 但要計算分子的「男性遊客過半國家數」就稍微 tricky 了。我們想做的是，針對每一國家（每一列），只有在該國訪台男性遊客百分比過半（超過 50%)的時候才會被納入結果。而 Data Studio 的 條件欄位 就是專門針對這種情況設計的。 使用 CASE 語法對每一列做 IF-ELSE 判斷 上面的公式用白話來說就是： 針對每一列的國家，看它的「男性百分比」欄位的值有沒有大於50。有的話值為1，否則為0。在針對每列做完條件判斷以後再把所有 1 加起來，就等於符合條件的國家數。 篩選器（filter）應用 根據上個分析，我們知道女性遊客過半的國家只佔 20%。假設我們想確切知道是哪些國家的女性遊客過半，可以從女性百分比最高的國家開始列出男女比： 訪台女性遊客過半國家 我們發現女性遊客過半的都是亞洲國家，或許我們可以簡單解釋成這些國家與台灣的距離短，適合女性遊客拜訪。而為了讓圖表易讀，上面這張組合圖額外建立一個篩選器來過濾掉男性遊客比女性多的國家： 新增一個篩選器以過濾男性遊客比例較高的國家 註：一般的長條圖可以直接透過設定限制長條圖數目 維度 VS 指標 在 Data Studio 裡頭，了解 維度跟指標的差異 很重要。 以我們現在的資料集為例，每一列就是一筆紀錄（record），每一行則是一個欄位。每個欄位則是維度或指標。 指標（Metric，底下藍色） 數值型欄位，有經過「匯總」，負責 quantify 資料 如「國家數」、「總人數」 維度（Dimension，底下綠色） 分類型欄位，負責 qualify 資料 如「國家」、「居住地」 fx 則代表是額外利用公式建立的欄位 像我們前面定義的「男性遊客過半國家數」欄位因為有經過 SUM 公式匯總成為一個數值，因此為一個指標（藍）。而如果我們透過 CASE 語法新定義一個「男性過半」欄位如下： 此欄位沒有經過匯總因此被視為維度，在上一張圖被標為綠色。因此一句話總結維度跟指標的功能就是： 維度負責「描述」資料； 指標則負責「衡量」資料。 資料透視表 (Pivot Table) 資料透視表很適合拿來看在不同條件下某個指標的表現。下圖是一個依照 居住地 國家 兩個維度計算「男性人數」指標的資料透視表： 依照 官方文件 有幾點值得注意： 資料透視表最多處理 50,000 筆資料，為了避免 scan 資料太花時間，可以額外建立一些篩選器 subset 資料 列維度跟欄維度最多可以分別設定 2 個維度（上例列欄各設定 1 個維度） 限制 可能因為還處在 beta 版本，在這篇文章寫的時候（2018/03）試用了一陣子發現 Data Studio 也有一些使用案例沒有辦法做到，像是： 篩選器（filter）只能設定像是「欄位 C 大於 X」這種條件，而不能做「當欄位 C1 > 欄位 C2」這種欄位間的比較。 同上，條件欄位也只能設定像是「欄位 C 大於某固定值 X」的條件 資料透視表包含的資料稍多 (> 2000筆)就開始變慢 .. 實戰演練 這篇文章用的報表連結在 此 ，可以自己試試不同 visualization。有任何 feedback 也歡迎聯絡。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/google-data-studio-basics.html","loc":"https://leemeng.tw/google-data-studio-basics.html"},{"title":"Pelican 實戰手冊(主題篇)","text":"有些人可能已經注意到這個部落格是用 Pelican 所寫成並且 host 在 Github 上。這篇主要紀錄如何使用 Jinja2 自訂主題。 Pelican 是一個用 Python 寫的靜態網頁生成器, 可以幫我們把 reStructedText, Markdown file 甚至 Jupyer notebook 轉成靜態的 HTML 檔案。 靜態網頁的好處就是我們不需要一個 web server 或者是資料庫來管理內容, 可以把 HTML 檔案 host 在想要的地方，比方說 Github Pages 。用 Pelican 官網一句來介紹的話就是： Pelican is a static site generator, written in Python, that requires no database or server-side logic. - Pelican Blog Google 一下你會發現除了 Pelican 以外還有很多其他像是 Jekyll, Hexo 等 靜態網頁生成器 。 之所以會選擇 Pelican 是因為以下幾點： Pelican 是用 Python 寫的，讓 Python 開發者（我）很容易客製化 可以把 jupyter notebook 轉成 HTML ，這對每天寫一堆 notebooks 的資料科學家很友善 主題是用強大的 Jinja2 模組引擎建立，可以用前人寫好的 主題 或是自己寫 templates，自由度很高，也是本篇重點。 如果你的需求類似而且想要自己架一個部落格，可以現在就跳入 Pelican Quickstart ，有問題再回來看這篇。 Jinja2 是 Python 知名的模組引擎 (templating engine)，可以有系統地產生 HTML，很常出現在 Flask 或是 Django Apps 裡頭。以下介紹在建立 Pelican blog 時常用到的功能。 再利用 HTML 區塊 比方說我們可以建立一個汎用的 template base.html 來定義整個部落格共用的資訊，像是 header 裡頭要 import 的 css / favicon 等等： <!DOCTYPE html> < html lang = \"en\" > < head > {% block head %} < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> {% endblock head %} </ head > < body > {% block content %} < p > 部落格內容 </ p > {% endblock content %} </ body > 注意到上面的 {% block head %} jinja2 語法。會在多個 HTML 檔案重複使用的部分我們可以用 {% block BLOCKNAME %} 以及 {% endblock BLOCKNAME %} 包起來，然後在獨立顯示一篇文章的 article.html 裡頭我們可以定義： { % extends \"base.html\" % } { % block head % } {{ super () }} < title > 文章標題 </ title > { % endblock head % } < body > { % block content % } < p > 文章內容 </ p > { % endblock content % } </ body > 上面的 code 基本上是告訴 jinja2 article.html 要繼承 base.html 的所有內容，而在 head block 除了用 {{ super() }} 繼承 base.html 的內容以外，在下面再追加新的內容。而 content block 則是完全取代。 因此最後 article.html 會被渲染成： <!DOCTYPE html> < html lang = \"en\" > < head > < link rel = \"stylesheet\" type = \"text/css\" href = \"css/vendor.css\" > < link rel = \"icon\" href = \"images/favicon.ico\" type = \"image/x-icon\" /> < title > 文章標題 </ title > </ head > < body > < p > 文章內容 </ p > </ body > 為當前文章取得前/後一篇文章連結 Pagination 範例: 顯示前後文章連結 依照主題不同，有些主題可能文章頁面裡頭並沒有提供前一篇/後一篇文章的連結。要像上圖為每一篇文章取得前後文章的連結，可以在 article.html 裡存取 articles Variable 並使用 jinja2 namespace 來取得前後文章( namespace 要在 jinja 2.10+ 以後才能使用) { # get prev- and next-article for pagination #} { % set ns = namespace ( found = false , prev = None , next = None ) % } { % for a in articles % } { # 要使用 break 要安裝 extension, 最佳化效率可省略 #} { %- if ns . found % }{ % break % }{ % endif % } { # 假設文章標題不會重複, unique #} { % if a . title == article . title % } { % set ns . found = true % } { % set ns . prev = loop . previtem % } { % set ns . next = loop . nextitem % } { % endif % } { % endfor % } 上面的 code 會 iterate 所有文章，當遇到當前文章的時候利用 loop.previtem 以及 loop.nextitem 把前後文章記下來。 jinja2 預設是無法在 loop 裡頭改變變數的值 ，但使用 namespace 即可。 接著就能利用剛剛取得的前後 article 物件來渲染前後連結： {# 方便起見的 assignment %} {% set prev_article = ns.prev %} {% set next_article = ns.next %} {% if prev_article %} < div > < a href = \"prev_article.url\" rel = \"prev\" > < span > Previous Post </ span > {{ prev_article.title }} </ a > </ div > {% endif %} {% if next_article %} < div > < a href = \"next_article.url\" rel = \"next\" > < span > Next Post </ span > {{ next_article.title }} </ a > </ div > {% endif %} 傳參數給子 template 有時候多個 templates 會使用類似的 HTML，像是當首頁 index.html 以及部落格 blog.html 都用相同格式渲染最新幾篇文章時，我們可以定義一個 article_entries.html 如下： { # 簡化版 #} { % for article in articles % } < article class = \"col-block\" > < a href = \"{{ SITEURL }}/{{ article.url }}\" > {{ article . title }} </ a > < p > {{ article . summary }} </ p > </ article > { % endfor % } 注意這時候如果直接在 index.html 使用 { % include 'article_entries.html' % } 是會出現 錯誤 的。理由是被 include 的 article_entries.html 看不到定義在 index.html 的 articles 變數。 解決方法是在 index.html 裡透過 {% with %} 語法定義一個 scope： { # 選擇前五篇文章來渲染 #} { % set articles_to_show = articles_page . object_list [ 5 ] % } { # 定義 scope #} { % with articles = articles_to_show % } { % include 'article_entries.html' % } { % endwith % } 使用 with 的好處是可以把子 template article_entries.html 當作 function 來使用，我們可以依照母 template 的需要，傳進想要渲染的文章即可。 Reference Jinja2 Extension if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/build-a-pelican-powered-blog-like-a-pro.html","loc":"https://leemeng.tw/build-a-pelican-powered-blog-like-a-pro.html"},{"title":"BeautifulSoup 筆記","text":"Beautifulsoup 是一個可以幫助我們 parse HTML 的 lib, 這篇主要紀錄使用 beautifulsoup 時常用的指令。 安裝 pip install beautifulsoup4 下載一個網頁並爬出特定內容 這邊假設我們想要把維基百科上的 「國家區域代碼」 的表格爬下來，並轉成一個 Pandas 的 Dataframe： 取得某個頁面的 HTML 字串 import urllib from bs4 import BeautifulSoup import pandas as pd html = urllib . request . urlopen ( \"https://zh.wikipedia.org/zh-tw/ISO_3166-1\" ) . read () soup = BeautifulSoup ( html , 'html.parser' ) 利用 class 從該 HTML 裡取得特定表格 table = soup . find ( 'table' , { 'class' : 'wikitable sortable' }) 產生欄位名稱 columns = [ th . text . replace ( ' \\n ' , '' ) for th in table . find ( 'tr' ) . find_all ( 'th' )] columns ['英文短名稱', '二位代碼', '三位代碼', '數字代碼', 'ISO 3166-2', '中文名稱', '獨立主權'] 產生每個國家的對應資料 trs = table . find_all ( 'tr' )[ 1 :] rows = list () for tr in trs : rows . append ([ td . text . replace ( ' \\n ' , '' ) . replace ( ' \\xa0 ' , '' ) for td in tr . find_all ( 'td' )]) rows [: 5 ] [['Afghanistan', 'AF', 'AFG', '004', 'ISO 3166-2:AF', '阿富汗', '是'], ['Åland Islands', 'AX', 'ALA', '248', 'ISO 3166-2:AX', '奧蘭', '否'], ['Albania', 'AL', 'ALB', '008', 'ISO 3166-2:AL', '阿爾巴尼亞', '是'], ['Algeria', 'DZ', 'DZA', '012', 'ISO 3166-2:DZ', '阿爾及利亞', '是'], ['American Samoa', 'AS', 'ASM', '016', 'ISO 3166-2:AS', '美屬薩摩亞', '否']] 產生 Dataframe df = pd . DataFrame ( data = rows , columns = columns ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 英文短名稱 二位代碼 三位代碼 數字代碼 ISO 3166-2 中文名稱 獨立主權 0 Afghanistan AF AFG 004 ISO 3166-2:AF 阿富汗 是 1 Åland Islands AX ALA 248 ISO 3166-2:AX 奧蘭 否 2 Albania AL ALB 008 ISO 3166-2:AL 阿爾巴尼亞 是 3 Algeria DZ DZA 012 ISO 3166-2:DZ 阿爾及利亞 是 4 American Samoa AS ASM 016 ISO 3166-2:AS 美屬薩摩亞 否 找出特定 HTML 物件 假設我們有一個字串代表一個表格： html = \"\"\"<div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div>\"\"\" 渲染成 HTML: x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 實際 HTML 架構： < div > < table border = \"1\" class = \"dataframe\" > < thead > < tr style = \"text-align: right;\" > < th ></ th > < th > x </ th > < th > y </ th > </ tr > </ thead > < tbody > < tr > < th > 0 </ th > < td > -2.863752 </ td > < td > -1.066424 </ td > </ tr > < tr > < th > 1 </ th > < td > -0.779238 </ td > < td > 0.862169 </ td > </ tr > </ tbody > </ table > </ div > 利用 BeautifulSoup 物件 parse HTML: from bs4 import BeautifulSoup soup = BeautifulSoup ( html , 'html.parser' ) soup <div><table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> 找到第一個符合條件的 table 標籤 table = soup . find ( 'table' , { 'class' : 'dataframe' }) table <table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table> 設定新屬性 / class 因為這時候我們取出來的 table 物件是 reference 到 soup 裡頭對應的物件, 只要直接改變對應的 attr 就會直接反映結果到 soup 物件: table [ 'class' ] = table [ 'class' ] + [ 'table' , 'table-striped' , 'table-responsive' ] soup <div><table border=\"1\" class=\"dataframe table table-striped table-responsive\"><thead><tr style=\"text-align:right;\"><th></th><th>x</th><th>y</th></tr></thead><tbody><tr><th>0</th><td>-2.863752</td><td>-1.066424</td></tr><tr><th>1</th><td>-0.779238</td><td>0.862169</td></tr></tbody></table></div> Iterate 標籤裡頭的子標籤 for c in table . children : print ( f ' {c.name} in {table.name} ' ) thead in table tbody in table 移除標籤 這邊假設我們要移除表格裡頭第一行的值 ( 第2個 tr 標籤 ), 可以對要移除的標籤物件使用 extract() func. x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 for i , tr in enumerate ( soup . findAll ( 'tr' )): if i == 1 : tr . extract () x y 1 -0.779238 0.862169 建立新標籤 假設我們想要建立一個新的 blockquote 標籤，並加入一些文字： text = 'I love BeautifulSoup!' blockquote = soup . new_tag ( 'blockquote' ) blockquote . append ( text ) blockquote <blockquote>I love BeautifulSoup!</blockquote> if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/beautifulsoup-cheat-sheet.html","loc":"https://leemeng.tw/beautifulsoup-cheat-sheet.html"},{"title":"Seaborn 筆記","text":"這篇記錄我在使用 seaborn 做資料分析還有 visualization 時常用的 code. 一般慣例會把 seaborn 更名成 sns for reference. % matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt 基本設定 這邊值得注意的是要調整的參數要一次全部設定, 用好幾次 set() 的話只有最後一次的 set() 的結果會被保留 sns . set ( font = 'IPAPMincho' , font_scale = 1.8 ) Histogram data = np . random . randn ( 1000 ) data [: 10 ] array([-0.53267554, 0.03851161, -0.16072742, -0.70889663, 0.23085979, -1.61295347, -0.46508874, 0.60112507, 0.42017249, -0.73656917]) seaborn 是建立在 matplotlib 之上, 因此 matplotlib 也可以直接拿來跟 seaborn 產生的圖互動 plt . figure ( figsize = ( 10 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . title ( 'Defualt style with kde' ) sns . distplot ( data , kde = True ); plt . subplot ( 1 , 2 , 2 ) sns . set_style ( 'dark' ) plt . title ( 'Dark style without kde' ); sns . distplot ( data , kde = False ); Scatter plot df = pd . DataFrame ({ 'x' : np . random . randn ( 100 ), 'y' : np . random . randn ( 100 )}) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -2.863752 -1.066424 1 -0.779238 0.862169 2 0.016786 -0.016519 3 0.948504 0.298314 4 2.029428 1.211997 要使用 seaborn 初始設定就再呼叫一次 set() sns . set () 注意點： 一般用lmplot畫, 然後設定 fit_reg=False 就可以讓 regression line 消失. 有時候有沒有那條線影響圖很大 一樣先 x , 再 y for fit_reg in [ True , False ]: sns . lmplot ( 'x' , 'y' , data = df , fit_reg = fit_reg , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) title = 'Show regression line' if fit_reg else 'Without regression line' plt . title ( title ) 想要將兩個 lmplot 並排 render 可以參考這個 stackoverflow 答案 . Correlation matrix / Heatmap df = pd . DataFrame ({ 'x1' : np . random . randn ( 100 ), 'x2' : np . random . randn ( 100 ), 'x3' : np . random . randn ( 100 ) }) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 0 1.269566 0.349083 -0.000743 1 -1.634587 0.072568 0.042596 2 -0.581238 -0.337935 -0.412084 3 -0.080881 -1.376481 1.361046 4 -0.609886 -1.061285 0.265788 這邊利用 pandas 本身的 corr() 計算 correlation matrix 然後使用 seaborn 做 vis. corr = df . astype ( float ) . corr () corr .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x1 1.000000 -0.034731 0.032407 x2 -0.034731 1.000000 -0.192169 x3 0.032407 -0.192169 1.000000 sns . set ( font_scale = 1.5 ) sns . heatmap ( corr , cmap = 'Blues' , annot = True , annot_kws = { \"size\" : 15 }, xticklabels = corr . columns . values , yticklabels = corr . columns . values ); References API References if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/seaborn-cheat-sheet.html","loc":"https://leemeng.tw/seaborn-cheat-sheet.html"},{"title":"SQLite 筆記","text":"Table of Contents Prettier output 調整每一個 column 寬度 在 sqlite3 shell 裡清空畫面 使用 SQL script 建立 tables 顯示目前的 tables 顯示 table schema 顯示 indexes 這篇主要紀錄使用 SQLite shell 下 SQL Query 的指令。基本上在 shell 裡頭都是用 dot-command, 使用 .help 可以顯示所有可用的指令. Prettier output 在 command-line program 裡頭使用的 response format .mode column .headers on Example output Code Name Price Manufacturer ---------- ---------- ---------- ------------ 7 CD drive 90 2 9 Toner cart 66 3 調整每一個 column 寬度 .width 5 18 15 缺點是不同的 tables, 不同的 columns 需要的寬度不同, 要自己調整 要重置設定: .width 0 在 sqlite3 shell 裡清空畫面 要看 OS 決定實際的 shell command .shell clear 除了 clear 以外, 其他 shell command都能使用, e.g., .shell cd 使用 SQL script 建立 tables 比方我們有一個 create_tables.sql 內容是： CREATE TABLE Departments ( Code INTEGER PRIMARY KEY, Name varchar(255) NOT NULL , Budget decimal NOT NULL ); INSERT INTO Departments(Code,Name,Budget) VALUES(14,'IT',65000); 我們可以用 .read dot-command 在 shell 跑該 script 建立 Department table: .read create_tables.sql 顯示目前的 tables .tables 顯示 table schema .schema <TABLE_NAME> 顯示 indexes .indexes 在 Table T 的 Column C 建立 index CREATE INDEX <INDEX_NAME> ON T(C); 砍掉 index DROP INDEX <INDEX_NAME> if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/sqlite-note.html","loc":"https://leemeng.tw/sqlite-note.html"},{"title":"Find Word Semantic by Using Word2vec in TensorFlow","text":"The goal of this assignment is to train a Word2Vec skip-gram model over Text8 data using Tensorflow. Word2vec is a kind of vector space model (VSM) in natural language processing (NLP) where the core assumption/intuition is that words that appear in similar 'context' share similar meaning and they should be near in the vector space. So what word2vec trying to do is to find a vector representation (embedding) for each word in our training corpus where words with similar meanings are near in the vector space. Figure 1 : words' representation in 2D vector space Unlike supervised learning, we don't have labels that tell us 'kitten' = 'cat'. So how do we train a model that will learn the relationship between these two words? Recap the assumption mentioned before, words with similar meaning tend to appear in similar context. Because 'kitten' and 'cat' appear in similar context, if we can train a model to predict the context of the target word 'cat' and 'kitten' respectively, model should learn a similar representation for both 'kitten' and 'cat' because they produce similar context. Figure 2 : Training data generated from target word and context As shown above, for each words like 'cat' in raw text, we will treat them as target word and the words surrounding it as context and construct the training instances (x, y) where x is target word and y is one of the word in context. And the definition of 'context' is decided by the parameter 'window size'. First, let's load the text data and build the training data in order to train our model. Libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. % matplotlib inline from __future__ import print_function import collections import os import math import random import zipfile import numpy as np import tensorflow as tf # from matplotlib import pylab # use pyplot instead import matplotlib.pyplot as plt from six.moves import range from six.moves.urllib.request import urlretrieve from sklearn.manifold import TSNE from tqdm import tnrange plt . style . use ( 'ggplot' ) Raw text data Download / load data Download the data from the source website if necessary. will store the zip file in the 'datasets' subdirectory. url = 'http://mattmahoney.net/dc/' def maybe_download ( filename , expected_bytes ): \"\"\"Download a file into 'datasets' sub-directory if not present, and make sure it's the right size. \"\"\" rel_path = 'datasets/ {} ' . format ( filename ) # if file in not found, download it if not os . path . exists ( rel_path ): filename , _ = urlretrieve ( url + filename , rel_path ) statinfo = os . stat ( rel_path ) if statinfo . st_size == expected_bytes : print ( 'Found and verified {} . size: {} ' . format ( rel_path , statinfo . st_size )) else : print ( statinfo . st_size ) raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) return rel_path filename = maybe_download ( 'text8.zip' , 31344016 ) Found and verified datasets/text8.zip. size: 31344016 Turn data into words Read the data into a string. def read_data ( filename ): \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\" with zipfile . ZipFile ( filename ) as f : data = tf . compat . as_str ( f . read ( f . namelist ()[ 0 ])) . split () return data words = read_data ( filename ) print ( 'Data size %d ' % len ( words )) Data size 17005207 Look into text corpus The 'data size' above mean how many words we have in the data. That is, there are about 17 millions words! Let's show some parts of the text data to make some sense of it. Some phrases which include word 'cat' with window size = '2'. cats = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'cat' or word == 'cats' ] print ( ' \\n ' . join ( cats [: 5 ])) del cats the cartoon cat garfield would amount of cats that roam politicians autodidacts cat lovers firearm and activists cat lovers epistemologists force australia cat six two Some phrases which include word 'kitten' with window size = '2'. kitten = [ ' ' . join ( words [ idx - 2 : idx + 3 ]) for idx , word in enumerate ( words ) if word == 'kitten' ] print ( ' \\n ' . join ( kitten [: 5 ])) del kitten put the kitten nermal in s sex kitten in the of tom kitten one nine as a kitten rudolph grey called a kitten which is Create training data In order to let TensorFlow make use of the text corpus, we have to transform the text corpus into sequence of numbers. The way to achieve this to build a dictionary which map every word to a unique number and use that dictionary to transform the corpus into number-based data. Figure 3 : Build dictionary and turn text into numbers Notice that some rare words may appear very few times in the entire text corpus. We may want to exclude these terms to keep our dictionary in a reasonable size. In order to do this, we will build the dictionary and view these terms as UNK tokens. UNK means unknown word that doesn't exist in the vocabulary set and the default number of a UNK in dictionary is 0 as shown above. Decide dictionary size Depend on the size of the vocabulary, we will construct a dictionary for top vocabulary_size - 1 common words. For example, if the vocabulary_size = 50000 , we will first count the frequencies of every word appeared in the text corpus and put the most common 49,999 terms into our vocabulary and make the rest of words as UNK token. ( thus the 50,000 th term in the vocabulary). vocabulary_size = 50000 Build dictionary and transform text corpus into sequence of numbers def build_dataset ( words ): \"\"\" Build training data for word2vec from a string including sequences of words divided by spaces. Parameters: ----------- words: a string with every word devided by spaces Returns: -------- dictionary: a dict with word as key and a unique number(index) as their value. dictionary[word] = idx reverse_dictionary: a dict with index as key and the corresponding word as value. reverse_dictionary[idx] = word counts: a list contain tuples (word, frequency) sorted descendingly by frequency while use ('UNK', unk_count) as first tuple. data: a list contain indices of the original words in the parameters 'words'. \"\"\" # count term frequencies and choose the most frequent # terms of vocabulary_size count = [[ 'UNK' , - 1 ]] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 )) dictionary = dict () # index term by their frequency. while UKN is indexed as 0, # the term with most frequencies is indexed as 1, the term with 2th frequencies # is indexed as 2, ... for word , _ in count : dictionary [ word ] = len ( dictionary ) # turn the text corpus into a sequence of number where each number is the # index of the original term in 'dictionary' dict and mark those UNK's number # as 0 which indicate that they're unknown words data = list () unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data . append ( index ) # update UNK's count in corpus count [ 0 ][ 1 ] = unk_count # create reverse dict to enable lookup the original word by their index reverse_dictionary = dict ( zip ( dictionary . values (), dictionary . keys ())) return data , count , dictionary , reverse_dictionary data , count , dictionary , reverse_dictionary = build_dataset ( words ) print ( 'Most common words (+UNK) in text corpus: \\n {} ' . format ( count [: 5 ])) Most common words (+UNK) in text corpus: [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)] See transformed text corpus sample_idx = 1000 print ( '\" {} \" \\n\\n was transformed into \\n\\n \" {} \"' \\ . format ( ' ' . join ( words [ sample_idx : sample_idx + 10 ]), ' ' . join ([ str ( i ) for i in data [ sample_idx : sample_idx + 10 ]]))) del words # Hint to reduce memory. \"american individualist anarchism benjamin tucker in one eight two five\" was transformed into \"64 10276 5234 3248 9615 5 4 13 10 16\" Function to generate a training batch for the skip-gram model. As usual, we will use mini-batch GD to update our model's parameters. Other than batch_size, we also have to decide the range of context surrounding target word (skip_window) and how many training instances are we going to create from a single (target, context) pair. Figure 4 : Build mini-batches by different num_skips # global variable to randomize mini-batch data_index = 0 def generate_batch ( batch_size , num_skips , skip_window ): \"\"\" Generate a mini-batch containing (target_word, context) pairs of `batch_size`. Parameters: ----------- batch_size: mini_batch's size, typically 16 <= batch_size <= 512 num_skips: how many times to reuse an input/target word to generate a label. skip_window: how many words to consider left and right. Returns: -------- batch: a list of target words labels: a list of context words corresponding to target words in batch \"\"\" global data_index assert batch_size % num_skips == 0 assert num_skips <= 2 * skip_window batch = np . ndarray ( shape = ( batch_size ), dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ), dtype = np . int32 ) # initialize first (target, context) sequence # = [ skip_window target skip_window ] span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ): buffer . append ( data [ data_index ]) data_index = ( data_index + 1 ) % len ( data ) # for every target word, for i in range ( batch_size // num_skips ): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] # generate #num_skips of training instances for j in range ( num_skips ): # randomly choose a context word that hasn't been chosen yet # exclude target word by default while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] # shift to next (target, context) sequence buffer . append ( data [ data_index ]) # randomize the start location of every mini-batch # by adding one offset data_index = ( data_index + 1 ) % len ( data ) return batch , labels print ( 'data: \" {} \"' . format ( ' ' . join ([ reverse_dictionary [ di ] for di in data [: 8 ]]))) for num_skips , skip_window in [( 2 , 1 ), ( 4 , 2 )]: data_index = 0 batch , labels = generate_batch ( batch_size = 8 , num_skips = num_skips , skip_window = skip_window ) print ( ' \\n (target, context) with num_skips = %d and skip_window = %d :' % ( num_skips , skip_window )) print ( ' {} ' . format ( ' \\n ' . join ( [ str (( reverse_dictionary [ t ], reverse_dictionary [ c ])) \\ for t , c in zip ( batch , labels . reshape ( 8 ))]))) data: \"anarchism originated as a term of abuse first\" (target, context) with num_skips = 2 and skip_window = 1: ('originated', 'anarchism') ('originated', 'as') ('as', 'originated') ('as', 'a') ('a', 'term') ('a', 'as') ('term', 'of') ('term', 'a') (target, context) with num_skips = 4 and skip_window = 2: ('as', 'term') ('as', 'anarchism') ('as', 'a') ('as', 'originated') ('a', 'as') ('a', 'originated') ('a', 'term') ('a', 'of') Word2Vec skip-gram model. Figure 5 : Word2vec model Computation graph batch_size = 128 embedding_size = 128 # Dimension of the embedding vector. skip_window = 1 # How many words to consider left and right. num_skips = 2 # How many times to reuse an input to generate a label. # We pick a random validation set to sample nearest neighbors. here we limit the # validation samples to the words that have a low numeric ID, which by # construction are also the most frequent. valid_size = 16 # Random set of words to evaluate similarity on. valid_window = 100 # Only pick dev samples in the head of the distribution. valid_examples = np . array ( random . sample ( range ( valid_window ), valid_size )) num_sampled = 64 # Number of negative examples to sample. graph = tf . Graph () with graph . as_default (), tf . device ( '/cpu:0' ): # Input data. train_dataset = tf . placeholder ( tf . int32 , shape = [ batch_size ]) train_labels = tf . placeholder ( tf . int32 , shape = [ batch_size , 1 ]) valid_dataset = tf . constant ( valid_examples , dtype = tf . int32 ) # Variables. embeddings = tf . Variable ( tf . random_uniform ([ vocabulary_size , embedding_size ], - 1.0 , 1.0 )) softmax_weights = tf . Variable ( tf . truncated_normal ( [ vocabulary_size , embedding_size ], stddev = 1.0 / math . sqrt ( embedding_size ))) softmax_biases = tf . Variable ( tf . zeros ([ vocabulary_size ])) # Model. # Look up embeddings for inputs. embed = tf . nn . embedding_lookup ( embeddings , train_dataset ) # Compute the softmax loss, using a sample of the negative labels each time. # this is how we speed up training phase loss = tf . reduce_mean ( tf . nn . sampled_softmax_loss ( weights = softmax_weights , biases = softmax_biases , inputs = embed , labels = train_labels , num_sampled = num_sampled , num_classes = vocabulary_size )) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # This is because the embeddings are defined as a variable quantity and the # optimizer's `minimize` method will by default modify all variable quantities # that contribute to the tensor it is passed. # See docs on `tf.train.Optimizer.minimize()` for more details. optimizer = tf . train . AdagradOptimizer ( 1.0 ) . minimize ( loss ) # Compute the similarity between minibatch examples and all embeddings. # We use the cosine distance: norm = tf . sqrt ( tf . reduce_sum ( tf . square ( embeddings ), 1 , keep_dims = True )) normalized_embeddings = embeddings / norm valid_embeddings = tf . nn . embedding_lookup ( normalized_embeddings , valid_dataset ) similarity = tf . matmul ( valid_embeddings , tf . transpose ( normalized_embeddings )) Train the model num_steps = 100001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( 'Initialized' ) average_loss = 0 for step in tnrange ( num_steps ): batch_data , batch_labels = generate_batch ( batch_size , num_skips , skip_window ) feed_dict = { train_dataset : batch_data , train_labels : batch_labels } _ , l = session . run ([ optimizer , loss ], feed_dict = feed_dict ) average_loss += l if step % 2000 == 0 : if step > 0 : average_loss = average_loss / 2000 # The average loss is an estimate of the loss over the last 2000 batches. print ( 'Average loss at step %d : %f ' % ( step , average_loss )) average_loss = 0 # note that this is expensive (~20% slowdown if computed every 500 steps) if step % 10000 == 0 : sim = similarity . eval () for i in range ( valid_size ): valid_word = reverse_dictionary [ valid_examples [ i ]] top_k = 8 # number of nearest neighbors nearest = ( - sim [ i , :]) . argsort ()[ 1 : top_k + 1 ] log = 'Nearest to %s :' % valid_word for k in range ( top_k ): close_word = reverse_dictionary [ nearest [ k ]] log = ' %s %s ,' % ( log , close_word ) print ( log ) final_embeddings = normalized_embeddings . eval () Initialized var element = $('#8a33befa-98fc-4987-9075-6d457eec4e61'); {\"model_id\": \"479bac87e8864793a2657306231c3b2a\", \"version_major\": 2, \"version_minor\": 0} Average loss at step 0: 7.942154 Nearest to up: refit, airmen, unexplored, scharnhorst, histones, envelopes, wanna, wick, Nearest to many: herbivorous, kazimierz, surgeries, juliette, merovingian, christadelphians, experimentation, strauss, Nearest to people: chicken, zulu, glaucus, temporarily, groundbreaking, mapuche, varnish, vinod, Nearest to some: platelets, mauritania, anzus, soaemias, plankton, orcs, cegep, danzig, Nearest to was: blotter, continuously, gulls, lineages, turbines, cardano, honky, gcb, Nearest to and: haller, potion, rickshaw, fares, soldier, mariam, sponsor, irs, Nearest to in: wrench, atwood, boys, fermat, uhf, midrash, hallucinogens, deflate, Nearest to be: infertility, olaf, faramir, dxf, latino, clem, aia, cation, Nearest to all: stripe, abbreviation, nationalised, maoi, hermann, three, lara, jay, Nearest to than: antiderivatives, deduce, brainiac, wry, propel, requested, selangor, transposed, Nearest to system: rmi, devine, elite, deism, transgressions, bows, primer, undoubtedly, Nearest to were: ange, oh, widest, fidel, kristallnacht, predicates, coprocessor, surnames, Nearest to see: astounding, equilateral, concubines, syllables, lackluster, transoxiana, kasparov, smooth, Nearest to that: figurative, cluster, wendell, horch, cards, prinz, ellesmere, maximilian, Nearest to s: endosperm, saberhagen, pipelined, recantation, calcium, infrastructural, manchuria, spears, Nearest to will: suppressive, rotations, predefined, dessau, confederation, hovered, radiohead, apicomplexa, Average loss at step 2000: 4.364343 Average loss at step 4000: 3.857699 Average loss at step 6000: 3.785119 Average loss at step 8000: 3.686496 Average loss at step 10000: 3.616513 Nearest to up: scharnhorst, refit, arabs, concubines, zeeland, airmen, calibers, diem, Nearest to many: some, kazimierz, herbivorous, timber, poorly, revitalize, psychoanalyst, studies, Nearest to people: ach, la, semester, several, glaucus, gediminas, mechanism, yupik, Nearest to some: many, overbearing, firmer, oracles, dissonant, limes, soaemias, cpu, Nearest to was: is, has, had, were, by, been, be, are, Nearest to and: or, s, but, scriptores, who, of, in, alans, Nearest to in: on, at, of, from, with, between, by, during, Nearest to be: have, was, is, do, receiving, latino, subclass, chisel, Nearest to all: marketed, maoi, abbreviation, stripe, protectors, nationalised, lara, chadic, Nearest to than: haer, deduce, caucus, omniglot, casings, propel, selangor, softer, Nearest to system: rmi, devine, elite, primer, hcl, try, deism, bows, Nearest to were: are, was, tiu, have, adware, ragged, popularizer, macrobiotic, Nearest to see: william, concubines, astounding, transoxiana, but, bullough, pigeon, artistic, Nearest to that: which, ellesmere, she, it, pectoral, also, who, breaches, Nearest to s: and, vu, his, the, was, carlist, tanoana, chulainn, Nearest to will: may, would, jews, barbarism, predefined, could, confederation, receiving, Average loss at step 12000: 3.606183 Average loss at step 14000: 3.572916 Average loss at step 16000: 3.410224 Average loss at step 18000: 3.453891 Average loss at step 20000: 3.539739 Nearest to up: unvoiced, scharnhorst, zeeland, him, refit, arabs, cesium, wick, Nearest to many: some, these, several, other, kazimierz, studies, all, psychoanalyst, Nearest to people: languages, gediminas, countries, glaucus, eplf, several, those, temporarily, Nearest to some: many, these, their, olav, overbearing, his, its, most, Nearest to was: is, has, were, had, became, be, chisel, been, Nearest to and: or, but, at, which, from, in, however, for, Nearest to in: at, on, during, from, for, by, with, and, Nearest to be: have, been, was, were, campaigner, by, receiving, combination, Nearest to all: these, marketed, maoi, marshals, many, lara, escalation, agate, Nearest to than: or, deduce, much, trough, haer, brainiac, tug, spalding, Nearest to system: hcl, rmi, elite, devine, oleg, bows, henotheism, primer, Nearest to were: are, was, had, tiu, have, be, by, is, Nearest to see: atzma, diaconate, transoxiana, syllables, showcasing, lackluster, bullough, rabbinical, Nearest to that: which, but, breaches, because, it, ietf, this, ellesmere, Nearest to s: pu, purr, tanoana, forum, traveller, predicated, plasmodium, integrator, Nearest to will: would, may, can, could, should, to, cannot, geographic, Average loss at step 22000: 3.502537 Average loss at step 24000: 3.485267 Average loss at step 26000: 3.483680 Average loss at step 28000: 3.478223 Average loss at step 30000: 3.503296 Nearest to up: him, begin, scharnhorst, them, unvoiced, refit, zeeland, calibers, Nearest to many: some, several, these, their, its, all, various, the, Nearest to people: countries, those, languages, dhea, temporarily, ach, vlsi, eplf, Nearest to some: many, these, several, the, their, olav, limes, this, Nearest to was: is, were, had, has, became, been, when, by, Nearest to and: or, who, in, but, from, of, rehearsal, tsunamis, Nearest to in: during, at, from, of, on, since, between, and, Nearest to be: have, is, been, are, were, mustelids, aldiss, impurity, Nearest to all: these, lara, some, many, inhabiting, several, maoi, marketed, Nearest to than: much, or, no, deduce, trough, spalding, tug, haer, Nearest to system: hcl, group, devine, francisco, oleg, rmi, raccoons, master, Nearest to were: are, was, have, had, is, tiu, dowager, been, Nearest to see: diaconate, include, atzma, showcasing, slider, transoxiana, rent, syllables, Nearest to that: which, this, however, but, what, where, kilometre, if, Nearest to s: his, forum, isbn, her, ancestral, insulated, sucking, and, Nearest to will: can, would, could, may, should, must, cannot, to, Average loss at step 32000: 3.500015 Average loss at step 34000: 3.495233 Average loss at step 36000: 3.458392 Average loss at step 38000: 3.301554 Average loss at step 40000: 3.431673 Nearest to up: out, him, them, unvoiced, back, arabs, down, begin, Nearest to many: some, several, these, various, those, their, certain, such, Nearest to people: languages, countries, those, quintessential, baum, serif, dhea, areas, Nearest to some: many, these, any, several, olav, their, those, most, Nearest to was: is, had, were, became, has, severing, being, been, Nearest to and: or, but, while, cynical, subsystem, where, mus, in, Nearest to in: from, of, during, on, for, and, bubbled, at, Nearest to be: been, have, were, are, is, continue, was, shotguns, Nearest to all: lara, these, both, two, many, travellers, lobster, citizen, Nearest to than: or, much, no, spalding, deduce, even, brainiac, significance, Nearest to system: systems, code, crusading, group, bows, smuts, hcl, completions, Nearest to were: are, have, was, tiu, had, be, been, being, Nearest to see: include, pigeon, syllables, algardi, diaconate, theroux, slider, lada, Nearest to that: which, however, what, this, because, where, it, oro, Nearest to s: his, forum, levine, fl, fender, isbn, tyr, empowerment, Nearest to will: would, can, could, may, should, must, might, cannot, Average loss at step 42000: 3.433565 Average loss at step 44000: 3.451384 Average loss at step 46000: 3.450792 Average loss at step 48000: 3.354962 Average loss at step 50000: 3.388338 Nearest to up: out, them, down, him, disproven, off, back, unvoiced, Nearest to many: some, several, these, various, those, most, such, other, Nearest to people: countries, languages, roots, vlsi, men, gediminas, those, scholars, Nearest to some: many, these, several, olav, most, their, any, the, Nearest to was: is, has, were, became, had, been, being, be, Nearest to and: but, or, in, while, of, from, whose, eurasia, Nearest to in: from, during, of, since, on, and, by, within, Nearest to be: have, been, were, was, is, become, being, are, Nearest to all: both, these, many, lara, writs, every, citizen, diffuses, Nearest to than: or, much, even, bam, deduce, while, spalding, significance, Nearest to system: systems, crusading, bows, code, hcl, renderings, smuts, group, Nearest to were: are, was, have, tiu, be, had, those, been, Nearest to see: include, pigeon, diaconate, algardi, theroux, bloomington, showcasing, pwnage, Nearest to that: which, however, what, often, uv, this, where, honoring, Nearest to s: whose, isbn, romanian, his, cleaner, adheres, fender, predicated, Nearest to will: would, could, can, may, must, should, shall, might, Average loss at step 52000: 3.437301 Average loss at step 54000: 3.427909 Average loss at step 56000: 3.438053 Average loss at step 58000: 3.394049 Average loss at step 60000: 3.394188 Nearest to up: out, them, down, him, off, back, disproven, replace, Nearest to many: some, several, these, various, all, most, such, other, Nearest to people: those, countries, men, scholars, roots, gediminas, others, vlsi, Nearest to some: many, several, these, olav, most, any, each, this, Nearest to was: is, had, became, has, were, been, be, although, Nearest to and: or, but, including, than, owning, shutting, with, sagan, Nearest to in: during, within, including, of, at, since, throughout, anew, Nearest to be: been, have, are, is, was, were, refer, become, Nearest to all: many, these, both, rediscovery, some, timers, menial, various, Nearest to than: or, much, but, spalding, and, deduce, no, far, Nearest to system: systems, code, crusading, group, hcl, software, depth, smuts, Nearest to were: are, was, had, have, tiu, those, haired, be, Nearest to see: include, diaconate, but, can, bloomington, pwnage, according, algardi, Nearest to that: which, however, this, what, where, it, often, ellesmere, Nearest to s: whose, infrastructural, isbn, tanoana, arbenz, flybys, vladimir, geographic, Nearest to will: would, can, could, may, must, should, might, cannot, Average loss at step 62000: 3.243753 Average loss at step 64000: 3.259946 Average loss at step 66000: 3.404854 Average loss at step 68000: 3.393782 Average loss at step 70000: 3.359670 Nearest to up: out, them, down, off, back, him, disproven, arabs, Nearest to many: some, several, these, various, all, most, olav, numerous, Nearest to people: men, those, countries, scholars, children, peoples, languages, historians, Nearest to some: many, several, these, olav, various, all, those, any, Nearest to was: is, were, has, had, became, severing, been, when, Nearest to and: or, while, but, which, like, stockade, than, unclassified, Nearest to in: within, during, on, throughout, through, from, for, at, Nearest to be: been, were, is, have, being, become, are, fully, Nearest to all: many, some, both, every, any, montag, various, these, Nearest to than: or, no, spalding, much, while, but, and, resign, Nearest to system: systems, crusading, bred, depth, group, hcl, smuts, code, Nearest to were: are, was, have, tiu, had, be, those, been, Nearest to see: include, diaconate, orators, list, algardi, according, but, fatality, Nearest to that: which, however, what, this, but, where, because, also, Nearest to s: pu, infrastructural, isbn, whose, chulainn, codifying, sequestered, purr, Nearest to will: would, could, may, can, should, must, might, shall, Average loss at step 72000: 3.372420 Average loss at step 74000: 3.350309 Average loss at step 76000: 3.322349 Average loss at step 78000: 3.355263 Average loss at step 80000: 3.377625 Nearest to up: out, off, down, them, him, disproven, back, arabs, Nearest to many: several, some, various, these, most, numerous, those, both, Nearest to people: men, children, countries, those, words, members, peoples, scholars, Nearest to some: many, several, various, these, olav, most, both, any, Nearest to was: is, were, has, had, became, been, being, when, Nearest to and: or, but, including, than, while, globalsecurity, rehearsal, lances, Nearest to in: during, within, on, until, at, since, after, through, Nearest to be: been, have, become, refer, being, were, is, proceed, Nearest to all: both, every, various, any, many, some, these, aguilera, Nearest to than: or, much, but, while, and, spalding, resign, even, Nearest to system: systems, crusading, smuts, software, code, familias, nonpartisan, game, Nearest to were: are, was, had, have, those, tiu, being, be, Nearest to see: include, according, diaconate, but, orators, includes, list, eth, Nearest to that: which, however, where, ellesmere, markham, what, this, thus, Nearest to s: pu, isbn, whose, chulainn, mondeo, infrastructural, electorates, adheres, Nearest to will: would, could, can, may, should, must, might, cannot, Average loss at step 82000: 3.407648 Average loss at step 84000: 3.411925 Average loss at step 86000: 3.389199 Average loss at step 88000: 3.359499 Average loss at step 90000: 3.365950 Nearest to up: out, off, down, them, back, him, disproven, arabs, Nearest to many: some, several, various, these, all, numerous, most, those, Nearest to people: children, men, women, persons, religions, god, screenwriters, countries, Nearest to some: many, several, these, all, any, various, those, most, Nearest to was: is, became, had, were, has, been, being, be, Nearest to and: or, but, while, emory, consistory, including, however, who, Nearest to in: within, during, around, of, throughout, under, between, near, Nearest to be: have, been, become, is, produce, was, refer, simulate, Nearest to all: many, both, some, every, various, several, each, any, Nearest to than: or, much, spalding, no, even, resign, showered, considerably, Nearest to system: systems, crusading, process, program, group, mee, unit, brockovich, Nearest to were: are, was, had, have, while, tiu, those, been, Nearest to see: list, diaconate, include, includes, references, refer, external, kliper, Nearest to that: which, however, what, but, markham, autos, where, ellesmere, Nearest to s: whose, isbn, infrastructural, his, pu, tyr, vladimir, mondeo, Nearest to will: would, could, can, may, must, should, might, cannot, Average loss at step 92000: 3.398944 Average loss at step 94000: 3.258866 Average loss at step 96000: 3.356221 Average loss at step 98000: 3.242518 Average loss at step 100000: 3.359612 Nearest to up: out, off, down, him, back, them, arabs, begin, Nearest to many: several, some, these, various, numerous, those, all, few, Nearest to people: persons, men, children, someone, women, countries, players, scholars, Nearest to some: many, several, these, any, various, olav, all, certain, Nearest to was: is, became, had, were, has, although, when, been, Nearest to and: or, but, like, while, including, when, than, who, Nearest to in: within, during, throughout, of, at, from, on, with, Nearest to be: been, have, is, refer, become, are, were, produce, Nearest to all: many, various, any, every, these, some, both, several, Nearest to than: or, spalding, much, and, even, showered, while, omniglot, Nearest to system: systems, crusading, process, program, software, familias, hcl, stewardship, Nearest to were: are, was, have, tiu, these, those, had, be, Nearest to see: diaconate, includes, references, list, include, links, refer, external, Nearest to that: which, however, what, this, lodges, ellesmere, who, dicke, Nearest to s: whose, isbn, his, tyr, ancestral, infrastructural, pu, starring, Nearest to will: would, could, must, can, should, may, might, cannot, Transform embedding into 2D using t-SNE final_embeddings . shape (50000, 128) %%time num_points = 400 tsne = TSNE ( perplexity = 30 , n_components = 2 , init = 'pca' , n_iter = 5000 , method = 'exact' ) two_d_embeddings = tsne . fit_transform ( final_embeddings [ 1 : num_points + 1 , :]) Visualize result def plot ( embeddings , labels ): assert embeddings . shape [ 0 ] >= len ( labels ), 'More labels than embeddings' plt . figure ( figsize = ( 15 , 15 )) # in inches for i , label in enumerate ( labels ): x , y = embeddings [ i , :] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ), xytext = ( 5 , 2 ), textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) plt . show () words = [ reverse_dictionary [ i ] for i in range ( 1 , num_points + 1 )] plot ( two_d_embeddings , words ) Todo CBOW An alternative to skip-gram is another Word2Vec model called CBOW (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset. References Original jupyter notebook from the Udacity MOOC course: Deep learning by Google . TensorFlow word2vec tutorial http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Deep Learning","url":"https://leemeng.tw/find-word-semantic-by-using-word2vec-in-tensorflow.html","loc":"https://leemeng.tw/find-word-semantic-by-using-word2vec-in-tensorflow.html"},{"title":"Simple Convolutional Neural Network using TensorFlow","text":"The goal here is to practice building convolutional neural networks to classify notMNIST characters using TensorFlow. As image size become bigger and bigger, it become unpractical to train fully-connected NN because there will be just too many parameters and thus the model will overfit very soon. And CNN solve this problem by weight sharing. We will start by building a CNN with two convolutional layers connected by a fully connected layer and then try also pooling layer and other thing to improve the model performance. Original jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import numpy as np import seaborn as sns import tensorflow as tf import matplotlib.pyplot as plt from six.moves import cPickle as pickle from six.moves import range from tqdm import tnrange import time # beautify graph plt . style . use ( 'ggplot' ) Load notMNIST dataset pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data Reformat into a TensorFlow-friendly shape: convolutions need the image data formatted as a cube of shape (width, height, #channels) labels as float 1-hot encodings. image_size = 28 num_labels = 10 num_channels = 1 # grayscale def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size , image_size , num_channels )) . astype ( np . float32 ) labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (200000, 28, 28, 1) (200000, 10) Validation set (10000, 28, 28, 1) (10000, 10) Test set (10000, 28, 28, 1) (10000, 10) def accuracy ( predictions , labels ): return ( 100.0 * np . sum ( np . argmax ( predictions , 1 ) == np . argmax ( labels , 1 )) / predictions . shape [ 0 ]) Helper for training visualization Let's define a function that make better visualization of our training progress. The function will draw mini-batch loss and training/validation accuracy dynamically. # dynamic showing loss and accuracy when training % matplotlib notebook def plt_dynamic ( x , y , ax , xlim = None , ylim = None , xlabel = 'X' , ylabel = 'Y' , colors = [ 'b' ], sleep_sec = 0 , figsize = None ): import time if figsize : fig . set_size_inches ( figsize [ 0 ], figsize [ 1 ], forward = True ) ax . set_xlabel ( xlabel ); ax . set_ylabel ( ylabel ) for color in colors : ax . plot ( x , y , color ) if xlim : ax . set_xlim ( xlim [ 0 ], xlim [ 1 ]) if ylim : ax . set_ylim ( ylim [ 0 ], ylim [ 1 ]) fig . canvas . draw () time . sleep ( sleep_sec ) NN with 2 convolutional layers Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes. Computation graph Although this assignment already provide good Tensorflow code to build convoluational networks, I found that I can't imagine what NN I was going to build by reading the code. So I tried to draw what we're going to build and explain some parameters used in code by comments. The convolutional network we're going to build: Figure 1 : CNN with 1 fully connected layer Something worth mentioning: We set both convoluational layers' output depth = 16. We use filters/patches of shape (5 * 5) to find features in local area of a image. The new width and height of the convoluational layer will be half of that in the previous layer because we use stride = 2 and SAME padding to 'slide' our patches. Thus 28 -> 14 -> 7. Notice that ReLU layers applied after convoluational layers are omitted for simplicity. The activations in C2 fully connected to the FC layer. For each neuron on the FC layer, there are $7 * 7 * 16 = 784$ weights ($785$ for bias ), so there are $785 * 64 = 50240$ parameters in the FC layer. For more details about CNN, I recommend CS231n . batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # When defining weights for a convoluational layer, use the notation # [filter_size, filter_size, input_depth, output_depth] layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) # in this CNN, two convoluational layers happen to have the same depth. # if we want, we can adjust them to be different like depth1, depth2 layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # because we use stride = 2 and SAME padding, our new shape of first feature map C1 # will be (image_size // 2, image_size //2). and because we use 2 convolutional layers, # the shape of second feature map C2 will be (image_size // 2 // 2, image_size // 2 // 2) # = (image_size // 4, image_size // 4). and because we have depth == 16, # the total neurons on C2 will be image_size // 4 * image_size // 4 * depth layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # this is where we set stride = 2 for both width and height and also SAME padding # the third parameters in tf.nn.conv2d is to set stride for every dimension # specified in the first parameter data's shape conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) conv = tf . nn . conv2d ( hidden , layer2_weights , [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) shape = hidden . get_shape () . as_list () # turn the C2 3D cube back to 2D matrix by shape (#data_points, #neurons) reshape = tf . reshape ( hidden , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model and visualize the result The best thing of the visualization is that it's rendered in a real-time manner. num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#c57275b3-2d7b-4625-b020-2eeee7d2cdfe'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#557e0733-61fb-4bd1-9708-eb8ea58b86ae'); {\"model_id\": \"255b2330df764c069bf0fa8b72e282b3\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 4.085.batch acc: 0.0%, Valid acc: 10.0%. Minibatch loss at step 100: 0.861.batch acc: 62.5%, Valid acc: 74.5%. Minibatch loss at step 200: 0.425.batch acc: 87.5%, Valid acc: 79.1%. Minibatch loss at step 300: 0.934.batch acc: 62.5%, Valid acc: 79.8%. Minibatch loss at step 400: 0.861.batch acc: 68.8%, Valid acc: 79.6%. Minibatch loss at step 500: 0.204.batch acc: 87.5%, Valid acc: 80.6%. Minibatch loss at step 600: 0.741.batch acc: 75.0%, Valid acc: 82.0%. Minibatch loss at step 700: 0.591.batch acc: 87.5%, Valid acc: 82.5%. Minibatch loss at step 800: 1.171.batch acc: 68.8%, Valid acc: 81.8%. Minibatch loss at step 900: 0.171.batch acc: 100.0%, Valid acc: 83.2%. Minibatch loss at step 1000: 0.487.batch acc: 93.8%, Valid acc: 82.6%. Test accuracy: 89.4% As shown above, mini-batch loss dropped rapidly at first 200 iterations, training and validation accuracy also improve quickly (both achieved about 80%). After 200 iterations, validation performance become stable but still improved about 5%. The test accuracy is about 89%. Problem 1 - Use pooling layers to reduce dimensionality The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation ( nn.max_pool() ) of stride 2 and kernel size 2. The reason why we're going to use pooling layer is that we can reduce spatial size thus parameters to reduce the chance of overfitting. And the advantage of pooling layer is that it require no new parameters. Let's see how much performance we can gain by using max pooling. Figure 2 : Max Pooling Build model with pooling layers Actually, what we will do is just to add pooling layers right after ReLU layers and let the convoluational layer use stride 1. In intuition, we let the convoluational layers look more 'closely' into the images, but also try to limit the number of activation and extract the important parts by pooling layers. batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ image_size // 4 * image_size // 4 * depth , num_hidden ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) layer4_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer3_weights ) + layer3_biases ) return tf . matmul ( hidden , layer4_weights ) + layer4_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 1001 step_interval = 50 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#71ba9e96-5e3a-4e41-a68a-662e878e1950'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#cda45262-10f1-442d-8561-e261321197c2'); {\"model_id\": \"ae81313ad6ad4b428ce63444b7f0ae73\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.485.batch acc: 18.8%, Valid acc: 13.0%. Minibatch loss at step 100: 0.886.batch acc: 75.0%, Valid acc: 73.2%. Minibatch loss at step 200: 0.417.batch acc: 87.5%, Valid acc: 79.7%. Minibatch loss at step 300: 0.759.batch acc: 81.2%, Valid acc: 80.9%. Minibatch loss at step 400: 0.835.batch acc: 75.0%, Valid acc: 80.5%. Minibatch loss at step 500: 0.366.batch acc: 87.5%, Valid acc: 82.0%. Minibatch loss at step 600: 0.630.batch acc: 75.0%, Valid acc: 83.3%. Minibatch loss at step 700: 0.605.batch acc: 81.2%, Valid acc: 83.2%. Minibatch loss at step 800: 1.031.batch acc: 68.8%, Valid acc: 83.5%. Minibatch loss at step 900: 0.240.batch acc: 93.8%, Valid acc: 84.6%. Minibatch loss at step 1000: 0.500.batch acc: 87.5%, Valid acc: 83.9%. Test accuracy: 90.7% There is some performance gain in my current iteration between model w/o pooling layer ( about 1.3% ). And it seems that after several iterations, the validation set performance is slightly better with the pooling layers. But it took about 1 minute and 30 seconds to train CNN with pooling layers, and only 30 seconds to train the one without pooling layers. I think it depends on whether you're willing to gain a little more performance by using more time to train the model. Problem 2 Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay. I will just try to add a convoluational layer here and train a little longer to see how the performance changed. Computation graph batch_size = 16 patch_size = 5 depth = 16 num_hidden = 64 graph = tf . Graph () with graph . as_default (): # Input data. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( batch_size , image_size , image_size , num_channels )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( batch_size , num_labels )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. layer1_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , num_channels , depth ], stddev = 0.1 )) layer1_biases = tf . Variable ( tf . zeros ([ depth ])) layer2_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer2_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) layer3_weights = tf . Variable ( tf . truncated_normal ( [ patch_size , patch_size , depth , depth ], stddev = 0.1 )) layer3_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ depth ])) # CONV -> FC layer4_weights = tf . Variable ( tf . truncated_normal ( [( image_size // 8 + 1 ) * ( image_size // 8 + 1 ) * depth , num_hidden ], stddev = 0.1 )) layer4_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_hidden ])) # FC -> output layer5_weights = tf . Variable ( tf . truncated_normal ([ num_hidden , num_labels ], stddev = 0.1 )) layer5_biases = tf . Variable ( tf . constant ( 1.0 , shape = [ num_labels ])) # Model. def model ( data ): # [CONV -> RELU -> POOL] * 3 conv = tf . nn . conv2d ( data , layer1_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer1_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer2_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer2_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) conv = tf . nn . conv2d ( pool , layer3_weights , [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) hidden = tf . nn . relu ( conv + layer3_biases ) pool = tf . nn . max_pool ( hidden , [ 1 , 2 , 2 , 1 ], [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) shape = pool . get_shape () . as_list () reshape = tf . reshape ( pool , [ shape [ 0 ], shape [ 1 ] * shape [ 2 ] * shape [ 3 ]]) hidden = tf . nn . relu ( tf . matmul ( reshape , layer4_weights ) + layer4_biases ) return tf . matmul ( hidden , layer5_weights ) + layer5_biases # Training computation. logits = model ( tf_train_dataset ) loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf_train_labels , logits = logits )) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.05 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits ) valid_prediction = tf . nn . softmax ( model ( tf_valid_dataset )) test_prediction = tf . nn . softmax ( model ( tf_test_dataset )) Train the model num_steps = 10001 step_interval = 500 with tf . Session ( graph = graph ) as session : # initialize weights tf . global_variables_initializer () . run () # plot for mini-batch loss and accuracy fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , sharex = True ) xs , batch_loss , batch_acc , valid_acc = [[] for _ in range ( 4 )] for step in tnrange ( num_steps ): # get new mini-batch for training offset = ( step * batch_size ) % ( train_labels . shape [ 0 ] - batch_size ) batch_data = train_dataset [ offset :( offset + batch_size ), :, :, :] batch_labels = train_labels [ offset :( offset + batch_size ), :] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) # draw loss and accuracy while training if ( step % step_interval == 0 ): xs . append ( step ) batch_loss . append ( l ) batch_acc . append ( accuracy ( predictions , batch_labels )) valid_acc . append ( accuracy ( valid_prediction . eval (), valid_labels )) plt_dynamic ( xs , batch_loss , ax1 , ( 0 , num_steps ), None , '#Iterations' , 'Mini-batch Loss' ) plt_dynamic ( xs , batch_acc , ax2 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Mini-batch Acc' ) plt_dynamic ( xs , valid_acc , ax3 , ( 0 , num_steps ), ( 0 , 100 ), '#Iterations' , 'Valid Acc' , colors = [ 'r' ], figsize = ( 7 , 7 )) if ( step % ( step_interval * 2 ) == 0 ): print ( 'Minibatch loss at step {} : {:.3f} .' . format ( step , l ) + 'batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test accuracy: %.1f%% ' % accuracy ( test_prediction . eval (), test_labels )) var element = $('#2a8cade5-4dd9-4583-917e-dfa95af483b3'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width); canvas.attr('height', height); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0']; var y0 = fig.canvas.height - msg['y0']; var x1 = msg['x1']; var y1 = fig.canvas.height - msg['y1']; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x; var y = canvas_pos.y; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); event.shiftKey = false; // Send a \"J\" for go to next cell event.which = 74; event.keyCode = 74; manager.command_mode(); manager.handle_keydown(event); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } var element = $('#1004120c-4e43-4bef-bac8-0c243cfb30b0'); {\"model_id\": \"b240065c4ffa4ca29432614b189f2e04\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 3.563.batch acc: 6.2%, Valid acc: 10.0%. Minibatch loss at step 1000: 0.502.batch acc: 87.5%, Valid acc: 84.9%. Minibatch loss at step 2000: 0.414.batch acc: 87.5%, Valid acc: 86.4%. Minibatch loss at step 3000: 0.164.batch acc: 93.8%, Valid acc: 87.0%. Minibatch loss at step 4000: 0.545.batch acc: 75.0%, Valid acc: 88.1%. Minibatch loss at step 5000: 0.898.batch acc: 75.0%, Valid acc: 88.4%. Minibatch loss at step 6000: 0.493.batch acc: 81.2%, Valid acc: 86.4%. Minibatch loss at step 7000: 0.613.batch acc: 81.2%, Valid acc: 89.3%. Minibatch loss at step 8000: 0.089.batch acc: 100.0%, Valid acc: 89.2%. Minibatch loss at step 9000: 0.280.batch acc: 93.8%, Valid acc: 89.6%. Minibatch loss at step 10000: 0.437.batch acc: 87.5%, Valid acc: 89.4%. Test accuracy: 94.6% By adding a new convoluational layer and train 10x steps, our model's performance can even boost to almost 95%! (though it take about 5 minutes to train on my pc) and I think there are still many things we can tune to make the model better, but I will stop here to move on to sequence model! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Deep Learning","url":"https://leemeng.tw/simple-convolutional-neural-network-using-tensorflow.html","loc":"https://leemeng.tw/simple-convolutional-neural-network-using-tensorflow.html"},{"title":"Regularization for Multi-layer Neural Networks in Tensorflow","text":"The goal of this assignment is to explore regularization techniques. The original notebook can be found here Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function from tqdm import tnrange import numpy as np import tensorflow as tf from six.moves import cPickle as pickle Load NotMNIST dataset First reload the data we generated in 1_notmnist.ipynb . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) X_train = save [ 'train_dataset' ] Y_train = save [ 'train_labels' ] X_valid = save [ 'valid_dataset' ] Y_valid = save [ 'valid_labels' ] X_test = save [ 'test_dataset' ] Y_test = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , Y_test . shape ) Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat dataset Reformat into a shape that's more adapted to the models we're going to train: data as a flat matrix, labels as float 1-hot encodings. As I did in previous notebook, this reformat operation will be different from the operation suggested by the original notebook . image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T return dataset , labels X_train , Y_train = reformat ( X_train , Y_train ) X_valid , Y_valid = reformat ( X_valid , Y_valid ) X_test , Y_test = reformat ( X_test , Y_test ) print ( 'Training set' , X_train . shape , Y_train . shape ) print ( 'Validation set' , X_valid . shape , Y_valid . shape ) print ( 'Test set' , X_test . shape , X_test . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (784, 10000) Using Accuracy as Default Metric Because as we explored before, there exist no unbalanced problem in the dataset, so accuracy alone will be sufficient for evaluating performance of our model on the classification task. def accuracy ( predictions , labels ): return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) 3-layer NN as base model In order to test the effect with/without regularization, we will use a little more complex neural network with 2 hidden layers as our base model. And we will be using ReLU as our activation function. Hyper parameters # hyper parameters learning_rate = 1e-2 lamba = 1e-3 keep_prob = 0.5 batch_size = 128 num_steps = 501 n0 = image_size * image_size # input size n1 = 1024 # first hidden layer n2 = 512 # second hidden layer n3 = 256 # third hidden layer n4 = num_labels # output size Build model # build a model which let us able to choose different optimzation mechnism def model ( lamba = 0 , learning_rate = learning_rate , keep_prob = 1 , learning_decay = False , batch_size = batch_size , num_steps = num_steps , n1 = n1 , n2 = n2 , n3 = n3 ): print ( \"\"\" Train 3-layer NN with following settings: Regularization lambda: {} Learning rate: {} learning_decay: {} keep_prob: {} Batch_size: {} Number of steps: {} n1, n2, n3: {}, {}, {}\"\"\" . format ( lamba , learning_rate , learning_decay , keep_prob , batch_size , num_steps , n1 , n2 , n3 )) # construct computation graph graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training X = tf . placeholder ( tf . float32 , shape = ( n0 , batch_size )) Y = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) global_step = tf . Variable ( 0 ) # use all valid/test set tf_X_valid = tf . constant ( X_valid ) tf_X_test = tf . constant ( X_test ) # initialize weights, biases # notice that we have two hidden # layers so we now have W1, b1, W2, b2, W3, b3 W1 = tf . Variable ( tf . truncated_normal ([ n1 , n0 ], stddev = np . sqrt ( 2.0 / n0 ))) W2 = tf . Variable ( tf . truncated_normal ([ n2 , n1 ], stddev = np . sqrt ( 2.0 / n1 ))) W3 = tf . Variable ( tf . truncated_normal ([ n3 , n2 ], stddev = np . sqrt ( 2.0 / n2 ))) W4 = tf . Variable ( tf . truncated_normal ([ n4 , n3 ], stddev = np . sqrt ( 2.0 / n3 ))) b1 = tf . Variable ( tf . zeros ([ n1 , 1 ])) b2 = tf . Variable ( tf . zeros ([ n2 , 1 ])) b3 = tf . Variable ( tf . zeros ([ n3 , 1 ])) b4 = tf . Variable ( tf . zeros ([ n4 , 1 ])) # training computation Z1 = tf . matmul ( W1 , X ) + b1 A1 = tf . nn . relu ( Z1 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z1 ), keep_prob ) Z2 = tf . matmul ( W2 , A1 ) + b2 A2 = tf . nn . relu ( Z2 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z2 ), keep_prob ) Z3 = tf . matmul ( W3 , A2 ) + b3 A3 = tf . nn . relu ( Z3 ) if keep_prob == 1 else tf . nn . dropout ( tf . nn . relu ( Z3 ), keep_prob ) Z4 = tf . matmul ( W4 , A3 ) + b4 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( Y ), logits = tf . transpose ( Z4 ))) if lamba : loss += lamba * \\ ( tf . nn . l2_loss ( W1 ) + tf . nn . l2_loss ( W2 ) + tf . nn . l2_loss ( W3 ) + tf . nn . l2_loss ( W4 )) # optimizer if learning_decay : learning_rate = tf . train . exponential_decay ( 0.5 , global_step , 5000 , 0.80 , staircase = True ) optimizer = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss , global_step = global_step ) else : optimizer = ( tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss )) # valid / test prediction Y_pred = tf . nn . softmax ( Z4 , dim = 0 ) Y_vaild_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_valid ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) Y_test_pred = tf . nn . softmax ( tf . matmul ( W4 , tf . nn . relu ( tf . matmul ( W3 , tf . nn . relu ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_X_test ) + b1 )) + b2 )) + b3 )) + b4 , dim = 0 ) # define training with tf . Session ( graph = graph ) as sess : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # generate randomized mini-batches from training data offset = ( step * batch_size ) % ( Y_train . shape [ 1 ] - batch_size ) batch_X = X_train [:, offset :( offset + batch_size )] batch_Y = Y_train [:, offset :( offset + batch_size )] # train model _ , l , batch_Y_pred = sess . run ( [ optimizer , loss , Y_pred ], feed_dict = { X : batch_X , Y : batch_Y }) if ( step % 200 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( batch_Y_pred , batch_Y ), accuracy ( Y_vaild_pred . eval (), Y_valid ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( Y_test_pred . eval (), Y_test ))) Train model without regularization model ( learning_rate = 0.5 , num_steps = 1601 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.5 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 1601 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#e3b1d8ee-9acf-4f0a-9901-a90cbbd7a2b6'); {\"model_id\": \"95a1e075513f4d02a8579c4fcd5b8509\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.374. batch acc: 14.1%, Valid acc: 28.4%. Minibatch loss at step 200: 0.600. batch acc: 82.0%, Valid acc: 84.9%. Minibatch loss at step 400: 0.429. batch acc: 89.8%, Valid acc: 85.8%. Minibatch loss at step 600: 0.372. batch acc: 87.5%, Valid acc: 85.7%. Minibatch loss at step 800: 0.454. batch acc: 89.1%, Valid acc: 87.7%. Minibatch loss at step 1000: 0.374. batch acc: 87.5%, Valid acc: 88.1%. Minibatch loss at step 1200: 0.251. batch acc: 91.4%, Valid acc: 88.8%. Minibatch loss at step 1400: 0.397. batch acc: 89.8%, Valid acc: 89.0%. Minibatch loss at step 1600: 0.470. batch acc: 82.0%, Valid acc: 88.9%. Test acc: 94.2% L2 regularization Introduce and tune L2 regularization for the models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t) . The right amount of regularization should improve your validation / test accuracy. # for lamda in [1 / 10 ** i for i in list(np.arange(1, 4))]: # model(lamba=lamda) model ( lamba = 0.1 , learning_rate = 0.01 ) Train 3-layer NN with following settings: Regularization lambda: 0.1 Optimizer: sgd Learning rate: 0.01 Batch_size: 128 Number of steps: 501 n1, n2: 512, 256 Initialized var element = $('#aa4201d9-8c8c-4d23-9732-2cc0def1caef'); {\"model_id\": \"6b6986da296646b9aac2266326edcf21\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 22969.777. batch acc: 9.4%, Valid acc: 19.3%. Minibatch loss at step 200: 13876.185. batch acc: 74.2%, Valid acc: 75.2%. Minibatch loss at step 400: 9266.566. batch acc: 78.1%, Valid acc: 74.3%. Test acc: 81.4% Case of overfitting Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens? model ( num_steps = 10 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 1 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#904c64d1-c89b-4c8e-b625-5682fbd1cee7'); {\"model_id\": \"312b63e26f364cec95540b452b4ec95c\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.442. batch acc: 8.6%, Valid acc: 11.4%. Test acc: 20.7% Dropout Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training. What happens to our extreme overfitting case? model ( num_steps = 10 , keep_prob = 0.5 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: False keep_prob: 0.5 Batch_size: 128 Number of steps: 10 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#f406642e-53a2-4e12-ba76-c1bcee776934'); {\"model_id\": \"f5a5be47b2a14f488d9d302df5d5988d\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.784. batch acc: 7.0%, Valid acc: 10.0%. Test acc: 17.3% Boost performance by using Multi-layer NN Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1% . One avenue you can explore is to add multiple layers. Another one is to use learning rate decay: global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step, ...) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) model ( learning_decay = True , num_steps = 1501 , lamba = 0 , keep_prob = 1 ) Train 3-layer NN with following settings: Regularization lambda: 0 Learning rate: 0.01 learning_decay: True keep_prob: 1 Batch_size: 128 Number of steps: 1501 n1, n2, n3: 1024, 512, 256 Initialized var element = $('#bb6fd9f4-9ef5-4053-a4cd-ccfd54311978'); {\"model_id\": \"ee2b75cf5f6542f99a17c5ad02bc49fd\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 2.395. batch acc: 12.5%, Valid acc: 37.0%. Minibatch loss at step 200: 0.589. batch acc: 82.0%, Valid acc: 84.7%. Minibatch loss at step 400: 0.409. batch acc: 89.1%, Valid acc: 86.2%. Minibatch loss at step 600: 0.396. batch acc: 88.3%, Valid acc: 86.5%. Minibatch loss at step 800: 0.435. batch acc: 88.3%, Valid acc: 87.6%. Minibatch loss at step 1000: 0.407. batch acc: 85.2%, Valid acc: 88.5%. Minibatch loss at step 1200: 0.262. batch acc: 91.4%, Valid acc: 88.9%. Minibatch loss at step 1400: 0.411. batch acc: 87.5%, Valid acc: 88.8%. Test acc: 94.3% if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Deep Learning","url":"https://leemeng.tw/regularization-for-multi-layer-neural-networks-in-tensorflow.html","loc":"https://leemeng.tw/regularization-for-multi-layer-neural-networks-in-tensorflow.html"},{"title":"Using TensorFlow to Train a Shallow NN with Stochastic Gradient Descent","text":"The goal here is to progressively train deeper and more accurate models using TensorFlow. We will first load the notMNIST dataset which we have done data cleaning. For the classification problem, we will first train two logistic regression models use simple gradient descent, stochastic gradient descent (SGD) respectively for optimization to see the difference between these optimizers. Finally, train a Neural Network with one-hidden layer using ReLU activation units to see whether we can boost our model's performance further. Previously in 1_notmnist.ipynb , we created a pickle with formatted datasets for training, development and testing on the notMNIST dataset . This post is modified from the jupyter notebook originated from the Udacity MOOC course: Deep learning by Google . Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import os import numpy as np import tensorflow as tf from six.moves import cPickle as pickle from six.moves import range Load notMNIST dataset This time we will use the dataset which has been normalized and randomized before to omit the data preprocessing step. Tips: Release memory after loading big-size dataset using del . pickle_file = 'datasets/notMNIST.pickle' with open ( pickle_file , 'rb' ) as f : save = pickle . load ( f ) print ( 'Dataset size: {:.1f} MB' . format ( os . stat ( pickle_file ) . st_size / 2 ** 20 )) train_dataset = save [ 'train_dataset' ] train_labels = save [ 'train_labels' ] valid_dataset = save [ 'valid_dataset' ] valid_labels = save [ 'valid_labels' ] test_dataset = save [ 'test_dataset' ] test_labels = save [ 'test_labels' ] del save # hint to help gc free up memory print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Dataset size: 658.8 MB Training set (200000, 28, 28) (200000,) Validation set (10000, 28, 28) (10000,) Test set (10000, 28, 28) (10000,) Reformat data for easier training Reformat both pixels(features) and labels that's more adapted to the models we're going to train: features(pixels) as a flat matrix with shape = (#total pixels, #instances) Figure 1 : Flattened features labels as float 1-hot encodings with shape = (#type of labels, #instances) Figure 2 : Flattened labels Tips: Notice that we use different shape of matrix with the original TensorFlow example nookbook because I think it's easier to understand how matrix multiplication work by imagining each training/test instance as a column vector. But in response to this change, we have to modify several code in order to make it works! Transpose logits and labels when calling tf.nn.softmax_cross_entropy_with_logits Set dim = 0 when using tf.nn.softmax Set axis = 0 when using np.argmax to compute accuracy One-hot encode labels by compare the label with the 0-9 array and transform True/False array as float array use astype(np.float32) image_size = 28 num_labels = 10 def reformat ( dataset , labels ): dataset = dataset . reshape (( - 1 , image_size * image_size )) . astype ( np . float32 ) . T # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = ( np . arange ( num_labels ) == labels [:, None ]) . astype ( np . float32 ) . T # key point1 return dataset , labels train_dataset , train_labels = reformat ( train_dataset , train_labels ) valid_dataset , valid_labels = reformat ( valid_dataset , valid_labels ) test_dataset , test_labels = reformat ( test_dataset , test_labels ) print ( 'Training set' , train_dataset . shape , train_labels . shape ) print ( 'Validation set' , valid_dataset . shape , valid_labels . shape ) print ( 'Test set' , test_dataset . shape , test_labels . shape ) Training set (784, 200000) (10, 200000) Validation set (784, 10000) (10, 10000) Test set (784, 10000) (10, 10000) Logistic regression with gradient descent For logistic regression, we use the formula $WX + b = Y'$ to do the computation. W is of shape (10, 784), X is of shape (784, m) and Y' is of shape (10, m) where $m$ is the number of training instances/images. After compute the probabilities of 10 classes stored in Y', we will use built-in tf.nn.softmax_cross_entropy_with_logits to compute cross-entropy between Y' and Y(train_labels) as cost. We will first instruct Tensorflow how to do all the computation and make it run the optimization several times. Build the Tensorflow computation graph We're first going to train a multinomial logistic regression using simple gradient descent. TensorFlow works like this: First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below: with graph.as_default(): ... Then you can run the operations on this graph as many times as you want by calling session.run() , providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below: with tf.Session(graph=graph) as session: ... Let's load all the data into TensorFlow and build the computation graph corresponding to our training: # With gradient descent training, even this much data is prohibitive. # Subset the training data for faster turnaround. train_subset = 10000 graph = tf . Graph () # when we want to create multiple graphs in the same script, # use this to encapsulate each graph and run session right after graph definition with graph . as_default (): # Input data. # Load the training, validation and test data into constants that are # attached to the graph. tf_train_dataset = tf . constant ( train_dataset [:, : train_subset ]) tf_train_labels = tf . constant ( train_labels [:, : train_subset ]) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. # These are the parameters that we are going to be training. The weight # matrix will be initialized using random values following a (truncated) # normal distribution. The biases get initialized to zero. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. # We multiply the inputs with the weight matrix, and add biases. We compute # the softmax and cross-entropy (it's one operation in TensorFlow, because # it's very common, and it can be optimized). We take the average of this # cross-entropy across all training examples: that's our loss. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. # We are going to find the minimum of this loss using gradient descent. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. # These are not part of training, but merely here so that we can report # accuracy figures as we train. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Tips: As we saw before, logits = tf.matmul(weights, tf_train_dataset) + biases is equivalent to the logistic regression formula $Y' = WX + b$ Transpose y_hat and y to fit in softmax_cross_entropy_with_logits Gradient descent by iterating computation graph Now we can tell TensorFlow to run this computation and iterate. Here we will use tqdm library to help us easily visualize the progress and the time used in the iterations. Tips: Use np.argmax(predictions, axis=0) to transfrom one-hot encoded labels back to singe number for every data points. Use .eval() to get the predictions for test/validation set from tqdm import tnrange num_steps = 801 def accuracy ( predictions , labels ): \"\"\"For every (logit/Z, y) pair, get the (predicted label, label) and count the occurence where predicted label == label and divide by the total number of data points. \"\"\" return ( np . sum ( np . argmax ( predictions , axis = 0 ) == np . argmax ( labels , axis = 0 )) / labels . shape [ 1 ] * 100 ) # Calculate the correct predictions # correct_prediction = tf.equal(tf.argmax(predictions), tf.argmax(labels)) # # Calculate accuracy on the test set # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) return accruacy with tf . Session ( graph = graph ) as session : # This is a one-time operation which ensures the parameters get initialized as # we described in the graph: random weights for the matrix, zeros for the # biases. tf . global_variables_initializer () . run () print ( 'Initialized' ) for step in tnrange ( num_steps ): # Run the computations. We tell .run() that we want to run the optimizer, # and get the loss value and the training predictions returned as numpy # arrays. _ , l , predictions = session . run ([ optimizer , loss , train_prediction ]) if ( step % 100 == 0 ): print ( 'Cost at step {} : {:.3f} . Training acc: {:.1f} %, Validation acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , train_labels [:, : train_subset ]), accuracy ( valid_prediction . eval (), valid_labels ), \">\" )) # Calling .eval() on valid_prediction is basically like calling run(), but # just to get that one numpy array. Note that it recomputes all its graph # dependencies. print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#02e692c4-0550-4cfb-a5de-3c1a03255703'); {\"model_id\": \"a776469c0f394bab94bc900cf03f469d\", \"version_major\": 2, \"version_minor\": 0} Cost at step 0: 20.057. Training acc: 6.4%, Validation acc: 10.0%. Cost at step 100: 2.326. Training acc: 70.9%, Validation acc: 70.7%. Cost at step 200: 1.868. Training acc: 73.9%, Validation acc: 73.4%. Cost at step 300: 1.611. Training acc: 75.4%, Validation acc: 74.5%. Cost at step 400: 1.436. Training acc: 76.4%, Validation acc: 74.8%. Cost at step 500: 1.306. Training acc: 77.1%, Validation acc: 75.1%. Cost at step 600: 1.207. Training acc: 77.8%, Validation acc: 75.5%. Cost at step 700: 1.127. Training acc: 78.5%, Validation acc: 75.6%. Cost at step 800: 1.062. Training acc: 79.2%, Validation acc: 75.9%. Test acc: 82.8% Logistic regression with SGD Or more precisely, mini-batch approach. From the result above, we can see it cost about 20 seconds (on my computer) to iterate 10,000 training instances by simple gradient descent. Let's now switch to stochastic gradient descent training instead, which is much faster. The graph will be similar, except that instead of holding all the training data into a constant node, we create a Placeholder node which will be fed actual data at every call of session.run() . Tips: The difference between SGD and gradient descent is that the former don't use whole training set to compute gradient descent, instead just use a 'mini-batch' of it and assume the corresponding gradient descent is the way to optimize. So we will keep using GradientDescentOptimizer but with a different loss computed from a smaller sub-training set. Figure 3 : SGD vs Gradient Descent Build computation graph batch_size = 128 graph = tf . Graph () with graph . as_default (): # Input data. For the training data, we use a placeholder that will be fed # at run time with a training minibatch. tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # Variables. weights = tf . Variable ( tf . truncated_normal ([ num_labels , image_size * image_size ])) biases = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # Training computation. logits = tf . matmul ( weights , tf_train_dataset ) + biases loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # Optimizer. optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # Predictions for the training, validation, and test data. train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_valid_dataset ) + biases , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( weights , tf_test_dataset ) + biases , dim = 0 ) Iterate using SGD num_steps = 3001 with tf . Session ( graph = graph ) as session : tf . global_variables_initializer () . run () print ( \"Initialized\" ) for step in tnrange ( num_steps ): # Pick an offset within the training data, which has been randomized. # Note: we could use better randomization across epochs. offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) # Generate a minibatch. batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] # Prepare a dictionary telling the session where to feed the minibatch. # The key of the dictionary is the placeholder node of the graph to be fed, # and the value is the numpy array to feed to it. feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#5ac524fc-6183-47cb-bd78-6ee0c7a04fd4'); {\"model_id\": \"2d2a36e34f9843b5a38a9fe105281093\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 20.939. batch acc: 6.2%, Valid acc: 9.7%. Minibatch loss at step 500: 2.546. batch acc: 70.3%, Valid acc: 75.1%. Minibatch loss at step 1000: 1.520. batch acc: 74.2%, Valid acc: 76.3%. Minibatch loss at step 1500: 1.441. batch acc: 76.6%, Valid acc: 77.8%. Minibatch loss at step 2000: 1.135. batch acc: 79.7%, Valid acc: 77.1%. Minibatch loss at step 2500: 1.225. batch acc: 72.7%, Valid acc: 78.8%. Minibatch loss at step 3000: 0.932. batch acc: 76.6%, Valid acc: 79.4%. Test acc: 86.9% It took only about 3 seconds in my computer to finish the optimization using SGD (which took gradient descent about 20 seconds) and got a even slightly better result. The key of SGD is take randomized samples / mini-batches and feed that into the model every iteration (thus the feed_dict term). 2-layer NN with ReLU units Instead all just linear combination of features, we want to introduce non-linearlity in our logistic regression. By turning the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes, we should be able to improve validation / test accuracy. A 2-layer NN (1-hidden layer NN) look like this: Figure 4 : 1 hidden-layer NN A ReLU activation unit look like this: Figure 5 : ReLU Build compuation graph In this part, use the notation $X$ in replace of dataset . The weights and biases of the hidden layer are denoted as $W1$ and $b1$, and the weights and biases of the output layer are denoted as $W2$ and $b2$. Thus the pre-activation output(logits) of output layer is computed as $ logits = W2 * ReLU(W1 * X + b1) + b2 $ batch_size = 128 num_hidden_unit = 1024 graph = tf . Graph () with graph . as_default (): # placeholder for mini-batch when training tf_train_dataset = tf . placeholder ( tf . float32 , shape = ( image_size * image_size , batch_size )) tf_train_labels = tf . placeholder ( tf . float32 , shape = ( num_labels , batch_size )) # use all valid/test set tf_valid_dataset = tf . constant ( valid_dataset ) tf_test_dataset = tf . constant ( test_dataset ) # initialize weights, biases # notice that we have a new hidden layer so we now have W1, b1, W2, b2 W1 = tf . Variable ( tf . truncated_normal ([ num_hidden_unit , image_size * image_size ])) b1 = tf . Variable ( tf . zeros ([ num_hidden_unit , 1 ])) W2 = tf . Variable ( tf . truncated_normal ([ num_labels , num_hidden_unit ])) b2 = tf . Variable ( tf . zeros ([ num_labels , 1 ])) # training computation logits = tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_train_dataset ) + b1 )) + b2 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = tf . transpose ( tf_train_labels ), logits = tf . transpose ( logits ))) # optimizer optimizer = tf . train . GradientDescentOptimizer ( 0.5 ) . minimize ( loss ) # valid / test prediction - y_hat train_prediction = tf . nn . softmax ( logits , dim = 0 ) valid_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_valid_dataset ) + b1 )) + b2 , dim = 0 ) test_prediction = tf . nn . softmax ( tf . matmul ( W2 , tf . nn . relu ( tf . matmul ( W1 , tf_test_dataset ) + b1 )) + b2 , dim = 0 ) Run the iterations num_steps = 3001 with tf . Session ( graph = graph ) as session : # initialized parameters tf . global_variables_initializer () . run () print ( \"Initialized\" ) # take steps to optimize for step in tnrange ( num_steps ): # generate randomized mini-batches offset = ( step * batch_size ) % ( train_labels . shape [ 1 ] - batch_size ) batch_data = train_dataset [:, offset :( offset + batch_size )] batch_labels = train_labels [:, offset :( offset + batch_size )] feed_dict = { tf_train_dataset : batch_data , tf_train_labels : batch_labels } _ , l , predictions = session . run ( [ optimizer , loss , train_prediction ], feed_dict = feed_dict ) if ( step % 500 == 0 ): print ( 'Minibatch loss at step {} : {:.3f} . batch acc: {:.1f} %, Valid acc: {:.1f} %.' \\ . format ( step , l , accuracy ( predictions , batch_labels ), accuracy ( valid_prediction . eval (), valid_labels ))) print ( 'Test acc: {:.1f} %' . format ( accuracy ( test_prediction . eval (), test_labels ))) Initialized var element = $('#14c2fb9d-1052-4a4e-9b5b-f9ef28003150'); {\"model_id\": \"f379ee5b4ed04eb18e5ee8d6b1e69a17\", \"version_major\": 2, \"version_minor\": 0} Minibatch loss at step 0: 409.203. batch acc: 4.7%, Valid acc: 30.5%. Minibatch loss at step 500: 12.319. batch acc: 75.8%, Valid acc: 80.7%. Minibatch loss at step 1000: 12.638. batch acc: 74.2%, Valid acc: 80.8%. Minibatch loss at step 1500: 7.635. batch acc: 77.3%, Valid acc: 81.2%. Minibatch loss at step 2000: 7.322. batch acc: 80.5%, Valid acc: 81.4%. Minibatch loss at step 2500: 10.451. batch acc: 76.6%, Valid acc: 80.1%. Minibatch loss at step 3000: 3.914. batch acc: 83.6%, Valid acc: 82.7%. Test acc: 88.7% Summary Because we use a more complex model(1 hidden-layer NN), it take a little longer to train, but we're able to gain more performance from logistic regression even with the same hyper-parameter settings (learning rate = 0.5, batch_size=128). Better performance may be gained by tuning hyper parameters of the 2 layer NN. Also notice that by using mini-batch / SGD, we can save lots of time training models and even get a better result. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Deep Learning","url":"https://leemeng.tw/using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent.html","loc":"https://leemeng.tw/using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent.html"},{"title":"Simple Image Recognition using NotMNIST dataset","text":"Today we're going to do some simple image recogintion using NotMNIST dataset. But before creating model for prediction, it's more important to explore, clean and normalize our dataset in order to make the learning go smoother when we actually build predictive models. I motified the notebook from Udacity's online Deep learning course and the objective of this assignment is to learn about simple data curation practices , and familiarize you with some of the data we'll be reusing later. This notebook uses the notMNIST dataset to be used with python experiments. This dataset is designed to look like the classic MNIST dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST. This notebook is mainly foucsing on data preprocessing rather than building models. Workflow Download / load raw notMNIST dataset Drop unreadable images and save the remaining images Combine all images and divide it into testing/validation/test set Shuffle / Randomize the dataset Remove duplicate images appear both in train/test or train/validation set Build simple model for image recognition using different size of training data After finishing this notebook, we learn Use matplotlib to read images, transform them to ndarray and render. Identify whether there exist unbalanced problem for the labels of classification . Understand why it's important to have both valid and test set. Identify the importance of randomizing data for better efficieny when training sequentially. Identify duplicate images between training/test set. Import libraries # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import matplotlib.pyplot as plt import numpy as np import os import sys import tarfile from IPython.display import display from PIL import Image from scipy import ndimage from sklearn.linear_model import LogisticRegression from six.moves.urllib.request import urlretrieve from six.moves import cPickle as pickle # Config the matplotlib backend as plotting inline in IPython % matplotlib inline Dataset Download compressed dataset if the dataset is not available yet First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labeled examples. Given these sizes, it should be possible to train models quickly on any machine. url = 'https://commondatastorage.googleapis.com/books1000/' last_percent_reported = None data_root = './datasets' # Change me to store data elsewhere def download_progress_hook ( count , blockSize , totalSize ): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int ( count * blockSize * 100 / totalSize ) if last_percent_reported != percent : if percent % 5 == 0 : sys . stdout . write ( \" %s%% \" % percent ) sys . stdout . flush () else : sys . stdout . write ( \".\" ) sys . stdout . flush () last_percent_reported = percent def maybe_download ( filename , expected_bytes , force = False ): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os . path . join ( data_root , filename ) if force or not os . path . exists ( dest_filename ): print ( 'Attempting to download:' , filename ) filename , _ = urlretrieve ( url + filename , dest_filename , reporthook = download_progress_hook ) print ( ' \\n Download Complete!' ) statinfo = os . stat ( dest_filename ) if statinfo . st_size == expected_bytes : print ( 'Found and verified' , dest_filename ) else : raise Exception ( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?' ) return dest_filename train_filename = maybe_download ( 'notMNIST_large.tar.gz' , 247336696 ) test_filename = maybe_download ( 'notMNIST_small.tar.gz' , 8458043 ) Found and verified ./datasets/notMNIST_large.tar.gz Found and verified ./datasets/notMNIST_small.tar.gz Extract the dataset into folders by characters Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labeled A through J. num_classes = 10 np . random . seed ( 133 ) def maybe_extract ( filename , force = False ): root = os . path . splitext ( os . path . splitext ( filename )[ 0 ])[ 0 ] # remove .tar.gz if os . path . isdir ( root ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping extraction of %s .' % ( root , filename )) else : print ( 'Extracting data for %s . This may take a while. Please wait.' % root ) tar = tarfile . open ( filename ) sys . stdout . flush () tar . extractall ( data_root ) tar . close () data_folders = [ os . path . join ( root , d ) for d in sorted ( os . listdir ( root )) if os . path . isdir ( os . path . join ( root , d ))] if len ( data_folders ) != num_classes : raise Exception ( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes , len ( data_folders ))) print ( data_folders ) return data_folders train_folders = maybe_extract ( train_filename ) test_folders = maybe_extract ( test_filename ) ./datasets/notMNIST_large already present - Skipping extraction of ./datasets/notMNIST_large.tar.gz. ['./datasets/notMNIST_large/A', './datasets/notMNIST_large/B', './datasets/notMNIST_large/C', './datasets/notMNIST_large/D', './datasets/notMNIST_large/E', './datasets/notMNIST_large/F', './datasets/notMNIST_large/G', './datasets/notMNIST_large/H', './datasets/notMNIST_large/I', './datasets/notMNIST_large/J'] ./datasets/notMNIST_small already present - Skipping extraction of ./datasets/notMNIST_small.tar.gz. ['./datasets/notMNIST_small/A', './datasets/notMNIST_small/B', './datasets/notMNIST_small/C', './datasets/notMNIST_small/D', './datasets/notMNIST_small/E', './datasets/notMNIST_small/F', './datasets/notMNIST_small/G', './datasets/notMNIST_small/H', './datasets/notMNIST_small/I', './datasets/notMNIST_small/J'] Problem 1 - Sample some images in dataset and render them Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display. For each character folder in dataset, randomly choose one picture in it and render them horizontally Keypoints: use os.listdir() to get list of file in a folder use mpl.image.imread to read in image as ndarray, and use plt.imshow to render ndarray as images ! ls ./datasets/notMNIST_small/ A B.pickle D E.pickle G H.pickle J A.pickle C D.pickle F G.pickle I J.pickle B C.pickle E F.pickle H I.pickle characters = 'abcdefghij' . upper () # sub folders to choose images from image_per_folder = 4 # number of images to show for each folder BASE_PATH = './datasets/notMNIST_small/' list_of_images = [] for _ in range ( image_per_folder ): for char in characters : char_folder = BASE_PATH + char + '/' images = os . listdir ( char_folder ) image_file_name = images [ np . random . randint ( len ( images ))] list_of_images . append ( char_folder + image_file_name ) def showImagesHorizontally ( list_of_files ): from matplotlib.pyplot import figure , imshow , axis from matplotlib.image import imread number_of_files = len ( list_of_files ) num_char = len ( characters ) for row in range ( int ( number_of_files / num_char )): fig = figure ( figsize = ( 15 , 5 )) for i in range ( num_char ): a = fig . add_subplot ( 1 , num_char , i + 1 ) image = imread ( list_of_files [ row * num_char + i ]) imshow ( image , cmap = 'gray' ) axis ( 'off' ) showImagesHorizontally ( list_of_images ) Data curation drop unreadable images normalization pickle the normalized data by characters / folders Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size. We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. A few images might not be readable, we'll just skip them. Debugging In case of following error occur: ImportError : Could not import the Python Imaging Library ( PIL ) required to load image files . Follow these steps: pip install pillow replace from IPython.display import display , Image to from IPython.display import display from PIL import Image Keypoints: Normalize image using image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth where pixel_depth == 255 . References https://stackoverflow.com/questions/41124353/importerror-could-not-import-the-python-imaging-library-pil-required-to-load image_size = 28 # Pixel width and height. pixel_depth = 255.0 # Number of levels per pixel. def load_letter ( folder , min_num_images ): \"\"\"Load the data for a single letter label.\"\"\" image_files = os . listdir ( folder ) dataset = np . ndarray ( shape = ( len ( image_files ), image_size , image_size ), dtype = np . float32 ) print ( folder ) num_images = 0 for image in image_files : image_file = os . path . join ( folder , image ) try : image_data = ( ndimage . imread ( image_file ) . astype ( float ) - pixel_depth / 2 ) / pixel_depth if image_data . shape != ( image_size , image_size ): raise Exception ( 'Unexpected image shape: %s ' % str ( image_data . shape )) dataset [ num_images , :, :] = image_data num_images = num_images + 1 except IOError as e : print ( 'Could not read:' , image_file , ':' , e , '- it \\' s ok, skipping.' ) dataset = dataset [ 0 : num_images , :, :] if num_images < min_num_images : raise Exception ( 'Many fewer images than expected: %d < %d ' % ( num_images , min_num_images )) print ( 'Full dataset tensor:' , dataset . shape ) print ( 'Mean:' , np . mean ( dataset )) print ( 'Standard deviation:' , np . std ( dataset )) return dataset def maybe_pickle ( data_folders , min_num_images_per_class , force = False ): dataset_names = [] for folder in data_folders : set_filename = folder + '.pickle' dataset_names . append ( set_filename ) if os . path . exists ( set_filename ) and not force : # You may override by setting force=True. print ( ' %s already present - Skipping pickling.' % set_filename ) else : print ( 'Pickling %s .' % set_filename ) dataset = load_letter ( folder , min_num_images_per_class ) try : with open ( set_filename , 'wb' ) as f : pickle . dump ( dataset , f , pickle . HIGHEST_PROTOCOL ) except Exception as e : print ( 'Unable to save data to' , set_filename , ':' , e ) return dataset_names train_datasets = maybe_pickle ( train_folders , 45000 ) test_datasets = maybe_pickle ( test_folders , 1800 ) ./datasets/notMNIST_large/A.pickle already present - Skipping pickling. ./datasets/notMNIST_large/B.pickle already present - Skipping pickling. ./datasets/notMNIST_large/C.pickle already present - Skipping pickling. ./datasets/notMNIST_large/D.pickle already present - Skipping pickling. ./datasets/notMNIST_large/E.pickle already present - Skipping pickling. ./datasets/notMNIST_large/F.pickle already present - Skipping pickling. ./datasets/notMNIST_large/G.pickle already present - Skipping pickling. ./datasets/notMNIST_large/H.pickle already present - Skipping pickling. ./datasets/notMNIST_large/I.pickle already present - Skipping pickling. ./datasets/notMNIST_large/J.pickle already present - Skipping pickling. ./datasets/notMNIST_small/A.pickle already present - Skipping pickling. ./datasets/notMNIST_small/B.pickle already present - Skipping pickling. ./datasets/notMNIST_small/C.pickle already present - Skipping pickling. ./datasets/notMNIST_small/D.pickle already present - Skipping pickling. ./datasets/notMNIST_small/E.pickle already present - Skipping pickling. ./datasets/notMNIST_small/F.pickle already present - Skipping pickling. ./datasets/notMNIST_small/G.pickle already present - Skipping pickling. ./datasets/notMNIST_small/H.pickle already present - Skipping pickling. ./datasets/notMNIST_small/I.pickle already present - Skipping pickling. ./datasets/notMNIST_small/J.pickle already present - Skipping pickling. Problem 2 Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot. Load test data and show normalized images for each pickled dataset notMNIST_small only in order to prevent memory-insufficient problem when load all train data Keypoints: use pickle.load(open(file_path, 'rb')) to load a pickle create a figure obj and use fig.add_subplot(1, len(images), i+1) to add subplot for each image render ndarray represent images using matplotlob.pyplot.imshow(image) images = [] for file_path in test_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) images . append ( data [ 0 , :, :]) from matplotlib.pyplot import figure , imshow , axis fig = figure ( figsize = ( 15 , 5 )) for i , image in enumerate ( images ): a = fig . add_subplot ( 1 , len ( images ), i + 1 ) imshow ( image , cmap = 'gray' ) axis ( 'off' ) Number of samples in ./datasets/notMNIST_small/A.pickle: 1872 Number of samples in ./datasets/notMNIST_small/B.pickle: 1873 Number of samples in ./datasets/notMNIST_small/C.pickle: 1873 Number of samples in ./datasets/notMNIST_small/D.pickle: 1873 Number of samples in ./datasets/notMNIST_small/E.pickle: 1873 Number of samples in ./datasets/notMNIST_small/F.pickle: 1872 Number of samples in ./datasets/notMNIST_small/G.pickle: 1872 Number of samples in ./datasets/notMNIST_small/H.pickle: 1872 Number of samples in ./datasets/notMNIST_small/I.pickle: 1872 Number of samples in ./datasets/notMNIST_small/J.pickle: 1872 Problem 3 We expect the data to be balanced across classes. By problem 2 above, we already see the data is balanced in test set, verify data is balanced in train data as well for file_path in train_datasets : data = pickle . load ( open ( file_path , 'rb' )) print ( 'Number of samples in {} : {} ' . format ( file_path , data . shape [ 0 ])) Number of samples in ./datasets/notMNIST_large/A.pickle: 52909 Number of samples in ./datasets/notMNIST_large/B.pickle: 52911 Number of samples in ./datasets/notMNIST_large/C.pickle: 52912 Number of samples in ./datasets/notMNIST_large/D.pickle: 52911 Number of samples in ./datasets/notMNIST_large/E.pickle: 52912 Number of samples in ./datasets/notMNIST_large/F.pickle: 52912 Number of samples in ./datasets/notMNIST_large/G.pickle: 52912 Number of samples in ./datasets/notMNIST_large/H.pickle: 52912 Number of samples in ./datasets/notMNIST_large/I.pickle: 52912 Number of samples in ./datasets/notMNIST_large/J.pickle: 52911 Merge seperate character dataset together (for all training/test/valid) Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune train_size as needed. The labels will be stored into a separate array of integers 0 through 9. Also create a validation dataset for hyperparameter tuning. Keypoints: Why we need vaild/test set? Our goal is to make sure the model we train can generalize to brand-new data. If we only divide the whole dataset into training/test set and try to get the best model by tuning parameters on test set, chances are that we actully give the model 'hints' on training set by our eyes. Because eventually, the best model will use the parameters incorporating our knowledge about training dataset, and when the very brand-new data comes, our model can't actually generalize to it and make wrong predictions. So we have to use vaild/dev set to tune our model, and only test the performance on test set to simulate model's 'real-world' performance after deploying it. def make_arrays ( nb_rows , img_size ): if nb_rows : dataset = np . ndarray (( nb_rows , img_size , img_size ), dtype = np . float32 ) labels = np . ndarray ( nb_rows , dtype = np . int32 ) else : dataset , labels = None , None return dataset , labels def merge_datasets ( pickle_files , train_size , valid_size = 0 ): num_classes = len ( pickle_files ) valid_dataset , valid_labels = make_arrays ( valid_size , image_size ) train_dataset , train_labels = make_arrays ( train_size , image_size ) vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v , start_t = 0 , 0 end_v , end_t = vsize_per_class , tsize_per_class end_l = vsize_per_class + tsize_per_class for label , pickle_file in enumerate ( pickle_files ): try : with open ( pickle_file , 'rb' ) as f : letter_set = pickle . load ( f ) # let's shuffle the letters to have random validation and training set np . random . shuffle ( letter_set ) if valid_dataset is not None : valid_letter = letter_set [: vsize_per_class , :, :] valid_dataset [ start_v : end_v , :, :] = valid_letter valid_labels [ start_v : end_v ] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set [ vsize_per_class : end_l , :, :] train_dataset [ start_t : end_t , :, :] = train_letter train_labels [ start_t : end_t ] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e : print ( 'Unable to process data from' , pickle_file , ':' , e ) raise return valid_dataset , valid_labels , train_dataset , train_labels train_size = 200000 valid_size = 10000 test_size = 10000 valid_dataset , valid_labels , train_dataset , train_labels = merge_datasets ( train_datasets , train_size , valid_size ) _ , _ , test_dataset , test_labels = merge_datasets ( test_datasets , test_size ) print ( 'Training:' , train_dataset . shape , train_labels . shape ) print ( 'Validation:' , valid_dataset . shape , valid_labels . shape ) print ( 'Testing:' , test_dataset . shape , test_labels . shape ) Training: (200000, 28, 28) (200000,) Validation: (10000, 28, 28) (10000,) Testing: (10000, 28, 28) (10000,) Randomize data Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match. The purpose of randomizing data is to perserve the assumption that we get the data randomly as predict phase. We don't want to train models sequantially on training instances like AAA ... BBB ... CCC. Keypoints: train set and test set should apply the same shuffle shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] Although we do only one randomization here, we may do it multiple times when we are going to use the same dataset many times. def randomize ( dataset , labels ): permutation = np . random . permutation ( labels . shape [ 0 ]) shuffled_dataset = dataset [ permutation ,:,:] shuffled_labels = labels [ permutation ] return shuffled_dataset , shuffled_labels train_dataset , train_labels = randomize ( train_dataset , train_labels ) test_dataset , test_labels = randomize ( test_dataset , test_labels ) valid_dataset , valid_labels = randomize ( valid_dataset , valid_labels ) Problem 4 - Sanity check after shuffling dataset Convince yourself that the data is still good after shuffling! Randomly sample data instances to make sure X is corresponding to y for both training and test set after shuffling. Keypoints: label is start from 0 to 9 for A to J lookup_labels = { k : v for ( k , v ) in zip ( np . arange ( 10 ), 'ABCDEFGHIJ' )} def sanity_check ( X , y , s = None ): print ( s ) m1 , m2 = X . shape [ 0 ], y . shape [ 0 ] assert ( m1 == m2 ) # randomly choose 10 images to check label indices = np . random . randint ( 0 , m1 , size = 10 ) fig = plt . figure ( figsize = ( 15 , 5 )) for i , idx in enumerate ( indices ): fig . add_subplot ( 1 , len ( indices ), i + 1 ) plt . imshow ( X [ idx , :, :], cmap = 'gray' ) plt . title ( lookup_labels [ y [ idx ]]) plt . axis ( 'off' ) sanity_check ( train_dataset , train_labels , 'Training dataset:' ) Training dataset: sanity_check ( test_dataset , test_labels , 'Test dataset:' ) Test dataset: Serialize dataset for later usage Everything looks good, finally, let's save the data for later reuse: pickle_file = os . path . join ( data_root , 'notMNIST.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : valid_dataset , 'valid_labels' : valid_labels , 'test_dataset' : test_dataset , 'test_labels' : test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 690800503 Problem 5 - Remove overlapping samples in test/valid set By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples. Optional questions: What about near duplicates between datasets? (images that are almost identical) Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments. Find duplicate images in test set I used Cosine Similarity to measure whether image A in training set is identical to those image B in valid/test set. As formula suggested, I reshape every image in training set into column vectors, and do the same for images in valid/test set and compute similarity between the vectors. Although we can compute cosine similarity for every (train image vector, test image vector) explicitly, it's better to use vectorization to speed up computation since that we have 200,000 training images and 10,000 valid/test images. (Although it still take about 10 minutes to run in my computer) Keypoints: Vectorize both training/test images and compute cosine similarity using cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y)) . The output matrix cosine_sim will be shape (m1, m2) where m1 is the number of training images and m2 the number of test images. cosine_matrix(i, j) mean the cosine similarity between training image #i and test image #j. %%time def get_duplicate_data ( source_dataset , target_dataset , threshold = 1 , num_duplicate_to_show = 0 ): X = source_dataset . reshape ( source_dataset . shape [ 0 ], - 1 ) Y = target_dataset . reshape ( target_dataset . shape [ 0 ], - 1 ) assert ( X . shape [ 1 ] == Y . shape [ 1 ]) dim = X . shape [ 1 ] cosine_sim = np . inner ( X , Y ) / np . inner ( np . abs ( X ), np . abs ( Y )) assert ( cosine_sim . shape == ( X . shape [ 0 ], Y . shape [ 0 ])) # for each image in training set, find corresponding duplicate in test/valid set dup_target_indices = [] show_duplicate_counter = 0 for source_idx in range ( cosine_sim . shape [ 0 ]): dup_indices = list ( np . where ( cosine_sim [ source_idx , :] >= threshold )[ 0 ]) # render duplicate images when is available. may omit if visual output is not required if dup_indices and num_duplicate_to_show and ( show_duplicate_counter < num_duplicate_to_show ): # show only non-redudent duplicate images for i in dup_indices : if i in dup_target_indices : dup_indices . remove ( i ) if not dup_indices : continue if len ( dup_indices ) == 1 : fig = plt . figure ( figsize = ( 3 , 15 )) fig . add_subplot ( 1 , len ( dup_indices ) + 1 , 1 ) plt . imshow ( source_dataset [ source_idx , :, :], cmap = 'gray' ) plt . title ( 'Source: ' + str ( source_idx )) plt . axis ( 'off' ) for i , target_idx in enumerate ( dup_indices ): fig . add_subplot ( 1 , len ( dup_indices ) + 1 , i + 2 ) plt . imshow ( target_dataset [ target_idx , :, :], cmap = 'gray' ) plt . title ( 'Target: ' + str ( target_idx )) plt . axis ( 'off' ) show_duplicate_counter += 1 dup_target_indices . extend ( dup_indices ) return list ( set ( dup_target_indices )) dup_indices_test = get_duplicate_data ( train_dataset , test_dataset , num_duplicate_to_show = 5 ) print ( 'Number of duplicates in test dataset: {} ' . format ( len ( dup_indices_test ))) Number of duplicates in test dataset: 1768 CPU times: user 5min 11s, sys: 4min 17s, total: 9min 29s Wall time: 7min 40s Duplicate images in validation set %%time dup_indices_valid = get_duplicate_data ( train_dataset , valid_dataset , num_duplicate_to_show = 5 ) print ( 'Number of duplicates in validation dataset: {} ' . format ( len ( dup_indices_valid ))) Number of duplicates in validation dataset: 1507 CPU times: user 7min 52s, sys: 5min 7s, total: 12min 59s Wall time: 8min 57s Serialize sanitized dataset for later model performance comparison Remove duplicates in test/valid set Save dataset Keypoints: Do the same operation to both X and y print ( \"Number of duplicate images in test set: {} \" . format ( len ( dup_indices_test ))) print ( \"Number of duplicate images in valid set: {} \" . format ( len ( dup_indices_valid ))) Number of duplicate images in test set: 1768 Number of duplicate images in valid set: 1507 non_duplicate_indices = [ i for i in range ( test_dataset . shape [ 0 ]) if not i in dup_indices_test ] sanitized_test_dataset = test_dataset [ non_duplicate_indices , :, :] sanitized_test_labels = test_labels [ non_duplicate_indices ] non_duplicate_indices = [ i for i in range ( valid_dataset . shape [ 0 ]) if not i in dup_indices_valid ] sanitized_valid_dataset = valid_dataset [ non_duplicate_indices , :, :] sanitized_valid_labels = valid_labels [ non_duplicate_indices ] pickle_file = os . path . join ( data_root , 'notMNIST_sanitized.pickle' ) try : f = open ( pickle_file , 'wb' ) save = { 'train_dataset' : train_dataset , 'train_labels' : train_labels , 'valid_dataset' : sanitized_valid_dataset , 'valid_labels' : sanitized_valid_labels , 'test_dataset' : sanitized_test_dataset , 'test_labels' : sanitized_test_labels , } pickle . dump ( save , f , pickle . HIGHEST_PROTOCOL ) f . close () except Exception as e : print ( 'Unable to save data to' , pickle_file , ':' , e ) raise statinfo = os . stat ( pickle_file ) print ( 'Compressed pickle size:' , statinfo . st_size ) Compressed pickle size: 680517003 Problem 6 - Build naive classifier using logistic regression Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it. Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model. Optional question: train an off-the-shelf model on all the data! from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score plt . style . use ( 'ggplot' ) np . random . seed ( 42 ) train_sizes = [ 100 , 1000 , 50000 , 100000 , 200000 ] # train models using different size of training set test_scores , test_scores_sanitized = [[] for _ in range ( 2 )] for train_size in train_sizes : # random choose #train_size of training instances indices = np . random . randint ( 0 , train_dataset . shape [ 0 ], train_size ) # reshape images to (train_size, dim * dim) for easier processing X = train_dataset [ indices , :, :] \\ . reshape ( - 1 , train_dataset . shape [ 1 ] * train_dataset . shape [ 2 ]) y = train_labels [ indices ] # train model clf = ( LogisticRegression ( random_state = 10 , solver = 'lbfgs' , multi_class = 'multinomial' ) . fit ( X , y )) # test on original test set and the sanitized one y_pred = clf . predict ( test_dataset . reshape ( test_dataset . shape [ 0 ], - 1 )) y_pred_sanitized = clf . predict ( sanitized_test_dataset . reshape ( sanitized_test_dataset . shape [ 0 ], - 1 )) test_score = accuracy_score ( y_pred , test_labels ) test_score_sanitized = accuracy_score ( y_pred_sanitized , sanitized_test_labels ) test_scores . append ( test_score ) test_scores_sanitized . append ( test_score_sanitized ) # print(classification_report(test_labels, y_pred)) # print(accuracy_score(test_labels, y_pred)) plt . figure ( figsize = ( 7 , 7 )) plt . xlabel ( 'Training size' , fontsize = 20 ) plt . ylabel ( 'Accuracy' , fontsize = 20 ) plt . xticks ( fontsize = 15 ) plt . yticks ( fontsize = 15 ) for x , y in zip ( train_sizes , test_scores ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) for x , y in zip ( train_sizes , test_scores_sanitized ): plt . text ( x + 50 , y , ' {:.2f} ' . format ( y )) plt . scatter ( train_sizes , test_scores , label = 'Test score' , color = 'blue' ); plt . scatter ( train_sizes , test_scores_sanitized , label = 'Test score(Sanitized)' , color = 'red' ); plt . legend ( loc = 4 ) plt . title ( 'Test set Accuracy vs Training size' , fontsize = 25 ); As show above, there are seveal things worth mentioning: Our model became better on classifiying labels in test set by using more data in training phase. By training a simple logistic regression model on all available training data, we can expect to get about 90% accuracy. There is a performance gap (2~3% on accuracy) between test set with duplicate images and the one without. Depends on our application, choose the one which best fit our use case. Notice that we just want to have a off-the-shelf model quickly, so we don't even tune hyper-parameters using validation set. We may be able to further improve model's predictive performance by tuning hyper-parameters or using Neural Network in another notebook. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine learning","url":"https://leemeng.tw/simple-image-recognition-using-notmnist-dataset.html","loc":"https://leemeng.tw/simple-image-recognition-using-notmnist-dataset.html"},{"title":"Purpose of this blog","text":"第一篇文章做一點 blog 的簡介，打算把自己在學 data science 還有 machine learning 過程中寫的筆記還有在 MOOC 上課的 code (主要是 jupyter notebook) 記錄下來方便自己以後搜尋。 雖然目前為止我都將 jupyter notebook render 成 HTML 然後存到 Evernote 搜尋, 但是如果該 notebook 一直更新的話就變得很不實際.. </br> 這次利用 python 的 pelican 將 jupyter notebook 轉成靜態網頁, 讓我可以不斷更新 notebooks 而且也希望 code 可以幫助到其他也在做 data science / machine learning 的人當作一些參考。 blog 的走向目前看來應該會是中英夾雜 .. This is the first post of my blog. </br> I decided to record my learning path toward data scientist / machine learning engineer and make it easier for my future self to review what I have learnt. This blog will include codes from the courses on MOOC like Coursera, Udacity and also some pet projects. It would also be wonderful if these code and thought can help someone who want to become a data scientist. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/purpose-of-this-blog.html","loc":"https://leemeng.tw/purpose-of-this-blog.html"},{"title":"Generate Anime using CartoonGAN and TensorFlow 2.0","text":"What images would you choose to transform if you could turn any of them into anime using AI? In this post, I will briefly introduce a CartoonGAN implemented in TensorFlow 2.0 Alpha by my friend mnicnc404 and me. I will also demo a TensorFlow.js app which allows you to generate anime on your browser directly. CartoonGAN (original paper) was published in 2018 CVPR and is a Generative Adversarial Network (GAN) which attempts to transform real world images into cartoon-style images. 您的瀏覽器不支援影片標籤，請留言通知我：S Top-left corner is real world image, and the other 3 images are generated by CartoonGAN using different anime styles The ideas demonstrated in the paper are very interesting and the authors also show good results using two of my favorite Japanese animator styles: Shinkai Makoto and Miyazaki Hayao . Therefore, I decided to build a TensorFlow.js app to allow anyone to try CartoonGAN with ease. Choose your favorite anime style and upload one image, it's just that simple. 您的瀏覽器不支援影片標籤，請留言通知我：S Under the hood, TensorFlow.js runs in your browser whilst: downloading the pretrained models cartoonizing uploaded images To get the best performance, it is highly recommended that you: try the app on a desktop/laptop rather than mobile devices (to speed up transformation) try the app when network speed is fast（to minimalize loading time） Try CartoonGAN yourself Without further ado, here is the tfjs app: Upload Image Shinkai Makoto Style Miyazaki Hayao Style Hosoda Mamoru Style Paprika Style How does your anime look? If the app seems to be stuck: Loading Models : simply means it's still downloading the model Cartoonizing images : it might indicate there is no sufficient computation power on your device You can still generate anime in the next section using TensorFlow if tfjs doesn't work for you. Generate anime using TensorFlow 2 In addition to TensorFlow.js, we also implemented CartoonGAN using TensorFlow 2.0 Alpha . If you want to transform larger images and/or gifs, you can run this Colab notebook : 您的瀏覽器不支援影片標籤，請留言通知我：S You can use CartoonGAN to transform any images with our colab notebook Google Colaboratory is a cloud Jupyter notebook environment allowing anyone to start their machine learning projects with free GPU available. In this notebook , everything you need is set up for you: Build TensorFlow 2.0 environment Clone our github repo and download pretrained models Download arbitrary images on the web Transform the images using CartoonGAN You get the idea. All you have to do is open the notebook and generate your anime. It's that simple. Gallery: some anime we generated Is better to share one's happiness than to enjoy it alone. In this section, we simply share some anime generated with CartoonGAN by ourselves. For easy comparison, every image below is divided into 4 parts where: top-left: original real world image top-right: Shinkai Makoto style bottom-left: Miyazaki Hayao style bottom-right: Hosoda Mamoru style Click the arrows on the left/right to view different results: <!-- https://www.w3schools.com/w3css/w3css_slideshow.asp --> .w3-content, .w3-auto { margin-left: auto; margin-right: auto } .w3-content { max-width: 980px } .w3-display-container:hover .w3-display-hover { display: block } .w3-display-container:hover span.w3-display-hover { display: inline-block } .w3-display-container { position: relative } .w3-button:hover { color: #000!important; background-color: inherit; } .w3-button { border: none; display: inline-block; padding: 8px 16px; vertical-align: middle; overflow: hidden; text-decoration: none; color: inherit; background-color: inherit; text-align: center; cursor: pointer; white-space: nowrap } .w3-button { -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none } .w3-button:disabled { cursor: not-allowed; opacity: 0.3 } .w3-display-left { position: absolute; top: 50%; left: 0%; transform: translate(0%, -50%); -ms-transform: translate(-0%, -50%) } .w3-display-right { position: absolute; top: 50%; right: 0%; transform: translate(0%, -50%); -ms-transform: translate(0%, -50%) } .mySlides {display:none;} Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. Your browser doesn't support video tag. ❮ ❯ var slideIndex = 1; showDivs(slideIndex); function plusDivs(n) { showDivs(slideIndex += n); } function showDivs(n) { var i; var x = document.getElementsByClassName(\"mySlides\"); if (n > x.length) {slideIndex = 1} if (n < 1) {slideIndex = x.length} for (i = 0; i < x.length; i++) { x[i].style.display = \"none\"; } x[slideIndex-1].style.display = \"block\"; } Marvel movies, cute cats, scenery or pop idols, the sky is the limit. When trained, we can use CartoonGAN to combine everything with anime. Using Python script in our github repo , you can generate these results with just one command: python cartoonize.py \\ --styles shinkai hayao hosoda You can imagine apps with similar features will become much more powerful in the near future. Everyone will be able to generate their own anime using various styles, while animators will be able to test their new ideas and draw drafts much more faster than ever before. Generative models like CartoonGAN can show us many possibilities and inspire us. Train your own CartoonGAN You may noticed that we've put a lot of focus on actual application rather than the algorithmic details for CartoonGAN. That's because: I suspect most readers will be more interested in how to use CartoonGAN as a service rather than how to train a model themselves There are already lots of great learning resources on how to train your GANs on the web For those who are interested in implementation details of CartoonGAN, we suggest you take a look at our github repo . If you have your own datasets, you can even train your own CartoonGAN simply in one command: Our Python script provides detailed messages which allow you to understand what is happening behind the scenes Our project can also be used to learn the latest version of TensorFlow since everything is implemented in Tensorflow 2.0 alpha : Use tf.keras to implement custom layers and GAN Use tf.data to properly load and process large amount of images Write custom train logic and make computation faster using tf.function Use TensorBoard to monitor model performance in real-time As some of you already knew, it is not easy to train a GAN. You must keep monitoring your model's performance to decide how to adjust your hyper-parameters or even model architecture accordingly. In our training script , TensorBoard is integrated perfectly so you can monitor your model's performance easily: One result of many experiments we tried on TensorBoard In addition to metrics and loss functions, it is good practice to keep an eye on the images generated by GAN during training as well. Using our script, monitoring generated images on TensorBoard is a no-brainer: the sooner you get feedback, the sooner you can get new ideas and improve your models Although we only record CartoonGAN-specific metrics and images here, you can easily apply same technique to monitor any other models you like. Finally, in order to get a sense of how CartoonGAN is doing during the training phase, we can save some images and use them as a validation set. The idea here is to \"test\" CartoonGAN using same images periodically so that we can observe how it improves over time: 您的瀏覽器不支援影片標籤，請留言通知我：S CartoonGAN becomed better and better transforming validation set into anime Even before the training is finished, you can tell that CartoonGAN somehow learns to transform real world images into cartoon-like images with clear edges and smooth color shading. Training a GAN like CartoonGAN is far more difficult than training a simple classifier . But in our github repo , we have tried our best to make the TensorFlow code clear and easy to understand, hoping it will help more people to get started. Needless to say, you will be able to get a better understanding of our code if you already know some basics. In the next section, I will list some good learning resources for your reference. Recommended learning resources Recently, deep learning-based generative models are very popular and GAN is one of the most exciting research areas. The CartoonGAN we saw in this post is just a simple application in the ever-growing GAN zoo . In a typical GAN setting, there are two independent neural networks, called generator and discriminator, competing with each other For those who want to learn more about GAN, I recommend the following resources: MIT 6.S191 Deep Generative Models GAN Lab let you train GANs on browser Andrej Karpathy's online GAN demo TensorFlow offical tutorial teach you how to generate MNIST using DCGAN Synced: Reproducing Japanese Anime Styles With CartoonGAN AI Open Questions about Generative Adversarial Networks MIT 6.S191 Deep Generative Models is a good place to start learning GAN concepts Research on generative models and GAN are evolving rapidly, but I believe you will be able to implement some simple GANs and understand code written by others after studying these learning resources. To sum up AI technology and applications like CartoonGAN should be more accessable to everyone, not only researchers or machine learning practitioners. With this in mind, I built the project to let you create your own anime in no time and hope it can encourge some of you to explore more about GAN and generative models, and create more interesting AI applications in the future. Many thanks to the authors of CartoonGAN, TensorFlow/TensorFlow.js teams and my friend mnicnc404 . Oh! Also don't forget to share what anime you generate! You can find me on Twitter or Facebook . Let the world be filled with anime! :D if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Miscellaneous","url":"https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2-en.html","loc":"https://leemeng.tw/generate-anime-using-cartoongan-and-tensorflow2-en.html"}]};