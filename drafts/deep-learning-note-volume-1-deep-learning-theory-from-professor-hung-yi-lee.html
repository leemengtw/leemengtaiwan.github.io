<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="zh-hant-tw"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="zh-hant-tw"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="zh-hant-tw">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Lee Meng" />
<title>LeeMeng - 深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory</title>
    <!--- article-specific meta data
    ================================================== -->
        <meta name="description" content="" />
        <meta name="keywords" content="" />


    <!--- Open Graph Object metas
    ================================================== -->
        <meta property="og:image" content="https://leemeng.tw/theme/images/background/default.jpg" />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://leemeng.tw/drafts/deep-learning-note-volume-1-deep-learning-theory-from-professor-hung-yi-lee.html" />
        <meta property="og:title" content="深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory" />
        <meta property="og:description" content="" />

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <!--for customized css in individual page-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/bootstrap.min.css">

    <!--for showing toc navigation which slide in from left-->
        <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/toc-nav.css">

    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/base.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/vendor.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="https://leemeng.tw/theme/css/ipython.css">
    <link rel="stylesheet" type="text/css" href='https://leemeng.tw/theme/css/progress-bar.css' />


    <!--TiqueSearch-->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/normalize.css">
    <link rel="stylesheet" href="https://leemeng.tw/theme/tipuesearch/css/tipuesearch.css">

    <!-- script
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/modernizr.js"></script>
    <script src="https://leemeng.tw/theme/js/pace.min.js"></script>


    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon"/>
    <link rel="icon" href="../theme/images/favicon.ico" type="image/x-icon"/>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106559980-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106559980-1');
</script>



</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="../index.html"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
        </div>
<!--navigation bar ref: http://jinja.pocoo.org/docs/2.10/tricks/-->



<nav class="header-nav-wrap">
    <ul class="header-nav">
        <li>
            <a href="../index.html#home">Home</a>
        </li>
        <li>
            <a href="../index.html#about">About</a>
        </li>
        <li>
            <a href="../index.html#projects">Projects</a>
        </li>
        <li class="current">
            <a href="../blog.html">Blog</a>
        </li>
        <li>
            <a href="https://demo.leemeng.tw">Demo</a>
        </li>
        <li>
            <a href="../books.html">Books</a>
        </li>
        <li>
            <a href="../index.html#contact">Contact</a>
        </li>

    </ul>

    <!--<div class="search-container">-->
        <!--<form action="../search.html">-->
            <!--<input type="text" placeholder="Search.." name="search">-->
            <!--<button type="submit"><i class="im im-magnifier" aria-hidden="true"></i></button>-->
        <!--</form>-->
    <!--</div>-->

</nav>
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->



    <!--TOC navigation displayed when clicked from left-navigation button-->
    <div id="tocNav" class="overlay" onclick="closeTocNav()">
      <div class="overlay-content">
        <div id="toc"><ul><li><a class="toc-href" href="#" title="深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory">深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory</a><ul><li><a class="toc-href" href="#Why-Deep-Structure?" title="Why Deep Structure?">Why Deep Structure?</a></li><li><a class="toc-href" href="#Optimization" title="Optimization">Optimization</a></li><li><a class="toc-href" href="#Generalization" title="Generalization">Generalization</a></li><li><a class="toc-href" href="#Computational-Graph-&amp;-Backpropagation" title="Computational Graph &amp; Backpropagation">Computational Graph &amp; Backpropagation</a></li><li><a class="toc-href" href="#Sequence-to-sequence-Learning" title="Sequence-to-sequence Learning">Sequence-to-sequence Learning</a></li><li><a class="toc-href" href="#Attention-based-Model" title="Attention-based Model">Attention-based Model</a></li><li><a class="toc-href" href="#Recurrent-Neural-Network" title="Recurrent Neural Network">Recurrent Neural Network</a><ul><li><a class="toc-href" href="#ML-course-21-1,-49-minutes" title="ML course 21-1, 49 minutes">ML course 21-1, 49 minutes</a></li><li><a class="toc-href" href="#ML-course-21-2,-1-hr-30-minutes" title="ML course 21-2, 1 hr 30 minutes">ML course 21-2, 1 hr 30 minutes</a></li></ul></li><li><a class="toc-href" href="#ML-Lecture-8-1:-&ldquo;Hello-world&rdquo;-of-deep-learning_1" title="ML Lecture 8-1: &ldquo;Hello world&rdquo; of deep learning">ML Lecture 8-1: &ldquo;Hello world&rdquo; of deep learning</a></li><li><a class="toc-href" href="#NMT" title="NMT">NMT</a></li></ul></li></ul></div>
      </div>
    </div>


    <article class="blog-single">

        <!-- page header/blog hero, use custom cover image if available
        ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://leemeng.tw/theme/images/background/default.jpg)">

            <div class="row page-header__content narrow">
                <article class="col-full">
                    <div class="page-header__info">
                        <div class="page-header__cat">
                        </div>
                    </div>
                    <h1 class="page-header__title">
                        <a href="https://leemeng.tw/drafts/deep-learning-note-volume-1-deep-learning-theory-from-professor-hung-yi-lee.html" title="">
                            深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory
                        </a>
                    </h1>
                    <ul class="page-header__meta">
                        <li class="date">2018-11-09 (Fri)</li>
                        <li class="page-view">
                            1 views
                        </li>
                    </ul>

                </article>
            </div>

        </div> <!-- end page-header -->

        <div class="KW_progressContainer">
            <div class="KW_progressBar"></div>
        </div>

        <div class="row blog-content" style="position: relative">
<div id="left-navigation">

    <div id="search-wrap">
        <i class="im im-magnifier" aria-hidden="true"></i>
        <div id="search">
            <form action="../search.html">
            <div class="tipue_search_right"><input type="text" name="q" id="tipue_search_input" pattern=".{2,}" title="想搜尋什麼呢？（請至少輸入兩個字）" required></div>
            </form>
        </div>
    </div>

    <div id="toc-wrap">
        <a title="顯示/隱藏 文章章節">
            <i class="im im-menu" aria-hidden="true" onclick="toggleTovNav()"></i>
        </a>
    </div>


</div>

            <div class="col-full blog-content__main">

                
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">-</span> <span class="n">why</span> <span class="n">empirical</span> <span class="n">important</span><span class="err">?</span> <span class="n">we</span> <span class="n">just</span> <span class="n">can</span><span class="s1">'t see all the possiblities and came up with very general theories. experiement helps to understand DL.</span>
<span class="o">-</span> <span class="n">遠古時代</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">dl</span> <span class="n">發展真的很快</span><span class="err">，</span><span class="n">很多</span> <span class="n">empirical</span> <span class="n">result</span> <span class="n">又過時了</span>
<span class="o">-</span> <span class="n">大家很努力在搞清楚</span> <span class="n">DL</span> <span class="n">的</span> <span class="n">NN</span> <span class="n">到底內部在做什麼</span><span class="err">（</span><span class="n">畫</span> <span class="n">error</span> <span class="n">surface</span> <span class="n">etc</span><span class="p">)</span>
<span class="o">-</span> <span class="n">如果說完全不懂線性代數或是微積分也沒關係也沒有錯</span><span class="err">，</span><span class="n">但是內心可能覺得不踏實</span>
<span class="o">-</span> <span class="n">把複雜的東西講得聽起來很複雜很簡單</span><span class="err">，</span><span class="n">講得簡單才是厲害</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep Learning Theory 10 影片</p>
<ul>
<li>400 minutes 總影片長度, 換幾篇 paper?</li>
<li>+60 mins </li>
<li>秒複習、怒算都是幫你抽象化不必要的細節（雖然功課可能有）</li>
<li>deep strucuture, optimization, generalization</li>
</ul>
<p>觀看人數</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-Deep-Structure?">Why Deep Structure?<a class="anchor-link" href="#Why-Deep-Structure?">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>現實世界中，我們常常要的要一個可以把 x 對應的 y 的 fn 。而給定一個 network structure，不管其是 shallow 或是 deep，理論上只要架構夠大，都能讓潛在包含的 function space 包含想要 fit 的 function 。那為何 deep 好？</p>
<ul>
<li>（抽象化）我們在討論這章的時候，也不考慮實際上 shallow / deep 架構所構成的 function space 怎麼做 optimize / generalization 來實際找到該 target fn, 而是先假設只要包含了，最後就能找到一組參數 fit 出 target function</li>
<li>可以更有效率地產生更多 piece-wise linear functions, 同樣 performance 下， shallow 需要的 neuron 數目是 deep 的指數倍</li>
<li>有些在 deep 才能 fit 的 functions 你在 shallow 無法 fit</li>
</ul>
<p>例子：</p>
<pre><code>- 獵人比喻 swallow network 的最好/最差狀態

</code></pre>
<p>結論：當你想要 fit 一個複雜的 fn, deep is exponentially better than shallow</p>
<pre><code>- 以相同的 performance 或是相同的 neurons 數來比較
- 就像是相加 vs 相乘能力（以 ReLU 來看)</code></pre>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Optimization">Optimization<a class="anchor-link" href="#Optimization">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>就是從（deep structure 組成的） function space 裡頭找出 function star 的方法</p>
<ul>
<li>（抽象化）optimization 不等於 learning<ul>
<li>給定一個 loss function, optimization 專注在找到能讓 loss function 最小的一組參數（fit training data well）。而至於使用該參數預測測試資料有沒有 overfit 問題（learning）不是現在這章的關注重點</li>
<li>反過來說，有時候找到 global minima 不是最重要的，而且反而容易 overfitting. local minimal 可能就足夠</li>
</ul>
</li>
</ul>
<p>例子：</p>
<pre><code>- 八門遁甲不是一個非常好的招式（deep structure 就算）

</code></pre>
<ul>
<li>training stuck 不一定就代表走到 critical point, 梯度為 0（有可能 loss 很低沒啥改變，但是 grad 的值還是很大）</li>
<li>且 training 過程，遭遇到的 gradient 是上上下下，而不是跟 loss 一樣直線下降</li>
<li>如果確定是遇到 critical point, 可以使用 Hessian Matrix 確認是 local minimum / saddle point / local maximum</li>
<li>ReLU network / non-linear netowrk 有 local minima, 要避免撞到這些盲點，選好 initial 的位置很重要；資料可能也有影響</li>
<li>而 empirical 結果以及一些猜測顯示，我們訓練的 network structure 越大，越不容易遇到 local minima, 而且 local minima 在 loss 越低的時候越容易出現，local minima 的結果可能已經跟 global minima 一樣好。只是未來需要更多研究  </li>
<li>長相非常不一樣的 networks 可以表現得非常像。因此不同 optimization strategy （adam, SGD）走到 solution 的方式的差異（最終流域、走的方向不同）可能比我們想像大，不止是速度的差別而已</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>
                        就算你的 training stuck，也不代表你遇到 criticial point (要檢查 grad）；就算玉到 critical point, 也不代表是 local minima（要檢查 hessian matrix）
                        <br/>
<br/>
</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generalization">Generalization<a class="anchor-link" href="#Generalization">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>在 ML 的觀念，當兩個 model 的 training error 差不多的時候，選 capacity 小的，比較不會 overfitting（但這不一定 apply 到 DL）</li>
<li>DL 有非常高的 capacity, 但卻沒有我們想像中的那麼容易 overfitting，仿佛自帶 regularization, 還有很多謎在裡頭<ul>
<li>例子：MNIST training error 0，但 test error 為零的時候，在一般的 ML 你都會想要減少 model capacity，但在 DL 裡頭，你再增加 model 複雜度還反而降低 test error，學得更好 ..</li>
</ul>
</li>
<li>但同時我們也看到 NN 可以硬記所有資料，就算你給他 random label，他也能在 training 時學到 100%</li>
<li>儘管我們能透過 validation set 來選擇兩個在 training set 都表現完美的 network, 有一些額外的 indicators 的話，可以讓我們有更多選擇一個更能 generalize 的 nn 的 guideline，並在未來訓練模型的時候就塞一些東西進去（比方說 regularization term, 然後這也成為一個研究領域），來讓 NN 的 indicators 符合我們想要的樣子<ul>
<li>sensitivity 的 empirical result 讓我們看到 generalization gap 跟 jacob 是成正比的。這代表我們可以在沒有正確 label 的情況下，針對一筆 test data 算 sensitivity，來預測 network 能答對的機率。如果答對的希望渺茫，我們能交給人工判斷，在實務上是一個非常好的應用</li>
<li>sharpness：直覺上找到的 local minima 附近的 error surface 越平滑，越能 generalize<ul>
<li>training / test set 的分佈有差，但是如果是平滑的 local minima 其跟 test set 分配的差距可能比較小</li>
<li>batch_size 小的 train 出來的 NN 的 generalization 能力比較強（雖然在 training 的時候都能表現非常好）<ul>
<li>有假說是，比較大的 batch_size 讓我們比較容易跑到本來不容易跑到的 shape local minima</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Computational-Graph-&amp;-Backpropagation">Computational Graph &amp; Backpropagation<a class="anchor-link" href="#Computational-Graph-&amp;-Backpropagation">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>computational graph 是一種描述 function 的語言，而用 graph 的好處是，可以很簡單地利用 chain rule 來怎麼算 node 之間的微分</li>
<li>在計算梯度時，從 root 開始用 reserve mode 算是比較有效率的。因為基本上我們在 train NN 都是要計算 loss function，而 loss function 以 graph 表示就是一個 scaler, 也就是一個 root, 剛好適合用來反向傳播</li>
<li>Jacobian Matrix 就是 vector 跟 vector 的偏微分</li>
<li>就算是 computational graph, 事實上要算出 grad，也是要先 forward pass，然後再做一次 backward pass</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>（看了 ML 21-1 &amp; 21-2 再從 52:40 繼續看）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 computation graph 裏頭，何謂一個 function 是自己定義的</p>
<p>RNN 考慮的是整個 sequence 的 cost, 所以在每個時間點算出的 Ci 要全部加起來</p>
<p>RNN 雖然是在不同時間點共用參數，但要計算出特定一個參數對 cost function 的影響，要將不同時間點下的 wh 對 C 的偏微分全部加起來，才是 wh 對 C 的總影響。而因為每一個時間點的 hidden ht (memory) 會收到前一個 memory ht-1 的值的影響，ht/ht-1 的偏微分讓 RNN 不會 train, 所以會考慮使用 LSTM. (LSTM 的其中一個初衷是解決 gradient vanishing 的問題）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sequence-to-sequence-Learning">Sequence-to-sequence Learning<a class="anchor-link" href="#Sequence-to-sequence-Learning">&para;</a></h2><p>1 hr 52 minutes</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>RNN 強在不管 input sequence 有多長，只要定義好一個 function，就能以不變應萬變，只用該 function 處理整個 sequence。</p>
<p>RNN 橫向代表的是同個 function 在不同時間的變化，而 Deep RNN 就是在縱向多定義幾個 function，吃前一個 function 的 output。</p>
<p>Bi-directional RNN 看 5:40</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sequence generation  大致上就等於 sequence to sequence learning.</p>
<p>而 sequence generation 的核心想法就是在每個時間點丟進上次個時間點 model 產生的 character/ word/sequence (以 vector 表示），讓它以此產生下一個詞的 distribution,而理想上 ground true 出現的 character/word/sequence 的機率當然要比較高，才比較容易被 sample 到。</p>
<p>Sequence generation 可以拿來做很多應用，只要你能把要輸出、輸入的東西都轉成 vector. (圖片、詩、翻譯後的句子、chatbot)，丟進去給 decoder 訓練。 vector 怎麼來？通常是另外一個 RNN / CNN 來吃輸入，將整個 sequence 讀入後，再拿最後一個時間點的 h / c / y vector 當作 decoder input. 這樣做是因為我們相信 encoder可以把 info 濃縮在最後一個時間點的 h 。而這當然也要 train, 所以 encoder and decoder jointly train, 又變成為 encoder and decoder model</p>
<p>這邊又有一個重點是，像是為圖片下標題、chat bot 、翻譯等應用都是需要機器「看到」
該輸入才能產生對應的 sequence, 這我們稱為 conditional sequence generation. 因為我們希望是有目的的產生，而非隨機。</p>
<p>Encoder 就是吃一個 sequence, 把它轉成一個 vector, 然後丟入 decoder, decoder 第一個時間點拿到該 vector, 開始產生 sequence, 而接下來每一步的 input vector 就是前一個時間點自己產生的分佈抽樣後結果的 vector (+ 初始 encoder vector 如果我們怕 decoder 產生產生就忘了當初的 input 的話）</p>
<p>Dynamic conditional generation 
也是著名的 attention based model,
Decoder 在每個時間點拿到的 conditional representation 都是不一樣的</p>
<p>Sequence to sequence model 需要比較多   Memory, 因為要把 所有 ht 儲存下來讓 decoder 搜尋</p>
<p>在 testing 的時候，看你的應用，決定是要從 decoder 的 output distribution  sampling 還是 argmax</p>
<p>BLEU Score 就是看產生的句子跟原句子的有多少相同字</p>
<p>為了讓 model 穩定容易 train + 讓 training 跟 test distribution 越接近，會使用 schedule sampling: 一開始看比較多 reference, 後面多相信 model 自己產生的結果</p>
<p>通常在每個時間點 decoder 產生 distribution 後我們要得到最後結果時會用 argmax. 但這是 Greedy algor. 我們在 Test 的時候可以用 beam search, 同時紀錄多個 （bean size =2 就是兩條） argmax 後機率最高的 sequence / paths, 最後再選擇連乘機率後 argmax 最高的當作結果。</p>
<p>如果遇到無法微分的 objective function, 就當作 RL 的 reward 做下去就對了</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-based-Model">Attention-based Model<a class="anchor-link" href="#Attention-based-Model">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Recurrent-Neural-Network">Recurrent Neural Network<a class="anchor-link" href="#Recurrent-Neural-Network">&para;</a></h2><h3 id="ML-course-21-1,-49-minutes">ML course 21-1, 49 minutes<a class="anchor-link" href="#ML-course-21-1,-49-minutes">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 slot fitting 的應用下，句子裡頭的 每個 word 就是一個 vector 包含所有的 x, 按照在句子中的順序被丟入 RNN, 而 output y vector 如果有 3 維， 就代表要預測該 word 分別是三種 slot 種類的機率 （dest, time, departure)</p>
<p>句子中 word 順序會有影響
同個字的 output ，依照前面的 word 所產生的 memory 會有所不同（leave / enter shop)</p>
<p>RNN 關鍵概念：時間不同，同個 network</p>
<p>Jordan 比 Elman 好的原因可萌是因為 Jordan 記憶是存上個時間點的輸出值 y 而不是 hidden layer ，且因為 y 是有 target 的比較好控制它的值</p>
<p>RNN 基本概念就已經包含雙向了，同時訓練正向及逆向的 RNN</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>LSTM 的 memory cell 可以想成是一個特殊的 neurons, 有三個 gates: input, output, forget gate。一個 LSTM 的 memory cell 有四個 input: input 本身以及其他三個操控 gate 開關的「信號」，最後則產生一個 output</p>
<p>LSTM 之所以叫做 Long Short-term Memory, 就是在說，相比於傳統 RNN，其 memory cell 裡頭的值能夠保留比較久（只要不 forget, 沒有 input)</p>
<p>人體 LSTM</p>
<p>LSTM 要差四條電源線才能跑，參數是 4 倍</p>
<p>基本上 LSTM 就是 standard, 別人說他用 RNN，有很大的機率就是在說他用 LSTM。而最簡單的 RNN 則通常叫做 SimpleRNN</p>
<p>LSTM 基本上 X 會跟四組 weights 做 4 次 linear transformation 產生跟一層 LSTM neuron 數目相同的 Z, Zi, Zo, Zf vectors, 然後將各個 vectors 利用一連串的矩陣運算組合起來（可以運用平行運算， GPU 優勢）再最後產生 Y vectors. 而且每個時間點不只是餵進 X, 還會將上一次的 Y 作為 hidden layer output 以及 memory cell 裡頭的 C vectors 拿過來，跟 X 組合起來一起跟 4 組 weights 做結合</p>
<p>而當然 LSTM 也可以 deep / multi-layer, 疊 5、 6 層的 LSTM cells</p>
<p>GRU（Gated Recurrent Unit） 是 LSTM 的一個簡化版本，只有兩個 gates, 少了 1/3 參數但聽說 performance 差不多，可以比較容易避免 overfitting （training 時比較 robust）</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ML-course-21-2,-1-hr-30-minutes">ML course 21-2, 1 hr 30 minutes<a class="anchor-link" href="#ML-course-21-2,-1-hr-30-minutes">&para;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>做 slot fitting 時，在做 training 時也要按裝 utterence (sequence) 當初的順序把字丟進去，要不 LSTM cells 就無法得到正確的 memory 值。</p>
<p>RNN 也跟 feed-forward network 一樣用 gradient descent 及 back-progagation 來訓練，但因為 RNN 要考慮時間序列，因此是使用 Back progagtion throught time (BPTT), 先想成進階版的 back progagation 即可。</p>
<p>RNN 不好 train，訓練過程中可能 loss 變動很大，而這可能不是程式 bug，而是因為 RNN 的 loss function 的 error surface 不是很陡峭就是很平滑，導致這種現象。而一個解決方法是 clipping: 限制 gradient 最大的值。</p>
<p>而其原因不是因為 activate function 是 sigmoid 這麼簡單，而是因為 RNN 在處理 time sequce 時，將 memory 的值轉換到 cell 時用到的 weights 是相同的。每個時間點用相同的 weight 疊加的結果，就是變成要嘛完全沒變化；要嘛天崩地裂。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為什麼要用 LSTM 取代傳統 RNN？ 因為可以解決 gradient vanishing 問題。（不過 Hinton 也有推出一篇用傳統 RNN + identity matrix initialization 證明能用簡單 RNN 贏過複雜的 LSTM）</p>
<p>GRU 精神：「舊的不去，新的不來」。讓 input gate 跟 forget gate 連動：</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>RNN applications</p>
<ul>
<li>Input element 跟 output element 一樣多<ul>
<li>input and output are both sequences with the same length</li>
<li>slot fitting (每個 input word 一定會對應到一個屬於 30 ~ 40 個 slots 的其中一個)</li>
</ul>
</li>
<li>Many-to-one<ul>
<li>input 是 vector sequence, output 只是一個 vector</li>
<li>中文的話一個字就是一個 char -&gt; input 是一個 char sequence</li>
<li>英文的話每個 word 就通過 embedding layer 變成一個 vector -&gt; vector sequence</li>
<li>setiment analysis (maybe 5 class-classification)<ul>
<li>在最後一個時間點把 hidden layer 結果拿出來，經過幾次 transformation 得到分類結果</li>
</ul>
</li>
<li>key term extraction</li>
</ul>
</li>
<li>Many-to-Many (output is shorter)<ul>
<li>speech recoginition <ul>
<li>很棒的例子：好棒 vs 好棒棒</li>
<li>因為通常 0.01 秒就取做一個 vector，而通常很多個 input vectors 才實際對應到一個中文 charactor(output is shorter)</li>
<li>with CTC = 允許 output NULL, not trimming, google 現在貌似就是用這個</li>
<li>不需要教 machine 一個詞彙長什麼樣子</li>
</ul>
</li>
</ul>
</li>
<li>Many-to-Many (No Limitation)<ul>
<li>事先不知道 input / output 誰長：Sequence-to-Sequence learning</li>
<li>machine translation<ul>
<li>用 RNN 把所有 input vectors 讀入，再看最後一個時間點，已經包含整個 sequence information 的 memory，用它來產生第一個 output character. 接著把該 charactor 再跟 memory 結合，產生下一個 output character</li>
<li>有趣的例子： ptt 推文接龍</li>
<li>RNN 不只 output 一般中文字，也會 output 「斷」</li>
</ul>
</li>
<li>google 嘗試將法語聲音訊號直接轉成英文文字<ul>
<li>代表我們不需要做「語音辨識」：直接跳過把法文聲音訊號轉成法文文字這件事情</li>
</ul>
</li>
</ul>
</li>
<li>Beyond Sequence<ul>
<li>Syntactic Parsing <ul>
<li>把語法樹表達成 sequence 的 format 以後，我們也能讓 RNN 訓練成看過一個句子，就能分析出語法樹</li>
</ul>
</li>
</ul>
</li>
<li>Sequence-to-sequence Auto-encoder - Text<ul>
<li>一般 BOW 方式沒考慮到語順，可能導致結果天差地遠</li>
<li>在有考慮 word sequence order 的情況下，利用 encoder 將一個 document encode 成一個 vector，再將該 vector 丟給 decoder 讓 decoder 還原。如果最後能訓練成功，就代表該 embedding vector 包含了原句子中重要的概念。</li>
<li>這個好處是不用 label data. 你只需要大量的句子</li>
<li>也可以用 hieratcial 的方式</li>
</ul>
</li>
<li>Sequence-to-sequence Auto-encoder - Speech<ul>
<li>Audio 的 word2vector</li>
<li>可以用來做語音搜尋</li>
<li>chat bot</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>感想：</p>
<ul>
<li>sequence-to-sequence learning 基本上都是要有 RNN encoder / decoder 同時訓練（jointly trained）。而 decoder 拿 encoder 在最後一個時間點（sequence 最後一個 input 輸入後的 memory 結果代表著整個 sequence 資訊）的 embedding vector (memory vector) 來想辦法產生想要產生的 output。</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Attention-base Model: 也有使用到 memory，RNN 的一個進階的版本</p>
<p>Attention-based Model 的應用</p>
<ul>
<li>reading comprehension<ul>
<li>先利用 semantic analysis 將每個句子表示成一個 vector，接著將 query 透過 DNN/RNN（中央處理器） 在眾多 vectors 裡頭找出 answer。實際上 RNN 是學會一個讀寫頭（Reading Head Controller）來決定要讀哪個 vector（從哪邊的「記憶」裡頭獲得資訊）。當然可以讀取多次、多個 vectors</li>
<li>因此我們可以透過觀察 reading head 的位置 =( attention 的位置) 來看出針對一個 input，machine 給出特定 output 的思路過程。</li>
</ul>
</li>
<li>Visual Question Answering<ul>
<li>跟 Reading comprehension 類似，只是 vectors 事先利用 CNN 將圖片的各個部分萃取成 vector</li>
</ul>
</li>
<li>Speech Question Answering<ul>
<li>托福的例子：(1) 將故事的錄音黨做語音辨識，取得文字黨以後做 semantic analysis, (2) 將問題也做 semantic analysis，然後對兩者做 attention，得到最終答案：就像是在畫重點</li>
<li>除了語音辨識的部分以外，其他整個都是一個 NN</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep &amp; Structured</p>
<p>Deep Learning 因為非線性，要得到 state-of-the-art 結果，還是比較推薦。但其實可以整合 DL &amp; Structured learning
未來的趨勢，就跟 Ian Fellow 的 Deep Learning 書的 Part 2, 3</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ML-Lecture-8-1:-&ldquo;Hello-world&rdquo;-of-deep-learning_1">ML Lecture 8-1: &ldquo;Hello world&rdquo; of deep learning<a class="anchor-link" href="#ML-Lecture-8-1:-&ldquo;Hello-world&rdquo;-of-deep-learning">&para;</a></h2><ul>
<li><a href="https://www.youtube.com/watch?v=Lx3l4lOrquw">https://www.youtube.com/watch?v=Lx3l4lOrquw</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="NMT">NMT<a class="anchor-link" href="#NMT">&para;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>dataset<ul>
<li><a href="http://www.manythings.org/anki/">http://www.manythings.org/anki/</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>準備資料需要的步驟</p>
<ul>
<li>Add a <strong>start</strong> and <strong>end</strong> token to each sentence.</li>
<li>Clean the sentences by removing <strong>special</strong> characters.</li>
<li>Create a word index and reverse word index (dictionaries mapping from word &rarr; id and id &rarr; word).</li>
<li>Pad each sentence to a <strong>maximum</strong> length.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Terminology</p>
<ul>
<li>context vector</li>
<li>Teacher forcing</li>
</ul>
</div>
</div>
</div>


                <!-- Tags -->
                <p class="blog-content__tags">
                    <span>Post Tags</span>

                    <span class="blog-content__tag-list">
                    </span>

                </p>













































































                <!-- end Tags -->


                <!-- Mail-list-subscribe -->
                <div id="article-inner-subscribe" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                        <div class="blog-content__prev">
                            <a class="open-popup" rel="subscribe">
                                <span>Get Latest Arrivals</span>
                                訂閱最新文章
                            </a>
                        </div>
                        <div class="blog-content__next">
                            <p>
                                跟資料科學相關的最新文章直接送到家。</br>
                                只要加入訂閱名單，當新文章出爐時，</br>
                                你將能馬上收到通知 <i class="im im-newspaper-o" aria-hidden="true"></i>
                            </p>
                        </div>
                    </div>
                    <div class="blog-content__all">
                        <a class="open-popup btn btn--primary " style="color: #FFFFFF">&nbsp;&nbsp;Subscribe&nbsp;&nbsp;&nbsp;</a>
                    </div>
                </div>
                <!-- end Mail-list-subscribe -->

                <!--Pagination-->
                <div id="article-inner-neighbor-pages" class="blog-content__pagenav">
                    <div class="blog-content__nav">
                    </div>

                    <div class="blog-content__all">
                        <a href="blog.html" class="btn btn--primary">
                            View All Post
                        </a>
                    </div>
                </div>
                <!-- end Pagination-->

            </div><!-- end blog-content__main -->


        </div>
        </div> <!-- end blog-content -->

    </article>

<div class="comments-wrap">
    <div id="comments" class="row">
        <div class="col-full">
            <div id="disqus_thread"></div>
        </div>
    </div>
</div>

<script type="text/javascript">
var disqus_shortname = 'leemengtaiwan';
var disqus_title = '深度學習筆記 Vol.1 - 李宏毅教授的 Deep Learning Theory';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="https://leemeng.tw/theme/images/logo.png" alt="Homepage"></a>
                </div>

                <ul class="footer-social">
<li><a href="https://github.com/leemengtaiwan" target="_blank">
    <i class="im im-github" aria-hidden="true"></i>
    <span>Github</span>
</a></li>
<li><a href="https://www.facebook.com/LeeMengTaiwan" target="_blank">
    <i class="im im-facebook" aria-hidden="true"></i>
    <span>Facebook</span>
</a></li>
<li><a href="https://www.instagram.com/leemengtaiwan/" target="_blank">
    <i class="im im-instagram" aria-hidden="true"></i>
    <span>Instagram</span>
</a></li>
<li><a href="https://www.linkedin.com/in/leemeng1990/" target="_blank">
    <i class="im im-linkedin" aria-hidden="true"></i>
    <span>LinkedIn</span>
</a></li>                </ul>
            </div>
        </div>

        <div class="row footer-bottom">
            <div class="col-twelve">
                <div class="copyright">
                    <span>Powered by <a href="http://getpelican.com/" target="_blank">Pelican</a></span>
                    <span>© Copyright Hola 2017</span>
                    <span>Design by <a href="https://www.styleshout.com/" target="_blank">styleshout</a></span>
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>
        </div> <!-- end footer-bottom -->
    </footer> <!-- end footer -->


    <div id="preloader">
        <div id="loader"></div>
    </div>

        <!-- Javascript
    ================================================== -->
    <script src="https://leemeng.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://leemeng.tw/theme/js/plugins.js"></script>
    <script src="https://leemeng.tw/theme/js/main.js"></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--https://instant.page/-->
    <script src="//instant.page/1.0.0" type="module" integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>


    <script type='text/javascript' src='https://leemeng.tw/theme/js/progress-bar.js'></script>
    <script type='text/javascript' src='https://leemeng.tw/theme/js/scroll-detect.js'></script>

    <!--show and hide left navigation by scrolling-->
    <script>
    $(document).scroll(function() {
        var y = $(this).scrollTop();
      if ( $(window).width() > 980 ) {
        if (y > 600) {
          $('#left-navigation').fadeIn(300);
        } else {
          $('#left-navigation').fadeOut(300);
        }
      }
    });
    </script>

<!--reference: https://gist.github.com/scottmagdalein/259d878ad46ed6f2cdce-->
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/embed.js" data-dojo-config="usePlainJson: true, isDebug: false">
</script>

<script type="text/javascript">
  function showMailingPopUp() {
    require(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us18.list-manage.com","uuid":"151cb59f2de814c499c76b77a","lid":"dd1d78cc5e"})})
    document.cookie = "MCPopupClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
    document.cookie = "MCPopupSubscribed=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
  };

  $(function() {
    $(".open-popup").on('click', function() {
      showMailingPopUp();
    });
  });
</script>
<!--reference: https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_overlay-->
<script>
function openTocNav() {
    document.getElementById("tocNav").style.width = "100%";
}

function closeTocNav() {
    document.getElementById("tocNav").style.width = "0%";
}

function toggleTovNav() {
    var current_width = document.getElementById("tocNav").style.width;
    if (current_width == "100%") {
        document.getElementById("tocNav").style.width = "0%";
    } else {
        document.getElementById("tocNav").style.width = "100%";
    }
}
</script>


</body>
</html>